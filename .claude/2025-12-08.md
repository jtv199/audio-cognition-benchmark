# Project Summary - December 8, 2025

## Session Goals

### Goal 1: Cross-Field Taxonomy Analysis
**Objective**: Reference and analyze taxonomies from other scientific fields beyond the core 4 pillars to identify:
- Alternative categorization frameworks for auditory cognition
- Complementary perspectives that could strengthen benchmark validity
- Potential gaps in current theoretical coverage

**Deliverable**: Comparative analysis of taxonomies with assessment of their applicability to AI audio reasoning evaluation

### Goal 2: Benchmark Competitive Analysis
**Objective**: Deep dive into existing AI audio benchmarks (AIR-Bench, AudioBench, MMAU, others) to map:
- **What they do**: Specific tasks and evaluation methods
- **Why they chose these tasks**: Underlying theoretical justification or gaps
- **What they lack**: Missing capabilities relative to human auditory cognition (using our 4-pillar framework as reference)

**Deliverable**: Gap analysis matrix showing opportunities for differentiation and areas of necessary overlap

---

## AI Audio Reasoning Benchmark

### Core Objective
Develop a benchmark to evaluate Audio Large Language Models (AudioLLMs) on their ability to perform higher-order reasoning tasks, moving beyond simple transcription and classification.

### The Central Problem
Current AI audio models excel at **Perception** (identifying "Dog bark") but lack **Reasoning** (inferring "The dog is barking behind a wall, so I cannot see it"). This project bridges that gap by analyzing how four distinct scientific fields categorize the hierarchy of human hearing.

---

## Theoretical Framework: The 4 Pillars

### 1. Ecological Psychology (The Physics View)
**Source**: Gaver, W. (1993) - Taxonomy of Everyday Listening

**Key Concept**: Humans hear events, not just frequencies.

**Core Categories**:
- Vibrating Solids: Impact, Scraping, Rolling (Mass/Texture detection)
- Liquids: Dripping, Pouring (Viscosity/Volume detection)
- Aerodynamics: Exploding, Hissing (Pressure/Force detection)

**Benchmark Application**: Tests the AI's "Physics Engine." Can the model infer material properties (wood vs. metal) or causal states (is the bottle full?) from sound alone?

---

### 2. Computational Auditory Scene Analysis (The Grouping View)
**Source**: Bregman, A. (1990) - Auditory Scene Analysis

**Key Concept**: The separation of signal from noise via Primitive vs. Schema-based mechanisms.

**Core Categories**:
- Primitive Segregation: Automatic grouping by pitch/temporal proximity (Bottom-up)
- Schema-Based Segregation: Attention-driven grouping using learned patterns (Top-down)

**Benchmark Application**: Tests the AI's "Attention Engine." Can the model use a specific prompt (a schema) to "hear out" a target signal in a "cocktail party" scenario, or does it fail when signals overlap?

---

### 3. Cognitive Neuroscience (The Architecture View)
**Source**: Rauschecker & Scott (2009) - Dual Stream Hypothesis

**Key Concept**: The brain splits processing into Identity and Action/Space.

**Core Categories**:
- Ventral Stream ("What"): Object identification and semantic mapping
- Dorsal Stream ("Where/How"): Spatial localization and sensorimotor reproduction

**Benchmark Application**: Validates structural integrity. Ensures the benchmark scores "Object Recognition" (Ventral) separately from "Spatial Tracking/Action" (Dorsal), preventing a single metric from hiding specific deficits.

---

### 4. Clinical Audiology (The Stress-Test View)
**Source**: Katz (1992) - The Buffalo Model

**Key Concept**: Processing is defined by its Failure Modes under stress.

**Core Categories**:
- Decoding: Speed of processing
- Tolerance-Fading Memory: Performance in noise + Short-term memory load
- Integration: Multimodal synthesis

**Benchmark Application**: Design of "Adversarial" samples. Uses "Tolerance-Fading Memory" concept to stress-test the AI's context window with noise, measuring at what point reasoning collapses.

---

## Three-Layer Task Hierarchy

### Layer 1: Foundational (Perception & Schema)
**Goal**: Test the model's ability to parse auditory scenes and understand environmental context.

**Tasks**:
- Stream Segregation: Separating a target voice from background noise/other speakers (Bregman, 1990)
- Schema Congruity: Identifying sounds that are contextually inappropriate, e.g., chainsaw in a library (Gaver, 1993)

### Layer 2: Cognitive Control (Attention & Load)
**Goal**: Test executive function, inhibition, and resource management.

**Tasks**:
- Selective Attention: The "Cocktail Party Effect"‚Äîfocusing on one speaker while ignoring distractors (Cherry, 1953)
- Divided Attention: Monitoring two streams simultaneously/multitasking (Broadbent, 1958)

### Layer 3: Inferential (Reasoning & Theory of Mind)
**Goal**: Test high-level social and causal reasoning.

**Tasks**:
- Paralinguistics: Interpreting intent/emotion (e.g., sarcasm) from prosody (Scherer, 2003)
- Causal Reasoning: Inferring a narrative sequence from environmental sounds, e.g., footsteps ‚Üí door open (Ballas, 1993)

---

## Excluded Domains
- Spatial Audio: Localization (requires binaural data)
- Music Cognition: Melodic structure/theory (too specialized)
- Low-Level Psychoacoustics: Gap detection/thresholds (likely trivial for models)

---

## Strategic Direction

**Perception Tasks (Current State of AI)**: Identifying a sound (Ventral Stream)

**Reasoning Tasks (Goal of Benchmark)**:
1. Causal Inference: "What made this sound?" (Ecological - Gaver)
2. Contextual Selection: "Find the specific voice in the crowd." (CASA - Bregman)
3. Spatial Logic: "Where is the object going?" (Neuro - Dorsal Stream)

---

## Current Phase: Phase 1 - Literature Review & Taxonomy Selection

### Next Steps
- [ ] Data Generation: Synthesize audio samples corresponding to Gaver's interaction types (e.g., scrape vs. roll)
- [ ] Task Design: Create "Schema" prompts for CASA tests (e.g., "Ignore the siren, track the footsteps")
- [ ] Stress Testing: Apply noise layers derived from the Buffalo Model (Tolerance) to baseline datasets

---

## Open Questions

1. **Strategic Differentiation**: Should the project "double up" on foundational tasks covered by existing benchmarks (AIR-Bench, AudioBench) or aggressively focus on under-explored "Control" and "Inferential" layers?

2. **Dataset Construction**: For "Cocktail Party" tasks, is it better to synthetically mix clean datasets (perfect ground truth) or use "in-the-wild" recordings (realistic but hard to validate)?

3. **Baselines**: Do we need to run our own human trials for baselines, or can we rely on literature values?

4. **Roadmap Risks**: What are the typical "kill steps" or high-risk phases in this type of project (Data Gen vs. Inference vs. Eval)?

---

## Relevant Benchmarks Analyzed
- **AIR-Bench**: Foundation vs. Chat tiers; introduces "Mixed Audio" tasks
- **AudioBench**: Standardizing tasks; introduces "Speech Instruction" (spoken commands)
- **MMAU**: Expert Reasoning and specific skills (role mapping, emotion flip); uses strict Multiple Choice format

---

## Key References
- Gaver, W. (1993). Taxonomy of Everyday Listening
- Bregman, A. (1990). Auditory Scene Analysis
- Rauschecker & Scott (2009). Dual Stream Hypothesis
- Katz (1992). The Buffalo Model
- Cherry (1953). Cocktail Party Effect
- Broadbent (1958). Divided Attention
- Scherer (2003). Paralinguistics
- Ballas (1993). Causal Reasoning

---

## Session Updates

### Taxonomy Extraction Complete ‚úÖ

**Completed Deliverables**:
1. ‚úÖ **[taxonomy.md](../output/taxonomy.md)** - Complete 4-Pillar taxonomy with 14 mermaid diagrams
2. ‚úÖ **[taxonomy-overview.md](../output/taxonomy-overview.md)** - 88 example questions across 35 leaf nodes
3. ‚úÖ **[analogies.md](../output/analogies.md)** - Evolutionary framework (Lizard‚ÜíDog‚ÜíMonkey)
4. ‚úÖ **Text extraction script** - First 2000 words from all 12 PDFs
5. ‚úÖ **[local-references.md](../structure/local-references.md)** - Updated with all processed papers

**Key Statistics**:
- **35 leaf nodes** identified across 4 taxonomies
- **88 example questions** created
- **12 papers** fully processed and cataloged
- **4 taxonomies** with complete mermaid visualizations

**Three-Layer Framework**:
- **Layer 1** (Lizard ü¶é): Basic perception, automatic processing
- **Layer 2** (Dog üêï): Attention, working memory, cognitive control
- **Layer 3** (Monkey üêµ): Causal reasoning, theory of mind

---

## Email Correspondence with Ting

### December 6, 2025 - Initial Taxonomy Proposal

**Jack's Question**: Shared preliminary taxonomy table with 6 included tasks (Stream Segregation, Schema Congruity, Selective Attention, Divided Attention, Paralinguistics, Causal Reasoning) and 3 excluded domains (Sound Localization, Melodic Structure, Low-level Psychoacoustics).

**Key Questions Raised**:
1. Differentiation strategy: Double up on foundational tasks or focus on under-explored areas?
2. Should I look for relevant datasets?
3. Should I establish human baselines from cognition tests?
4. Timeline and milestones?
5. AI tool usage policy?

### Ting's Response

**Main Feedback**:

1. **Systematic Framework Request**: "Is there any systematic definition of human hearing capabilities? What you listed covers most cases, but the points seem scattered. A systematic framework to categorize this would be very helpful."
   - ‚úÖ **Addressed**: Created 4-Pillar framework with Gaver (Physics), Bregman (Grouping), Rauschecker (Architecture), Katz (Clinical)

2. **On "Double Up" Concern**: "I wouldn't say this is a 'double up'; rather, different perspective. Create a separate table listing similarities and differences between our work and the benchmarks."
   - ‚è≥ **Next Step**: Create combined-taxonomy-table.md comparing with AIR-Bench, AudioBench, MMAU

3. **On Datasets**: "Yes! Check which datasets are available. We can create our own if needed."
   - ‚è≥ **Next Step**: Dataset availability mapping

4. **On Human Baselines**: "Very good point! Review cognition tests to see how we could create labels. Final objective: always possible to compare results with human performance."
   - ‚è≥ **Next Step**: Map tasks to human cognition tests and performance metrics

5. **On Timelines**: "What you are doing now is the most difficult step. Finding and defining the right questions is the hardest part."
   - ‚úÖ **Current Phase**: Question definition and taxonomy complete

6. **On AI Tools**: "They are useful, but don't rely on them 100%. Always think critically‚Äîcheck references in Google Scholar."
   - ‚úÖ **Acknowledged**: All citations manually verified

---

## Next Steps

### Immediate (Next Session)
1. **Create combined-taxonomy-table.md**: Map email taxonomy + 4-Pillar framework + existing benchmarks
2. **Benchmark gap analysis**: What do AIR-Bench, AudioBench, MMAU cover vs. miss?
3. **Dataset availability mapping**: Which tasks have datasets, which need creation?

### Short-term
4. **Human baseline research**: Identify cognition tests for each task layer
5. **Progress report email**: Summarize taxonomy work and next phase
