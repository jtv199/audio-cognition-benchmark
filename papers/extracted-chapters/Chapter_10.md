# Chapter 2: The Auditory World

*Source: Auditory Cognition and Human Performance by C. L. Baldwin (2012)*

---

# 2 The Auditory World

## **INTRODUCTION **

Sound is around us at all times. Close your eyes for just a few moments, and you can better appreciate the many different sounds present in the environment. Many of these sounds typically go unnoticed in our day-to-day existence. But, when we close our eyes for just a few minutes we begin to notice the barrage of electronic buzzing sounds from lights, ventilation systems, and other home or office appliances that incessantly bombard our senses. If you are fortunate enough to escape these modernday devices for a period of time, such as on a hike in the woods or a camping trip, then you may notice a new orchestra of the sounds of nature, which are often masked by the technological sounds of modern-day life.

Before one can investigate how we process sounds, whether natural or artificial, we need a sound understanding (pun intended) of their physical characteristics. This chapter highlights some of the many sources of sound present in everyday and work environments and describes their characteristics in relation to the human auditory processing system. This will provide a foundation for understanding the subsequent material. However, the reader who is already an expert on these topics may wish to skip to the next chapter. To begin, a discussion of the many different sources of sound in the home and in various work settings is presented.
## **SOURCES OF SOUND **

Music and speech are the two most valued, as well as cognitively complex, sources of sound (Zatorre, Belin, & Penhume, 2002). However, a plethora of additional sources of sound (i.e., auditory displays, alerts, and warnings) has proliferated in modern societies. It is common to have conversations and other activities interrupted by one or more myriad sounds stemming from modern technologies, such as mobile phones, pagers, and home/office electronics. Both music and speech are discussed in further sections. First, the multitude of nonverbal sounds is considered.
### **Nonverbal Sounds **

The majority of sounds we hear are nonverbal, although many of these receive little or no direct attention. As I sit at my desk, I can hear the hum of a ventilation system, the whir of a fan in a nearby computer, the shuffling of papers, doors closing in nearby hallways, and the sounds made by my keyboard as I type. By definition, any of the many sounds that form the constant auditory canvas of our environment that are unwanted or irrelevant to the current task at hand can be broadly classified as noise. Irrelevant or distracting sounds can increase the mental workload of processing information from auditory as well as other sensory channels. Because of their impact on mental workload, noise and particularly irrelevant speech are important topics discussed in more detail. A discussion of nonverbal noise can be found in Chapter 7 , “Nonverbal Sounds and Workload,” and the particular case of irrelevant speech is discussed in Chapter 8 , pertaining to speech processing.

Setting the topic of noise aside, nonverbal sounds can be used to provide a wealth of information and have powerful alerting capabilities. Therefore, nonverbal sounds are used to provide information in a wide variety of home and occupational environments.

The following personal example illustrates the challenge that can be posed by the cornucopia of sounds in the everyday environment.

After just moving into a new home, I was awakened one night by an intermittent, although seemingly persistent, faint auditory tone. After noticing the sound and not immediately being able to identify its source, my curiosity was piqued, not to mention my annoyance from having been awakened. My first thought was that it might be coming from one of the appliances, perhaps a signal that the washer, dryer, or dishwasher had finished its cycles. A quick check indicated this was not the case. The next logical guess was that perhaps a smoke detector battery was low. However, remembering that the smoke detectors were wired into the electrical system, this possibility was quickly ruled out. After more consideration, it occurred to me that the sound was a familiar alert. In fact, it sounded just like the new message alert on my mobile phone. However, after checking the phone several times, I was satisfied that the sound was not coming from my mobile phone. So, the question remained, where could it be coming from? Tired and a bit annoyed at this point, I put a pillow over my head and went back to sleep. When I awoke in the morning the sound was still present, intermittent, but nevertheless present. Again, I asked myself, was it something that actually needed my attention, or was it just a source of annoyance at this point? After many more failed attempts at identification, I finally discovered the source of the sound. It seemed to have been coming from a box, among a stack of boxes yet to be unpacked. Inside the box, I discovered that my old mobile phone had inadvertently been switched on, and the sound was indeed a new message alert signaling me of the arrival of a message from my mobile phone provider. The phone that I had had disconnected some months ago had no service plan, and the former service provider had been kind enough to send a message stating this. How thoughtful of them!

This trivial little story points to just one of the many sounds present in our homes that have been ushered in with the digital age. Homes, offices, vehicles—all are alive with sound. Microwaves, washers, dryers, dishwashers, toaster ovens, and cappuccino makers, to name a few, provide us with auditory displays. Many of these sounds are in the form of alerts and buzzers that have been intentionally designed by the manufacturers for a particular purpose. Others, such as the sound of water swirling in the washer or dishwasher, are merely sounds inherent to the operation of the machine. But, even such intrinsic sounds can provide information. A particular favorite of mine is the gurgling sound produced by my coffee pot in the morning signaling that a fresh pot of java is nearly ready.

**FIGURE 2.1 **Water-activated wristband safety alarm. (From Bridget Lewis.)

The modern home environment contains many sources of sound, including alerts and alarms. Doorbells and chimes, appliance signals, smoke and carbon monoxide detectors, and intruder alarms are examples of common household alarms. More recently, specialized alarm systems have been designed. As illustrated in Figure 2.1 , one example is a system designed to sound an alarm if someone wearing a special wristband (e.g., a small child) enters a swimming pool.

It is clear that the home environment is replete with sound. We turn the focus now on the many sources of sound in the work environment. It is worth pointing out, however, that as many of us take more of our work home and engage in telecommuting and various other work-related activities outside the confines of the conventional work environment, there may be no clear distinction between sources of sound in the workplace and in the home.
## **SOURCES OF SOUND IN WORK ENVIRONMENTS **

As with the home, there are a multitude of sources of sound in the modern work environment. We begin by discussing auditory alerts, warnings, and displays in general and then move on to discuss sources of sound in three work environments that have received considerable attention in human factors research. These three work settings are aviation, surface transportation, and health care. All three have witnessed a proliferation of auditory alarms, including bells, such as the simple one pictured in Figure 2.2 , to whistles, buzzers, and gongs, to name but a few.

In recent years, considerable human factors research has led to improved understanding of the human performance capabilities and performance issues associated with warnings. Critical issues in current warnings theory include not only signal characteristics associated with detection and comprehension but also such issues as urgency mapping, compliance, and preferred display format (i.e., visual, verbal vs. nonverbal, haptic, or multisensory).

**FIGURE 2.2 Fire bell **.
### **Auditory Warnings **

Auditory alerts and warnings are most abundant in high-workload, high-stress environments where the consequences of performance failure are dramatic (Edworthy & Adams, 1996). Auditory warnings come in a wide variety of formats, including bells, whistles, buzzers, and gongs (Stanton & Edworthy, 1999b), and have proliferated in recent times in both number and type. Identification and rapid processing of these warnings is frequently critical to both human and system safety. Numerous investigations have established empirical support for human factors design guidelines for auditory warnings. Acoustic characteristics such as intensity, frequency, and onsetoffset time affect important warning parameters such as noise penetration, alerting effectiveness, perceived urgency, and annoyance (Baldwin, 2011; Baldwin & May, 2011; Edworthy & Stanton, 1995; Hellier, Edworthy, & Dennis, 1993; Patterson, 1990a). For example, auditory warnings of high amplitude and frequency are generally perceived as more urgent than warnings of low amplitude and frequency (Hellier & Edworthy, 1999b; Hellier, Edworthy, Weedon, Walters, & Adams, 2002). A wide variety of auditory alerts, along with a host of auditory displays and interfaces, is common in the modern workplace.

**Auditory warnings can be nonverbal tones and signals, verbal alerts or messages, and the more recently developed categories consisting of auditory icons and earcons (Belz, Robinson, & Casali, 1999), hearcons (Sodnik, Dicke, Tomazic, & Billinghurst, 2008), and spearcons (Walker, Nance, & Lindsay, 2006). Traditional nonverbal auditory alerts are defined by their acoustic parameters, while auditory icons are representational sounds, such as breaking glass or screeching brakes (Belz et al., 1999). Earcons are made of structured musical tones that listeners learn to identify as representing a specific system state or condition. As one example of the use of auditory icons, Belz and colleagues found that, compared to conventional nonverbal auditory alerts, auditory icons can improve brake response time and driver performance in simulated collision situations. As will be discussed further, various auditory alerts are increasingly being installed in modern vehicles to aid in collision avoidance. Common examples of nonverbal auditory alerts include the familiar backup alerts on forklifts and many commercial trucks, emergency vehicle sirens, fire alarms, telephone rings, and the auditory bell sound signifying receipt of a new e-mail message **.
#### **Nonverbal Alarms **

Nonverbal alarms are frequently used in many home and occupational environments. They come in a variety of forms, ranging from simple tones and melodic patterns to complex representational sounds. We begin with a discussion of sirens.

*Sirens *

Stationary sirens are used to indicate various critical states, such as emergency weather conditions (e.g., tornados and hurricanes) and fires. Stationary sirens must be easily recognizable since the action responses may be quite different. For example, if the siren signifies a tornado, the action response is to seek shelter inside a permanent structure; however, if the siren indicates a fire, the action response is to vacate the building as quickly as possible. For these reasons, siren signals must be designed to consist of distinct acoustic patterns, and people must be trained to recognize these patterns and practice appropriate action responses. Sirens on moving vehicles require additional constraints.

Sirens on emergency response vehicles not only must be easily detected and recognized but also localized. Since humans cannot easily localize pure tones or sounds consisting of limited-frequency bandwidths, moving sirens should consist of a broad range of frequencies. However, many sirens currently use a frequency range of 500 Hz to 1.8 kHz, too narrow a band to be easily localized (see discussion by Withington, 1999).

Patterson (1982) and Hass and Edworthy (1996) identified new sounds for use in sirens and other alerts that have better detection and localization rates. In one onroad investigation, compared to conventional sounds, the new siren sounds improved the accuracy of determining the direction of an ambulance siren by as much as 25% (Withington, 1999).

*Auditory Icons *

Auditory icons are nonverbal representational sounds (Belz et al., 1999). Since they represent familiar sounds, auditory icons do not require overt learning (Keller & Stevens, 2004). In addition to the screeching brake example described, auditory icons have been incorporated in a number of computer applications (Gaver, 1986), including educational software programs (Jacko, 1996). A taxonomy developed by Keller and Stevens for classifying auditory sounds in terms of their association with the object or referent they represent is described in detail in Chapter 11, “Auditory Display Design.”

*Earcons *

Earcons are musical patterns that operators learn to associate with system states or events. Ronkainen (2001) gave the example that all Nokia mobile phones come with a default auditory pattern of two short notes followed by a pause and then two short notes again to signify the presence of a new text message. This “musical” pattern or earcon is quickly learned by users, although without learning it would have no inherent meaning in and of itself. Earcons can be used to alert trained listeners to critical system states and therefore can be considered a class of nonverbal warning.

**FIGURE 2.3 **Fire truck.

However, they can also be considered more broadly as auditory displays and share similarities with data sonification. The difference is that with earcons listeners learn to associate a musical pattern with a certain state, whereas in sonification, changing data or values (i.e., temperature or velocity) are represented by sounds in a manner similar to an auditory graph (Walker, 2002).
#### **Verbal Warnings **

As Stanton and Baber (1999) pointed out, to be effective alarms must be recognized and comprehended in addition to being heard. Learning and experience aid in the recognition of common auditory alarms, such as the well-known siren sounds emitted from a fire truck, like the one illustrated in Figure 2.3 . Verbal warnings can capture attention and convey information at the same time (Hawkins & Orlady, 1993). In this way, they are similar to auditory icons. However, verbal warnings can quickly convey a wide range of information that might not be associated with an easily identifiable sound. Auditory warnings are found in numerous environments, including the automobile and the airplane. For example, in the modern cockpit a verbal warning associated with the Ground Proximity Warning System (GPWS) and the Altitude Alerting System instructs pilots through a digitized or synthesized voice to “pull up” when too close to terrain. Verbal warnings have also been examined for use in collision warning systems (CWSs) in automobiles (Baldwin, 2011; Baldwin & May, 2011).
### **Auditory Displays **

When sounds, whether verbal, nonverbal, or both, are added to a device or computerbased system to provide information to the human operator, the result is an auditory display. Effective design of these displays is an important human factors issue and is examined in the concluding chapter of this book, after a detailed discussion of various aspects of auditory cognition and human performance. At this point, however, it is instructive to examine briefly some major categories of auditory displays.
#### **Sonification **

*Sonification *refers to representing multidimensional information that would normally be presented in a graphic format in an auditory format. Sonification has obvious implications for assisting visually impaired individuals but may also be used in the sighted population when visual information channels are either overloaded or temporarily unavailable (e.g., in dark caverns or tunnels). Sonification has been used to develop usable computer interfaces with many of the same benefits as graphical user interfaces for blind individuals (Mynatt, 1997).

Empirical evidence indicates that information presented in visual and auditory scatterplots is used in much the same way (Flowers, Buhman, & Turnage, 1997). People are able to estimate the direction and magnitude of data points as well as detect the presence and magnitude of outliers in auditory scatterplots in efficient ways comparable to visual displays.

More complex data, such as statistical distributions and time series data, can also be conveyed with equal clarity through auditory relative to visual displays (Flowers & Hauer, 1995). Flowers and colleagues pointed out that in addition to developing interfaces to assist blind individuals, data sonification can be used to develop efficient data representation tools for scientists and engineers, particularly when visual attention must be directed somewhere else (Kramer et al., 1999). We turn now to one more general format for using sound to convey information, the verbal display.
#### **Verbal Displays **

Verbal displays have increasingly been added to a number of human-machine interfaces. Probably the most familiar verbal display is the automated voice messaging systems that are utilized by computerized call centers and automated phone messaging systems that are the default settings on voice mailboxes and telephone answering systems (the synthesized voice that comes prerecorded telling callers to “Please leave a message”). Verbal displays are also used in a variety of other settings. For example, mall parking garages may utilize automated verbal displays to remind shoppers where they have parked their cars, and moving walkways caution travelers that they are “approaching the end of the moving walkway.” These verbal displays are often presented in synthesized voice (Hawkins, 1987).

*Synthesized Voice *

Considerable effort beginning in the 1960s was expended toward developing synthesized voices that were intelligible and acceptable to the operator (Hawkins & Orlady, 1993; Simpson & Marchionda-Frost, 1984; Simpson & Williams, 1980). Acoustic factors such as speaking rate and fundamental frequency affect speech intelligibility as well as listeners’ perceptions of the personality characteristics of the synthesized voice. For example, synthesized voices presented at fast speaking rates are perceived as less benevolent and more competent, while voices with high fundamental frequencies are perceived as not only less benevolent but also less competent (Brown, Strong, & Rencher, 1974). Synthesized voices are currently used in a wide variety of settings, ranging from automated voice messaging systems to synthesized verbal alerts on aircraft and in other operational environments. As discussed in more depth in further chapters, synthesized voice requires more effort to process relative to natural or digitized natural speech. But, because synthetic speech (particularly when generated in real time from text-to-speech [TTS] synthesizers) requires considerably less data storage capacity relative to digitized voice recordings, it is preferable for many mobile applications.

*Spearcons *

A less-common variety of verbal display that is increasingly being examined for use in auditory menus is the spearcon. Spearcons are created by speeding up a spoken phrase to the point that it is nearly, if not completely, unintelligible without altering its frequency (Walker & Kogan, 2009; Walker et al., 2006). Although spearcons may not be comprehensible as the actual word they represent in isolation, they resemble the word closely enough that they are learned more effectively than arbitrary sounds and show promise as a method of providing a means of navigating complex menus. Spearcons may be particularly beneficial when used in small portable devices without enough space to provide an adequate visual display and yet complex menus are needed and storage capacity.
## **Summary **

Operational sources of sound range from simple auditory alerts and alarms in the form of bells, beeps, and buzzers to sonographic and verbal displays capable of presenting detailed information in an auditory format. As previously mentioned, sources of sound in three particular work environments (the cockpit, the modern vehicle, and medical care facilities) are discussed next. The multitude of sounds used to convey information in these environments will help to underscore the importance of understanding the mental workload of auditory processing.
## **SOURCES OF SOUND IN THE COCKPIT **

The modern cockpit is replete with sound. Heavy demands are placed on visual attention, as pilots are continually required to shift attention across numerous flight deck displays and to maintain awareness of views outside the cockpit window. Therefore, the auditory modality is a good channel for presenting time-critical information. Sources of auditory workload include the simultaneous processing of radio communications, copilot communications, and auditory flight deck alerts, warnings, and displays. Hamilton (1999) pointed out that the multiple sources of simultaneous auditory information require pilots to choose to ignore some information to attend to prioritized sources of information. Ideally, flight deck instruments should assist in minimizing external workload influences by presenting required information in an efficient, integrated manner. In practice, this goal has proven difficult to accomplish, and a vast array of flight deck displays implemented in piecemeal fashion has great potential for increasing total pilot workload.
### **Proliferation of Auditory Avionics Displays **

Dramatic changes in avionics displays took place between 1981 and 1991, including the introduction of the new generation of “glass cockpit” airliners (Learmount, 1995). Concomitant developments in aircraft warning systems have resulted in centralized alerting and monitoring systems that present information regarding flight status, system states, automation mode changes, and traffic management assistance in integrated multifunction displays directly to pilots (Noyes, Starr, Frankish, & Rankin, 1995).
#### **Auditory Avionics Alarms **

Auditory alarms are used on the flight deck to alert crew to dangerous conditions, potentially dangerous conditions, the arrival of new information on visual displays (Patterson, 1982) and to signal changes in system states. According to Patterson (1982), existing auditory warning systems perform their alerting function well, too well in some cases, presenting unnecessarily loud sounds. Loud tones capture attention but can also startle, annoy, and block crew communications at critical points (Patterson, 1982; Wickens & Hollands, 2000). Further, loud noises can increase arousal. During time-critical situations arousal may already be too high; thus, further increases may exacerbate the risk of performance impairment.

According to Patterson’s estimate in 1982, there were as many as 16 different auditory warnings and alerts on some aircraft. Modern aircraft have the same number of alerts, and the trend has been for further increases as new avionics systems are added (Hawkins & Orlady, 1993; Noyes, Cresswell, & Rankin, 1999). For example, Noyes et al. (1999) pointed out that for visual alerts alone, there were 172 different warnings on the DC 8, and this number increased to 418 on the DC 10. Similarly, the number of visual warnings on the Boeing 707 was 188, and this increased to 455 on the Boeing 747 (Hawkins & Orlady, 1993). According to Hawkins and Orlady (1993), during the jet era alone a number of auditory alerts were also incorporated, including a bell, clacker, buzzer, wailer, tone, horn, intermittent horn, chime, intermittent chimes, and later a synthesized voice.

This number is well above the recommended guidelines for the number of auditory dimensions that can be identified on an absolute basis (see discussion in Sanders & McCormick, 1993). For example, Deatherage (1972) suggested that for absolute judgments, four or five levels could be identified on the basis of intensity, four to seven on the basis of frequency, two or three on the basis of duration, and a maximum of nine on the basis of both intensity and frequency. This means that a pilot must develop an extensive repertoire of stored knowledge associations between individual warnings and the events they represent.

The extensive number of alerts in modern aircraft has led to the recognition that performance issues will most likely be related to cognitive factors (attention, learning, memory, and understanding) rather than the perceptual factors such as detection (Hawkins & Orlady, 1993). For example, in one investigation of auditory warnings in military aircraft, Doll and Folds (1986) found high numbers of confusable auditory warnings and alerts (11–12 of similar type on some aircraft). More problematic, however, was the lack of consistency in audio signaling across aircraft. That is, different sounds were used to signify the same event in different aircraft. These factors increase the mental workload of processing auditory warnings. In addition, Doll and Folds found that warning sounds were typically not matched appropriately with the level of hazard or urgency for the event they signified. Inappropriately urgent auditory warnings can cause distraction and increase the mental workload of communicating with flight crew members and performing other flight-related tasks.

To improve perceptual factors associated with auditory warnings, Patterson (1982) suggested auditory guidelines to minimize the adverse effects of loud noises without compromising reliability. Edworthy and colleagues proposed guidelines for improving auditory warning urgency or hazard mapping (Edworthy, Loxley, & Dennis, 1991; Edworthy & Stanton, 1995; Hass & Edworthy, 1996). And, to address cognitive issues, recommendations have been made for attensons (attention-getting sounds) coupled with voice alerts and visual displays within an integrated warning system (Hawkins & Orlady, 1993). All of these issues can potentially have an impact on mental workload and are therefore discussed more thoroughly in subsequent chapters.

In addition to warnings and alerts, aviation operations rely on extensive communication between personnel on the flight deck, flight attendants, and air traffic controllers.
#### **Radio Communications (Radiotelephony) **

Communication between air traffic control (ATC) and pilots primarily occurs over voice-radio channels. Recent implementations of a text-based system called datalink in some airports/aircraft aside, the majority of pilot and ATC communications take place via speech. Miscommunication has been a major contributor to aviation accidents, possibly accounting for over 50% of all major incidents (Nagel, 1988).

In an effort to reduce communication-related incidents, the Federal Aviation Administration (FAA) implemented the use of a standard set of phrases and terminologies for ATC and pilot communication (Mitsutomi & O’Brien, 2004; Roske-Hofstrand & Murphy, 1998). This standardized set of communications, referred to as ATC terminology (Mitsutomi & O’Brien, 2004), requires that both pilots and controllers learn what essentially amounts to a new vocabulary or what Mitsutomi referred to as an example of English for special purposes (ESP). The use of ATC terminology along with standardized procedures for communication have reduced but not eliminated the potential for errors due to miscommunication.

Current ATC terminology is designed to be brief and concise, with an emphasis on promoting accuracy (Mitsutomi & O’Brien, 2004). Grammatical markers and conventions are often eliminated, and a set of “carrier” syllables may be used to promote accurate communication of alphanumeric characters. For instance, the communications code alphabet (alpha, bravo, Charlie, etc.) increases the correct identification of alphabetic characters (Wickens & Hollands, 2000). Procedural deviations and the trade-off between keeping communications brief yet maintaining accuracy remain a challenge, and controller-pilot miscommunications are still cited as a contributing factor to a substantial number of aviation accidents and incidents.

Controllers and pilots must learn the ATC terminology as well as the accepted procedures for communication. Once learned, this communication frequently takes place in an atmosphere of congested frequencies from multiple aircraft attempting to maintain communication with a limited set of controllers, thus adding to the information-processing requirements of all parties. In addition, controller-pilot communications occur amid resource competition stemming from a host of visual displays. The impact of resource competition stemming from multiple sensory input channels on auditory processing is further discussed in Chapter 9 . We turn now to another arena in which sounds are increasingly being used as an important source of information.
## **IN-VEHICLE AUDITORY TELEMATICS **

In-vehicle telematics have proliferated in recent years, increasing the complexity and potentially the attentional demands of the driving task. Many of these advanced systems utilize auditory interfaces. Examples of advanced in-vehicle auditory interfaces include CWSs, lane departure warning systems, route guidance systems (RGSs), infotainment systems (such as speech-based e-mail, Web surfing, and satellite radio that allow users to make MP3 recordings), and advanced traveler information systems (ATISs). Further, despite considerable recent controversy, the use of personal cellular phones while driving is also on the rise. Assessing the mental workload required by processing information from these devices is essential to transportation safety (Harms & Patten, 2003; Horrey & Wickens, 2002; Verwey, 2000). Much like the task of piloting, driving requires heavy visual demands. Empirical research indicates that even small changes in the direction of gaze result in lateral steering deviations (Readinger, Chatziastros, Cunningham, Bulthoff, & Cutting, 2002), making the auditory modality well suited for many auxiliary in-vehicle systems. However, research clearly indicated that the auditory processing requirements of even hands-free devices have the potential to exceed drivers’ attentional capabilities (Lee, Caven, Haake, & Brown, 2001). We begin with a discussion of the proliferation of cellular phone use, some implications for driving, and the different types of cellular phones commonly found in modern vehicles.
### **Cellular Phones **

One need only take a quick look around anywhere that people congregate to notice the exponential growth in the use of mobile cellular communications. Recent figures from the Cellular Telecommunications Industry Association (CTIA) reports verify this proliferation in cellular phone usage, citing an increase from roughly 28 million users in 1995 to well over 276 million users in 2009 (CTIA, 2009) and more recently over 302 million users as of 2010, with over a quarter of all households in the United States having only wireless telecommunications (CTIA, 2010). Correspondingly, there has been an increase in the number of individuals who choose to use their mobile phones while driving (Glassbrenner, 2005).

In a 1997 National Highway Traffic Safety Administration (NHTSA) survey, an overwhelming majority of drivers reported talking on a cellular phone on at least a few average trips (Goodman et al., 1997). Ten percent of the respondents indicated that they talked on cell phones on roughly 50% of their trips, and 16% reported use on most of their trips. The mental workload imposed by talking on a cellular phone (regardless of whether it is handheld or hands free) has been estimated to increase the probability of a crash by four times (Redelmeier & Tibshirani, 1997).

Many recent investigations have examined the impact of cellular phone use on driving. Cellular phone use while driving increases mental workload (Haigney, Taylor, & Westerman, 2000; Ranney et al., 2005), resulting in poorer driving performance. When using a mobile phone, responses to traffic hazards are delayed, and drivers exhibit increased steering error. Drivers use a number of compensatory strategies in an attempt to maintain driving within safe margins; however, empirical evidence suggested that these strategies may in fact not actually decrease risk (Haigney et al., 2000). For example, drivers tend to decrease speed when using a mobile phone. However, at least in simulation studies, decreased speed did not result in fewer lane deviations or off-road instances. These findings suggest that drivers may be more confident in their compensation strategies than actual performance would indicate.
#### **Hands Free and Voice Activated **

Although it may be easy to understand how tasks such as dialing could disrupt driving performance, Goodman and colleagues’ (1997) review of the existing literature regarding crash data indicated that it is the conversation, not reaching for the phone or dialing, that increases crash risk. So, while hands-free, voice-activated cellular phones eliminate risks associated with tasks such as dialing a number (Ranney et al., 2005), performance degradations are still consistently observed during phone conversations (Caird, Willness, Steel, & Scialfa, 2008). For example Strayer, Drews, and Johnston (2003) observed delayed brake response times and impaired recognition of roadway signs in a driving simulator when drivers were engaged in simulated hands-free cellular phone conversations. In a similar investigation comparing the effects of hands-free phone conversations on young and older drivers, delayed reaction times and increased speed variability were observed when drivers were engaged in phone conversations (Strayer & Drews, 2004). Drivers in this investigation were also more likely to be involved in a rear-end collision when engaged in a phone conversation. In addition to cellular phones, there are a number of other sources of sound in the modern vehicle.
### **Collision Warning Systems **

Technological advances coupled with continued efforts to improve transportation safety have led to the development and implementation of in-vehicle CWSs. CWSs designed to alert drivers to potential hazardous situations are being installed in many new cars, public transportation buses, and commercial trucks (Hancock, Parasuraman, & Byrne, 1996; Siuru, 2001). These CWSs have been demonstrated to improve collision avoidance behaviors (Brown, Lee, & Hoffman, 2001), but they also have the potential to distract drivers, particularly when the systems are unreliable or overly sensitive.

As with auditory warnings in general, conveying the appropriate hazard level (perceived urgency) is a central component of effective CWSs (Hellier et al., 2002). Collisions are rare events. If a CWS alert is only provided when a collision is imminent, then the alert event would be so rare that it would possibly be unrecognizable to the driver (Brown, Lee, & McGehee, 2001). Conversely, a CWS that provides alerts too frequently, when the probability of a collision is low, is likely to be perceived as extremely annoying, and drivers may ignore or disable the system.

Verbal CWS messages show promise for appropriately matching the hazard level of a potential collision avoidance system with the urgency of the alert (Baldwin, 2011). Both presentation level and choice of signal word have been shown to affect ratings of the perceived urgency of spoken words (Baldwin, 2011; Baldwin & May, 2011; Hellier et al., 2002). Within a simulated driving context, increasing the amplitude of verbal CWS messages to moderately high levels (+4 dB signal to noise ratio or S/N) increases alerting effectiveness and perceived urgency without having a significant impact on annoyance levels (Baldwin, 2011). Further, verbal CWS alerts have been shown to decrease crashes in driving simulations (Baldwin & May, 2005), particularly among older drivers (Baldwin, May, & Reagan, 2006). Empirical evidence indicated that using auditory alerts in combination with visual warnings significantly improved driver performance and brake response time in potential collision situations (Belz et al., 1999; Brown, Lee, & Hoffman, 2001a).
### **Displays for Maintenance of Separation **

Auditory displays that are functionally similar to CWSs have been developed to alert drivers when their following distance is less than an acceptable range. These systems of headway monitoring, including a visual display coupled with an auditory alert, have demonstrated success in increasing the percentage of time drivers maintain a safe following distance in actual on-the-road investigations (Ben-Yaacov, Maltz, & Shinar, 2002; Shinar & Schechtman, 2002).

Some vehicles come with additional auditory displays to aid in parking. For example, Saab’s optional park-assist system uses sensors mounted on the rear bumper to judge the distance between the rear of an operator’s vehicle and another object (i.e., vehicle or wall). The system provides an auditory alert that changes frequency and pulse rate as the distance between the vehicle and the object changes. BMW has a similar auditory alert system they call the parking guidance system; it allows drivers to indicate the distance at which they would like the alert to come on. Another alert that is becoming more common in the modern automobile warns a driver when he or she makes an unplanned lane departure.
### **Lane Departure Warnings **

Auditory warnings designed to alert drivers to unplanned lane departures have been the focus of considerable research. Several vehicle manufacturers have incorporated auditory lane departure warning systems in at least some of their recent models (i.e., Volvo, Infiniti, and Nissan). These safety systems provide an audible alert to drivers when the camera sensor system of the vehicle detects that the vehicle has drifted out of the appropriate pavement markings without the driver’s intent. The audible alert may be presented alone or in conjunction with visual or tactile warnings (Navarro, Mars, Forzy, El-Jaafari, & Hoc, 2010). Simulated rumble strip vibrations to the steering wheel in combination with an auditory alert have been particularly effective at reducing the effects of unplanned lane departures (Navarro, Mars, & Hoc, 2007). Based on recent crash reports from the Fatal Analysis Reporting System (FARS) and the National Automotive Sampling System General Estimates System for reported crashes occurring from 2004 to 2008, Jermakian (2010) estimated that these systems have the potential to reduce up to 7,500 fatalities and 179,000 crashes per year.

Of course, for fatalities and crash rates to decrease, the systems must effectively alert drivers, and drivers must use them. In a survey of drivers who had lane departure systems in their cars, a substantial proportion indicated that they only sometimes (23%) or never (7%) use them (Braitman, McCartt, Zuby, & Singer, 2010). The most frequent (40%) reason given for not using the system was that they found the warning sound annoying, with another 24% indicating the reason to be because the system provided too many false alarms.

Navarro et al. (2010) examined different tactile vibrations as a means of decreasing lane departures. They used one vibration in the seat, two different types on the steering wheel (one designed to elicit motor priming or MP), and the MP vibration in conjunction with an auditory rumble strip sound. They concluded that the MP asymmetric oscillations of the steering wheel led to better driver response (i.e., shorter duration of lateral excursions, faster acceleration of appropriate steering wheel response, etc.). But, the MP system had the lowest subjective driver acceptance rating, while the auditory rumble strips had the highest. Combining the auditory rumble strips with the MP increased driver acceptance of the MP alert. Clearly, there is room for additional improvements in the design of lane departure warning sounds, a topic that is addressed further in Chapter 11.

These examples illustrate the many new ways that sound is being utilized in the modern automobile to improve safety. Auditory displays are also increasingly being used to provide drivers navigational information in a format that allows them to keep their eyes on the road.
### **Auditory Route Guidance Systems **

Due to the heavy visual demands placed on drivers, auditory RGSs appear to have a number of safety advantages relative to their visual-only counterparts. Drivers using auditory RGSs have been shown to respond faster to targets in a visual scanning task (Srinivasan & Jovanis, 1997b) and to make significantly fewer safety errors (Dingus, Hulse, Mollenhauer, & Fleischman, 1997) relative to using a visual RGS. Auditory RGSs are now available in several formats. An issue of concern with auditory RGSs is the level of complexity that can be conveyed and the need to ensure that these systems do not exceed the information-processing demands of the driver.

In-vehicle routing and navigational systems (IRANSs) and RGSs have proliferated in recent years. RGSs have many potential economic and safety advantages, including reduced traffic congestion, enabling drivers to find destinations more easily while avoiding traffic congestion and delays, decreasing travel time and distance, and resulting in fewer instances of disorientation or getting lost, greater confidence, and less-stressful driving experiences (Eby & Kostyniuk, 1999). However, RGSs also have the potential to distract drivers by increasing the attentional processing requirements (i.e., mental workload) of the driving task. Therefore, the most effective system is one that assists the driver in navigating through and developing a cognitive map of the area without disrupting driving performance or significantly increasing mental workload. An RGS that assists the driver in establishing a cognitive map of an unfamiliar area may decrease the information-processing requirements of navigation and ultimately decreases reliance on the system in the shortest amount of time (Furukawa, Baldwin, & Carpenter, 2004).

Previous research (Dingus, Hulse, & Barfield, 1998; Noy, 1997; Tijerina et al., 2000) suggested that a combination of visual and auditory displays should be used in a comprehensive in-vehicle RGS. Complex routing information is facilitated by visual guidance information relative to a message of comparable complexity presented in the auditory modality (Dingus et al., 1998; Srinivasan & Jovanis, 1997a, 1997b). However, driving performance is degraded less by auditory guidance information relative to visual guidance information while the vehicle is in motion (Dingus et al., 1998; Noy, 1997; Streeter, Vitello, & Wonsiewicz, 1985).

Srinivasan and Jovanis (1997b) found improved driving performance and reduced workload, in complex driving situations in particular, when drivers used a system incorporating auditory guidance in conjunction with an electronic map, relative to using an electronic map alone. Specifically, when provided auditory guidance directions drivers spent more time with their eyes on the road and were able to better maintain their speed at or near the speed limit for the roadway traveled. In addition, drivers reported significantly lower workload levels while using the auditory system relative to the electronic map alone.

Although research supported using both auditory and visual guidance versus one modality alone, it could be argued that due to heavy visual demands of driving, the auditory modality should be used in lieu of visual formats when possible.

Auditory guidance is also frequently a preferred format relative to visual formats (Streeter et al., 1985). Streeter et al. (1985) reported that the auditory guidance format used in their experiment was rated easier to use than other formats. However, as mentioned previously, complex information such as spatial layouts is processed less efficiently when presented through the auditory channel relative to the same information conveyed visually. In fact, Walker, Alicandri, Sedney, and Roberts (1990) found that complex auditory instructions interfered with participants processing the instructions relative to less-complex instructions. The work of Walker and colleagues is frequently cited as a basis for current design guidelines, and at present auditory instructions are limited to terse commands. For example, a standard auditory instruction might be “turn left in two blocks.” This type of command corresponds to what is termed a “route” style of navigation (Jackson, 1998; Lawton, 1994), a style characterized by a serial progression of instructions. Route instructions get the individuals to their destination without necessarily facilitating the individuals’ formation of a thorough understanding of the area traveled.
#### **Voice Guidance Formats **

Many commercially available RGSs include a speech interface to provide turn- by-turn guidance to drivers. The speech interface typically consists of digitized prerecorded instructions, although some systems use synthesized speech (Burnett, 2000b). As discussed in a previous section, digitized speech is generally more acceptable to users than synthesized speech (Simpson & Marchionda-Frost, 1984); however, the trade-off is that digitized speech requires greater system memory; therefore, the number of instructions that can be issued tends to be fairly limited (Burnett, 2000b).

Standard voice messages most commonly include the direction of turn in an egocentered or driver forward field of view reference and the distance to the turn. A few systems also include the street name (Burnett, 2000b). For example, systems such as the Garmin StreetPilot 2720 and Magellan RoadMate 800 announce the direction and street name for each turn. Some systems even allow voice queries and provide answers in the form of speech output. The new Earthmate GPS LT-20 by DeLorme, which works in conjunction with a portable personal computer (PC), allows the driver to ask questions such as, “How far to my destination?” and receive a verbal reply from the system.

Recent research indicated that although not commercially available yet, enhanced auditory RGSs providing drivers with salient landmark information in addition to conventional route guidance instructions can significantly decrease the time required to learn novel routes and form cognitive maps in unfamiliar locations without disrupting driving performance (Reagan & Baldwin, 2006), as well as reduce navigational errors during the drive (Burnett, 2000a, 2000b).
### **Infotainment Systems **

In addition to the many emerging in-vehicle technologies previously discussed, a host of entertainment or infotainment systems is rapidly emerging on the market. Infotainment systems extend beyond the conventional audio devices for playing music (i.e., radios, CDs, and DVDs) and provide means of watching videos, checking e-mail, and surfing the Web. Bluetooth®-enabled systems allow motorists to sync their home computers and cellular phones with the car system. This means sharing contact lists, files, and other data between home, business, and mobile platforms.

Satellite or digital radio, also increasing in popularity, provides motorists with an extensive list of channels to choose from, including commercial-free music stations, talk shows, and weather reports. Recent advances allow users to record and play back songs on one radio (i.e., at home) for later use on another compatible system (i.e., in the car). The proliferation of auditory displays and devices that is found in modern automobiles is also present in medical care facilities.
## **SOUNDS IN MEDICAL CARE ENVIRONMENTS **

Advances in technology have greatly improved medical care and have simultaneously ushered in a multitude of new sounds (Donchin & Seagull, 2002; Loeb, 1993; Loeb & Fitch, 2002; Wallace, Ashman, & Matjasko, 1994). A number of sophisticated technologies, including numerous patient-monitoring devices, are available to provide health care workers with continuous information. Donchin and Seagull (2002) described recent trends in medical environments:

Each year, new devices were added—automatic syringe pumps, pulse oximetry and capnography, pressure transducers, and monitors—that were bigger and occupied more space. To increase safety, alarms were added to almost every device. Around the patient bed there are, at minimum, a respirator, a monitor, and an intravenous pool with two to ten automatic infusion pumps. ( p. 316)

The omnidirectional characteristic of auditory displays and alarms is particularly useful to health care professionals. Previous research indicated that health care workers frequently failed to detect changes in visual displays (Loeb, 1993). Auditory alarms, in conjunction with visual displays, increased both detection and recognition of physiologic changes in patient state (Loeb & Fitch, 2002). However, auditory alarms in medical environments are known to have a number of critical problematic issues. False alarms are extremely common (Tsien & Fackler, 1997), which can add to stress in both health care workers and their patients (Donchin & Seagull, 2002).

Before leaving our discussion of the sources of sound, we turn now to one more important use of sound to convey information: auditory assistive devices for the visually impaired. Many of the devices and auditory displays previously discussed can be used by all normal-hearing users. However, increased attention has been devoted in recent years to the development of auditory displays aimed at increasing the mobility and safety of visually impaired persons.
## **AUDITORY DEVICES FOR THE VISUALLY IMPAIRED **

A range of auditory assistive devices has been developed or is well under way in development to help visually impaired individuals. For example, auditory interfaces are being developed to assist blind people in navigating through hypermedia material, such as the auditory World Wide Web (Morley, Petrie, O’Neill, & McNally, 1999), and to learn algebraic problems (Stevens, Edwards, & Harling, 1997). For blind people interpreting tables and graphs, nonverbal sound graphs have been shown to result in significantly less workload, less time to complete tasks, and fewer errors than speech interfaces and less workload than haptic graphs used alone (Brewster, 2002).

Navigation systems for the blind have been developed for both obstacle avoidance and way finding. These systems are portable, wearable GPS-based navigation systems utilizing auditory interfaces such as spatialized speech from a virtual acoustic display (Loomis, Golledge, & Klatzky, 2001). Stationary acoustic beacons have also been developed that assist sighted individuals in conditions of low visibility but also have particular applications to assisting visually impaired individuals in auditory navigation (Tran, Letowski, & Abouchacra, 2000; Walker & Lindsay, 2006). Acoustic beacons are currently installed at many intersections with high traffic density in cities, such as Brisbane, Australia, to assist blind individuals at intersection pedestrian crossings. Although still needing further development, navigation systems for the blind and visually impaired show promise for facilitating mobility egress in emergency situations.
## **SUMMARY **

We have highlighted some of the myriad sounds present in our home and work environments. With the digital age, the number of auditory displays in the form of alerts and warnings as well as radio, telephone, and sonography has proliferated. The modern home and the modern flight deck, automobile, and medical environment are replete with sound.
## **CONCLUDING REMARKS **

To process the multitude of sounds present in the everyday world, humans must be able not only to hear but also to interpret the incoming stimulus. In the next chapter, we distinguish between these two processes and discuss how they are integrated, also describing the mechanisms involved. Readers well acquainted with these sensory-cognitive issues may wish to skim or skip the next chapter, moving directly to the issues of the mental workload assessment in Chapter 4 and then the mental workload of auditory processing beginning in Chapter 5 and continuing on through the remainder of the book.

This book is concerned with the mental effort (workload) humans expend to identify, select, and extract meaningful information from the acoustic milieu constantly bombarding the airwaves. What characteristics aid or deter from this process? Under what circumstances does auditory processing succeed, and when does it fail? How does the amount of effort extended toward processing auditory information affect performance of other tasks? These are just some of the many questions addressed in the chapters to come. As Plomp (2002) so eloquently put it, the ability of human listeners to

seek out which sound components belong together and to capture each individual sound with its associated characteristics … are so sophisticated that they have to be seen as an active process. They operate so perfectly, interpreting every new sound against the background of earlier experiences, that it is fully justified to qualify them as *intelligent *processes. (p. 1)

In the next chapter, we begin to examine some of the characteristics of this intelligent process.