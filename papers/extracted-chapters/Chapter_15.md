# Unknown Chapter

*Source: Auditory Cognition and Human Performance by C. L. Baldwin (2012)*

---

# 7 **Nonverbal Sounds and Workload **

## **INTRODUCTION **

Nonverbal sounds are an integral, if underappreciated, part of human experience. When we walk into a room, the echo and reverberations of our footsteps provide important clues to the nature of our environment. The sound of the wind reminds us to wrap up before going outside; sounds can alert us to oncoming vehicles and approaching friends. Nonverbal sounds complement, supplement, and under certain conditions supplant visual information.

Music represents one of the more popular forms of nonverbal sounds. Music perception is a significant cultural accomplishment and may play a fundamental role in many forms of human adaptation. One example is the role of music in facilitating parental commitment through infant bonding and promoting infant mood states associated with optimal growth and development (see Trehub, 2001). Our auditory systems are predisposed to efficient processing of universal musical patterns, suggesting that musical systems (e.g., conventional Western music) have developed to capitalize on efficiencies of the auditory system rather than vice versa. These ideas are explored in more detail in this chapter, which also examines the attentionenhancing role of certain musical structures and the distracting effects of unwanted sound. The chapter focuses on issues such as the mental effort involved in detecting and categorizing sounds, auditory imagery, music perception, noise, and the impact of both music and noise on performance.

In Chapter 3 , the topic of auditory stream segregation was introduced, including discussion of some basic properties of perceptual grouping that enable us to distinguish between multiple competing sources of sound. We continue that discussion here by focusing on many of the mechanisms used to organize nonverbal sounds in our environment.
## **AUDITORY PERCEPTUAL ORGANIZATION **

The auditory system uses a number of sophisticated perceptual organizational principles to make sense of the sounds around us, including identifying whether we are hearing one or two different sources and which sounds belong to which source. These principles assist in the process referred to as auditory stream segregation, discussed previously in Chapter 3 (and see Bregman, 1990, for a comprehensive review). These processes make use of regularities in the acoustic information and are briefly reviewed here.
### **Acoustic Regularity **

Most sounds outside the laboratory are complex: They are made up of a base or fundamental sound of a particular frequency and a complex blend of harmonics (multiples of the fundamental frequency). For example, a complex sound with a fundamental frequency of 200 Hz would also likely consist of harmonics at 400, 600, 800, and perhaps 1,000 Hz. We make use of these harmonics in segregating acoustic streams. Our auditory systems are capable of engaging in complex spectral analysis and are able to separate multiple concurrent frequencies into their respective sources, partly based on recognizing different harmonic groupings.

Alain, Arnott, and Picton (2001) demonstrated how harmonics are used to segregate auditory streams. They asked listeners to decide whether they were listening to one or two sounds. The sounds presented to listeners consisted of a fundamental frequency of either 200 or 400 Hz and then a series of 12 harmonics that were either all in tune (400, 600, etc.) or with a second harmonic that was mistuned by a small amount (1–16%). The proportion of listeners reporting that they heard two sounds (rather than one) increased as the percentage of mistuning increased. When the second harmonic was mistuned by at least 8%, the overwhelming majority of listeners reported that they were hearing two sounds. This experiment provided convincing evidence that spectral analysis of harmonics can be used to aid auditory stream segregation.

Stimulus onset time, location, and pitch are used in a similar manner. It is highly unlikely that sounds coming from unrelated sources will start and stop at the same time, much less be coming from exactly the same location (Bregman, 1993). Likewise, auditory stream segregation is aided by the fact that when a sound changes its frequency, all of the partials or harmonics of the sound change in the same way. Therefore, if two complex but overlapping series of sounds are being heard and their partials are changing pitch (becoming higher or lower in pitch) at different ratios, the two streams can be easily separated (Bregman, 1993). Pitch continuity or the tendency for these pitch changes to be fluent, gradual changes also aids auditory stream segregation.
## **AUDITORY OBJECT RECOGNITION **

Humans can determine through sound alone much about their environment, including the type of objects it contains and their actions. McAdams (1993) provided a vivid example. He asked us to imagine the sounds associated with a pile of ceramic dinner plates falling off a counter, “tumbling through the air knocking against one another, and finally crashing on to a relatively hard surface upon which all but one of the plates break—the unbroken one is heard turning on the floor and then finally coming to a rest” (p. 146). Forming an auditory image of this event was probably relatively easy for most of us. We are able to determine a surprising amount about an object by sound alone, including what it is: if it is a common object, its size, what it is made of, and its stability, for example.

Auditory object recognition has been studied from two primary theoretical approaches: information processing (Broadbent, 1958) and ecological (Gibson, 1966), with considerably more research investigations framed within the former (McAdams, 1993). Information-processing approaches assume that recognition occurs in a series of stages, beginning with basic acoustic analysis of stimulus properties and then culminating in later cognitive stages of recognition based on stored memory patterns. Ecological acoustics, conversely, assumes that our auditory systems are well adapted for directly perceiving the invariant acoustic information necessary for perception of relevant biologically significant or experientially salient events and objects. From an ecological perspective, it is not necessary to analyze sounds into their constituent parts and match them to memory; rather, the overall structure of the sound is perceived directly. Information-processing approaches assume a memory component.
### **Information-Processing Approaches **

Information processing approaches to auditory object recognition generally assume that recognition begins with analysis of the acoustic features or stimulus properties (McAdams, 1993; Molholm, Ritter, Javitt, & Foxe, 2004; Murray, Camen, Andino, Clarke, & Bovet, 2006), often referred to as data-driven processing. This initial stage includes the translation of information, such as the frequency, intensity, timbre, and duration of an auditory event into a neural code, a process referred to as *transduction *. Subsequent stages rely on auditory grouping, analysis of features, and the matching of auditory patterns with information stored in the auditory lexicon. Depending on the specific theory of prescription, these stages may or may not be interactive—meaning that higher-order stages may shape lower-level analysis and grouping in some theoretical accounts.

The ability to identify specific objects or states of objects is developed with experience. A trained mechanic learns to recognize the sounds of various automobile components: how they differ among different vehicles and whether a component is functioning properly. The layperson may or may not know what component is making the strange clicking or clunking sound when trying to describe it to the mechanic. Yet, this same person is demonstrating that he or she knows the sound is not a normal part of the auditory “soundscape” produced by the vehicle.

Auditory object recognition (for nonverbal sounds) has received considerably less attention than visual object recognition or speech recognition, but recently some important progress has been made. This research has sought to determine the features used to distinguish between different auditory objects, the extent to which auditory objects are processed like visual objects, and whether grouping auditory features into objects helps in their retention (Griffiths & Warren, 2004).

Dyson and Ishfaq (2008) suggested that object-based coding occurs for both auditory and visual stimuli. It has been known for some time that information in visual short-term memory tends to be stored in the form of objects rather than individual features (Duncan, 1984; Luck & Vogel, 1997). Luck and Vogel, for instance, found that we can store about four objects containing at least two features in memory or four individual features. This strongly suggests that visual short-term memory is object based. More recently, Dyson and Ishfaq observed a similar phenomenon with auditory information processing. People were concurrently presented with two different sounds and then asked about features present either within or across the different sounds. People were much faster at responding if the two dimensions in question were from the same object rather than different objects. Such an “object superiority” effect has often been observed in studies of visual perception and attention (Duncan, 1984). Rather than storing sound features independently, this suggests sound features are grouped into auditory objects in much the same way that visual features are stored as objects.

To answer basic questions (e.g., how good people are at recognizing everyday sounds, how long it takes to recognize them, and which features are used), Ballas (1993) examined listeners’ perceptual-cognitive ratings of everyday sounds with different acoustical properties. He compared these ratings to their identification responses and the naturally occurring frequency of each sound. Ballas observed that on average it took people roughly 1–7 s to listen to and identify everyday sounds such as a doorbell ringing, a toilet flushing, hammering, and bacon frying. He found that people use a combination of acoustic, perceptual, and cognitive characteristics to identify sounds. Sound identification accuracy was strongly associated with how long it took someone to identify a given sound (identification time). In general, the longer it took people to identify a sound, the less sure they tended to be of the source of the sound and in turn the less likely they were to be accurate. Identification accuracy was significantly associated with a number of bottom-up acoustic features of spectral-temporal properties, including the presence of harmonics, continuous bands, and similar spectral bursts. The presence of harmonics in continuous sound coupled with the number of spectral bursts in noncontinuous sound together was strongly correlated with both identification accuracy and identification time. Acoustic factors such as these could explain about 50% of the variance in identification time. Top-down factors such as familiarity, prior experience, or exposure ratings based on subjective and objective estimates of how frequently the sound is heard and estimates of how many different sources could make the sound were even stronger predictors of identification time. Together, four different aspects of a sound—its spectral, temporal, envelope (e.g., the ratio of burst durations to the total duration), and frequency of occurrence—could account for 75% of the variance in its identification time.

Identification accuracy and time were also associated with the number of possible sources of a given sound, which can also be termed its *source neighborhood *(Ballas, 1993). For example, a clicking noise could be caused by many different things and would tend to take longer for people to attempt to identify, and they would more often be inaccurate.

Aldrich, Hellier, and Edworthy (2009) examined the descriptive and acoustic features that are important to sound identification. Based on past literature, they examined the characteristics of loudness, spectral spread, bandwidth, and pitch, as well as familiarity, in people’s judgments of similarity and grouping of complex real-world sounds. They also compared two methodologies: paired comparisons and grouping methods. Considerable overlap was observed between the two methods. People tended to classify sounds based on categories consisting of the source or function of the sound (birds, animals, human nonverbal noises, etc.) somewhat more when using the grouping method than the paired comparison method. Use of acoustic and descriptive features was also observed for both methods. They found that root mean square (RMS) power (a measure of sound intensity) was a particularly important acoustic characteristic in sound classification.

Ballas (1993) found that the power of a sound was correlated not only with loudness but also with perceptual-cognitive characteristics such as hardness, angularity, sharpness, tenseness, and unpleasantness. In fact, strong correlations were found between a number of acoustic aspects and ratings of the perceptual-cognitive characteristics of a sound. For example, the degree of concentration of sound in the octave band centered at 2,500 Hz was strongly related to the relaxed versus tense rating of the sound. The amount of energy in frequencies above 3,150 Hz and between 1,100 and 2,500 Hz was strongly related to ratings on a dull-sharp dimension.

The strong relationship between familiarity with a sound and the speed and accuracy with which it can be named as well as the interaction between bottom-up and top-down factors in sound identification provide support for the informationprocessing approach to auditory object recognition. Still, others take an ecological approach to auditory object recognition.
### **Ecological Acoustics **

The ecological acoustic approach to auditory object recognition takes the different perspective that global symmetrical patterns are the key to recognition. Rather than analyzing individual elements, the complex, presumably invariant inherent symmetrical patterns are recognized directly (Casey, 1998; Gaver, 1993), consistent with a direct perception viewpoint of ecological perception (Gibson, 1966; Warren & Verbrugge, 1984). Two types or categories of listening are usually considered. The first one, and a primary focus for discussion here, is concerned with *everyday listening *—identifying the objects in our natural environment. This can be distinguished from musical listening or what Casey (1998) referred to as *reduced listening *. Note that reduced does not mean easier; it simply means “listening to inherent sound patterns, without regard to their causal identity” (p. 27).

VanDerveer (1979, unpublished data cited in Warren & Verbrugge, 1984) presented everyday sounds (i.e., jingling keys, footsteps) and asked listeners to recognize them. She noted a strong tendency for people to respond with a description of the mechanism making the sound. Acoustic characteristics were described only when the sound could not be identified. This observation can be taken as support for an ecological approach.

There is also some evidence that sounds that would typically require some type of action (i.e., like a ringing phone) activate different cortical regions than sounds that would not typically require action, like piano notes (De Lucia, Camen, Clarke, & Murray, 2009).

From an ecological perspective, more complex sounds are generally easier to identify. Complex sounds have a richer, more informative pattern. For example, imagine that you are trying to identify a noise coming from the next room. The more complex the sound pattern—meaning the more harmonics you are able to detect and the longer the pattern continues—the more likely you are to identify it. Along similar lines, radio stations sometimes play a game of “name that song,” playing only the first two or three notes of a song. The more of the song we hear, the more likely it is we will recognize the pitch contour and thus the song being played. The use of acoustical filters that clip or leave out too wide a range of frequencies results in speech that is difficult to understand. Thus, we see that in both information-processing and ecological acoustics explanations, auditory object recognition is facilitated by complexity, a rather counterintuitive phenomenon.

Another important form of nonverbal sound is music. Music processing has much in common with speech processing. It is a culturally rich, possibly biologically adaptive, form of sound used to convey information and emotion. At the same time, music may have dedicated neural circuitry separate from language processing. For example, based largely on patterns of musical abilities in neurologically impaired individuals, Peretz and colleagues (Hyde & Peretz, 2004; Peretz, 2003, 2006) suggested that separate modules may exist for processing lyrics versus language versus melodies. They observed patterns of disability in one of each of these areas within an individual with intact functioning of the other two. So, what specifically is music? The role of music in culture is considered first, and then attempts to answer this question are discussed.
## **MUSIC **

After speech, music is probably the most meaningful and cherished form of sound. Generations define themselves by the music they listen to and create. Couples associate musical pieces with budding romance, and people of all cultures and ages use music in rituals and celebrations and to give form to physical expression through dance. Although the average listener may not put much time or thought into asking how much mental effort is required to listen to a particular musical piece, considerable attention has been focused on music perception (e.g., see Deutsch, 1999; Krumhansl, 1990; Zatorre & Peretz, 2003). One area of investigation that has received considerable popular attention in recent years has been whether music plays a role in developing or increasing intellectual abilities. Phenomena such as the so- called Mozart effect or the notion that listening to particular Mozart compositions can temporarily increase spatial reasoning ability have received considerable attention in the popular media. In this section, we discuss the evidence, or lack thereof, for these claims.

First, we begin with an examination of some of the aspects involved in music perception and musical knowledge. Included in this discussion is a glimpse of the neural pathways and mechanisms involved as well as how we perceptually organize music into patterns for storage and subsequent recognition. This discussion sets the stage for gaining a deeper understanding of how sensory and cognitive processes interact during musical processing to affect performance and cognitive functions.

Music perception and speech perception share many commonalities. We develop a considerable knowledge base of musical information, or a musical lexicon, thought to be much like the mental lexicon used in speech processing. For many of us without or prior to formal musical training, this knowledge base develops implicitly. Much like grammar and syntax, children and adults without formal musical training will implicitly grasp musical rules that would be nearly impossible to express verbally. Both speech and music rely on continuously unfolding temporal sequences and the ability to hold sounds in memory until their patterns can be organized and comprehended. Furthermore, music and speech are both forms of communication and expression.

Music perception differs from other complex auditory processing (i.e., speech perception) in some important ways. For example, while speech perception relies heavily on temporal changes in broadband sounds, music perception depends more on the ability to discriminate slower, more precise changes in frequency (Zatorre, Belin, & Penhume, 2002). As discussed in Chapter 3 , Zatorre and his colleagues (2002) observed that temporal resolution is better in the left hemisphere, while the right hemisphere seems more specialized for spectral resolution (essential to music perception). However, pitch and melodic pattern processing make use of bilateral temporal cues (Griffiths et al., 1998; Zatorre, 1998). We return to the topic of musical pitch perception later in the chapter. First, we discuss some additional basic concepts of musical processing, beginning with a short description of what constitutes musical sound.
### **Music Defined **

What exactly is music? Like the concept of mental workload, discussed extensively in Chapter 5 and more generally throughout this book, everyone has some idea of what constitutes music versus random sounds (although certainly not everyone would agree on any particular composition). Does simply striking the keys on a piano, for instance, constitute music? What if that striking results from an inanimate object, such as a book or vase, falling on the keys? Most of us would agree that the resulting sound would not constitute music. Could Fido produce music by chasing the cat across the keys? Perhaps these illustrations border on the absurd, but the point is that music possesses something beyond mere sounds and beyond the type or quality of the instrument producing the sound. Structure is one key element of music. Musical scores represent highly organized structures or patterns of sound. Interestingly, there is virtually universal agreement regarding the variations to this pattern that are acceptable, just as there is general agreement on which sounds or notes can be played together to produce a harmonic chord. This general pattern of agreement transcends culture, class, and musical training. In fact, newborns seem to possess the same general preference for musical chords and patterns as adults. Much of this agreement appears to rely on our ability (albeit unconscious) to detect mathematical relationships between sounds, as was noted by the great Greek philosopher, Pythagoras ( Figure 7.1 ).

Pythagoras helped establish the first natural law based on this principle, that a mathematical relationship exists between pitch and the length of a vibrating string (see Ferguson, 2008). Musicians had been using stringed instruments, such as the lyre illustrated in Figure 7.2 , for centuries, realizing that sometimes they sounded pleasant and sometimes they did not. But, it was Pythagoras who was able to determine precisely why.

Pythagoras is rumored to have used a box with a single string stretched across to make mathematical calculations and to examine pitch relationships. This device, called a *monochord *( Figure 7.3 ), was really more of a scientific instrument than a musical instrument, but nonetheless Pythagoras noted that certain string ratios sounded well together, while others did not.

**FIGURE 7.1 **Pythagoras.

**FIGURE 7.2 Lyre **.

**FIGURE 7.3 Monochord **.

Some 2,000 years later, the relationship between frequencies and pitch became measurable. Most modern tunings now make use of the mathematical properties of frequencies to establish scales. For example, the Western scale has 12 notes, with each successive frequency the frequency of the previous note times a 12th root of 2 (which is approximately 1.059). For all major scales, the first note is the dominant, and the fifth note in the seven-note scale will be the tonic. A ratio of 2:3 exists between the frequencies of the dominant and tonic notes. So, for example, for the key of C major, the dominant starting note is middle C on a piano (with a frequency of 261.6 in the dominant Western tuning system), and the tonic will be G at a frequency of 391.9, with 261.6/391.9 representing roughly a 2:3 ratio.

It is well beyond the scope of this chapter to discuss these mathematical relationships and their relation to musical theory in detail. The reader interested in these topics is referred to excellent works by Deutsch (1999) and Temperley (2001) and Krumhansl (1990). See also an article by Trainor and Trehub (1993) for an accessible introduction to musical theory and discussion of the cycle of fifths in particular. For our present purposes, we are most concerned with the psychoacoustic effects of these mathematical relationships and examine some of the most important factors affecting musical perception and their potential to affect the difficulty (mental workload) of performing this processing task.
### **Musical Structure **

One of the psychoacoustic effects of the mathematical relationship within musical scores important for understanding musical structure is the concept of scales and octaves.
#### **Octaves **

Long before the relationship between vibration rate and frequency was understood, at least as early as the era of Pythagoras around 500 BC, the Greeks understood that musical notes within a given scale or key exhibited mathematical relationships. For example, in Western music, which is the dominant musical form of reference in this chapter, musical notes repeat in patterns of 12 notes within an octave. Notes are given letter names and are generally referenced starting with the middle C (or C4) on a piano. Seven major notes are represented with the letters C, D, E, F, G, A, and B, with five half steps residing between them (see Figure 7.4 ). The notes that can be played together to form a chord with *consonance *(pleasant harmony) rather than *dissonance *(an unstable sound that implies a need for resolution) have strict mathematical ratios. A ratio of 1:2 represents an octave, such as C4 and C5 (the C note exactly one octave above middle C). Refer to Figure 7.4 for a visual illustration. Other ratios resulting in consonance include 2:3 (called a fifth); 3:4, a fourth; and 4:5, a major third.

Much later, in the 17th century, it was discovered that musical pitch corresponded to the rate of vibration. According to Pierce (1999), Mersenne and Galileo made the discovery independently, with Galileo further suggesting that consonance could be explained by the “agreeable” pulses experienced when sounds were synchronized, such that, for instance, when hearing two notes separated by an octave, the tympanic membrane would experience two pulses corresponding to the high note for every one pulse experienced as a result of the low note. Pierce explained that Galileo’s position was a rhythmic theory of consonance. However, subsequent research examining the temporal resolution of sound (less than 1 ms when involving localization) did not provide support for the rhythmic theory.

**FIGURE 7.4 **Piano keyboard with note names and frequencies.

**FIGURE 7.5 **Musical notation that maintains contour though played with different notes. (Drawn by Melody Boyleston.)
#### **Contour **

If the mathematical relationship between notes used to form chords exhibiting consonance was not evidence enough that music is highly structured, perhaps the topic of musical contour will suffice. *Contour *refers to the overall pattern of notes (i.e., pattern of up and down pitches) within a musical piece. Familiar melodies are recognized more by their pattern or contour than by their key or the absolute pitches involved (Edworthy, 1985). Think of a familiar song, such as the children’s song, “Twinkle, Twinkle Little Star.” Figure 7.5 presents a simple musical notation of the melody from this song. Note that the contour in Figure 7.5 is the same even though the individual notes are completely different. Both melodies are easily recognized as the familiar childhood song, and in fact numerous other variations can be made and recognized as long as the basic contour remains unchanged.
### **Musical Pitch Perception **

Musical pitch perception differs from acoustical pitch perception. It involves creating the perception of octaves, harmonies, tension buildup and release, and a host of additional properties not found in nonmusical sound. Pitch information, critical to musical processing, is obtained through spectral (frequency) and temporal processing, both early on (at the level of the cochlea) and in cortical areas in and around the primary auditory cortex (A1). Processing of this information can be explained by two different theories of pitch perception, referred to as the place and time code of pitch perception, for spectral and temporal changes, respectively. Griffiths and colleagues (1998) were able to demonstrate the importance of temporal information by examining pitch and melodic pattern perception in stimuli void of spectral information. The unique type of stimuli utilized by Griffiths (p. 426) was “add-same iterated rippled noise (IRNS),” presented through headphones. As Zatorre (1998) described in his discussion of Griffiths’ investigation, the stimuli are an outgrowth of a relatively old observation by Huygens, who noted that “the periodic reflections of the noise made by a fountain from the stone steps of a staircase resulted in an audible pitch” (p. 343). This type of perceptible pitch modeled by Griffiths et al. in the lab is generated solely on the basis of analysis of the temporal structure of the sound. It contains no actual spectral cues. Therefore, any pitch perceived must be a result of the analysis of the time code. This observation corresponds nicely with frequency theories of pitch perception.

As a general tutorial, the two dominant theories of pitch perception, once rivals but now recognized as complementary, suggest that our ability to perceive pitch is based on the timing of neural firing and thus is termed the temporal or timing theory. The location along the basilar membrane where maximal stimulation occurs also plays a key role and thus is called the place theory.

We now know that tonotopic organization aids pitch perception in both the cochlea (see review in Pierce, 1999) and the auditory cortex (Liegeois-Chauvel et al., 2003; Qin, Sakai, Chimoto, & Sato, 2005; Rauschecker, Tian, & Hauser, 1995). The relatively early observation that different frequencies result in different patterns of vibration along the basilar membrane led to what was termed the place theory of pitch perception, originally postulated by Helmholtz (1863/1930) and later demonstrated by Bekesy (1960). A major problem for the place theory has always been the issue of the missing fundamental (see discussion in Zatorre, 2005). Recall that despite the presence of multiple harmonics, we perceive the pitch of a sound to be the lowest common denominator of the harmonic frequencies, called the fundamental frequency. In the situation of the missing fundamental, we perceive the fundamental frequency of a set of harmonics even if there is no acoustical power at that frequency (Moore, 1995). Organ makers are said to make use of this auditory illusion to give the perception of extremely low notes that would require organ pipes too large for the accompanying building structure. They simply present the illusion of the lower note by simultaneously playing multiple harmonics of the missing note. (The higher frequencies require shorter pipes.)

An alternative theory that could better account for the phenomenon of the missing fundamental frequency was the timing theory, dating at least to August Seeback in the 1840s (Goldstein, 1973). According to frequency theory, pitch perception occurs in more central processing mechanisms and is a reflection of periodic fluctuations of patterns of neural firings traveling up the auditory nerve. Specifically, patterns in the rate of firing stemming from different harmonics become phase locked (firing at rates of peak compression) for rates up to about 4,000 Hz. This phase-locked pattern or rate of temporal firing is perceived as the fundamental frequency of the sound. It is now widely recognized that pitch perception, essential to both music and speech perception, relies heavily on this temporal coding of frequency that occurs in central processing mechanisms (see review in Moore, 1995).

Now that some of the basic issues regarding what constitutes music and how it is perceived are described, we briefly discuss how we acquire musical knowledge, followed by an examination in greater depth of the impact that music has on performance.
### **Musical Knowledge **

Implicit musical knowledge consists of knowledge of the relationships between sounds and sound dimensions (such as tonal hierarchies, keys, and patterns of durations) as well as knowledge of musical forms (such as sonatas or gap-fill melodies) that listeners may distinctly recognize even if they are unable to name them (Bigand, 1993; McAdams, 1989). Knowledge of different musical forms is typically learned as part of the acculturation process even in the absence of formal musical training. However, a number of music universals appear to be present in the newborn before any acculturation has taken place.
#### **Music Universals **

Newborns and infants have a number of musical abilities, which can be thought of as music universals. For example, infants appear to recognize similarities and differences in melodic contours between two melodies separated by as much as 15 s or with an intervening series of distractor tones (see review in Trehub, 2001). Recognition is established through an ingenious procedure developed by Trehub and her colleagues. Generally, the infant is seated on his or her parent’s lap while listening to stimuli. Infants are “trained” to turn toward a Plexiglas screen located at a 45° angle within a specific period of time if they notice a particular auditory change or event. The experimenter then displays a brightly colored toy for a brief period of time to reinforce correct responses. Through this paradigm, Trehub and colleagues have been able to learn a lot about the kinds of auditory stimuli that infants can distinguish. Other investigations of infant musical perceptions have involved preferential looking time (the time after habituation that an infant chooses to look at a particular stimulus object). Pitch contour plays a prominent role in the infant’s ability to distinguish between familiar and novel musical patterns. It is also likely to be a key component of “motherese” or “parentese”—the form of infant-directed speech used by parents and other adults to communicate with preverbal children (Kuhl, 2004).

Infant-directed speech consists of melodic patterns, higher fundamental frequencies (F0), and more exaggerated intonations and prosodic features than adult-directed speech. Adults are able to recognize major themes of infant-directed speech (prohibition, approval, comfort, and attention) more accurately than the intent of adult-directed speech even when the linguistic or semantic cues have been removed (Fernald, 1989) or when presented in a language completely unfamiliar to the listeners (Bryant & Barrett, 2007). Since they are not able to use semantic cues, they must rely solely on nonverbal cues such as pitch contours, intensity change, and intonations.

Infants also seem to be able to detect the difference between consonant and dissonant musical patterns, demonstrating a preference for consonant patterns. For example, Zentner and Kagan (1998) observed that infants as young as 4 months of age demonstrated preferential looking and less agitation (fretting and turning away) for consonant melodies versus dissonant melodies.

Newborn infants also appear to have an innate ability to detect rhythmic beats. Sleeping infants notice a missing beat in a rhythmic sequence, as evidenced through an event-related potential (ERP) paradigm (Winkler, Haden, Ladinig, Sziller, & Honing, 2009). The infants, ranging in age from 2 to 3 days old, demonstrated greater mismatch negativity (MMN) for deviant rhythmic sequences relative to standard tonal sequences. The MMN (described in Chapter 3 ) is an ERP component that is thought to represent the automatic recognition by the brain of a significant difference between successively presented stimuli.

The ability to move in time to music and to detect relative pitch relationships may not be solely a human ability. Evidence suggested that some nonhuman species demonstrated intriguing capabilities in these areas (i.e., see Patel, Iversen, Bregman, & Schulz, 2009). Studies have even found that some nonhuman animals (i.e., cotton-top tamarins) could distinguish between consonant and dissonant tonal stimuli (McDermott & Hauser, 2004). But, unlike humans, the tamarins showed no preference between the two and in fact, given the choice, would prefer quiet to music.

The musical abilities of infants, children, and adults far surpass those of other species. Humans are easily able to recognize a wrong note in a familiar melodic sequence, or even in an unfamiliar sequence if it conforms to their cultural expectations. Even individuals with no formal musical training can often remember hundreds, if not thousands, of melodies and songs and can easily tap in time to nearly any musical rhythm (for a review, see Trehub & Hannon, 2006). Since humans seem so well adapted to musical abilities, it seems reasonable to ask what impact music might have on performance. Does listening to music enhance or degrade performance on other tasks?
## **MUSIC AND PERFORMANCE **

Numerous researchers have examined the influence of music on performance. We begin with a discussion of the effects of background music in general and its impact on both physical and cognitive performance and then turn the focus to empirical investigations of the phenomenon known as the Mozart effect, which has attracted widespread attention in the popular media.
### **Background Music **

Numerous investigations have examined the effects of background music on performance (Furnham & Stanley, 2003). Vocal music is generally found to disrupt performance more than instrumental music (Furnham & Stanley, 2003; Salame & Baddeley, 1989), a finding that can be attributed to the irrelevant speech effect discussed previously. However, the performance effects of instrumental music are less clear. Music may increase morale and may work as an incentive to increase productivity for workers. However, much appears to depend on the type of work performed as well as the music played. Minimally, a distinction must be made between mental or cognitive work and physical work. The impact of music on physical performance has received considerable attention in the literature on sport psychology and exercise. Evidence pertaining to the impact of music on factory work and cognitive task performance is even more extensive, dating at least to the early 1930s (see review in Kirkpatrick, 1943). We begin with a discussion of the more recent studies on physical performance and exercise and then discuss cognitive performance.
### **Physical Performance **

The impact of music on physical performance and work productivity has been of interest for some time. Music is often, although not always, found to have an ergogenic effect: It seems to increase the psychophysiological capacity to work while reducing feelings of fatigue and exertion. The effects of music have been of interest to sports psychologists and exercise scientists. Music may directly or indirectly enhance physical performance during exercise (Edworthy & Waring, 2006). Perceived exertion is frequently found to be lower when exercisers are listening to music rather than silence (Karageorghis & Terry, 1997). Likewise, when subjective effort is controlled such that people are performing at a level they perceive to be the same exertion level, they perform significantly more physical work when listening to music than when they are not (Elliott, Carr, & Orme, 2005; Elliott, Carr, & Savage, 2004; Karageorghis & Terry, 1997). In other words, there seems to be a scientific basis for why so many people like to listen to music in the gym. Listening to music may increase the pace of workouts while leaving the exerciser feeling like he or she has exerted less effort.

Synchronous music appears more likely to result in beneficial effects than asynchronous background music. Synchronous music is generally always found to have some beneficial effects, but the effects are generally limited to exercise within the submaximal range (see review in Anshel & Marisi, 1978; Karageorghis & Terry, 1997). Motivational music, which may be either synchronous or asynchronous, appears to result in improved affect, reduced ratings of perceived exertion (RPEs), and improved posttask attitudes (see review in Elliott et al., 2005). As Elliott et al. pointed out, positive affect during exercise and postexercise plays a key role in determining the likelihood that an individual will repeat and maintain a pattern of exercise behavior. However, determining precisely what constitutes “motivational” music has been problematic. Oudeterous music—music that is neither motivational nor nonmotivational (Karageorghis, Terry, & Lane, 1999)—has sometimes been observed to have effects similar to music classified as motivational (Elliott et al., 2005).

Karageorghis et al. (1999) developed a theoretical framework for categorizing the motivational characteristics of music. They proposed a four-factor hierarchical framework: (a) rhythm response, (b) musicality, (c) cultural impact, and (d) association. *Rhythm response *, the most important characteristic in predicting the motivational impact of a particular piece of music, refers primarily to the tempo of the piece in terms of beats per minute (bpm). *Musicality *refers to aspects more traditionally considered musical aspects, including things such as pitch-related elements, harmony and melody. Cultural impact refers to the popularity or pervasiveness of the piece in society; the final, least-important, motivational aspect is the associations the musical piece has for the particular listener. **TABLE 7.1 Original BMRI and Instructions **

**Not at All Motivating **

**Extremely Motivating **

1. Familiarity

1

2

3

4

5

6

7

8

9

10

2. Tempo (beat)

1

2

3

4

5

6

7

8

9

10

3. Rhythm

1

2

3

4

5

6

7

8

9

10

4. Lyrics related to physical activity

1

2

3

4

5

6

7

8

9

10

5. Association of music with sport

1

2

3

4

5

6

7

8

9

10

6. Chart success

1

2

3

4

5

6

7

8

9

10

7. Association of music with a film or video

1

2

3

4

5

6

7

8

9

10

8. The artist/s

1

2

3

4

5

6

7

8

9

10

9. Harmony

1

2

3

4

5

6

7

8

9

10

10. Melody

1

2

3

4

5

6

7

8

9

10

11. Stimulative qualities of music

1

2

3

4

5

6

7

8

9

10

12. Danceability

1

2

3

4

5

6

7

8

9

10

13. Date of release

1

2

3

4

5

6

7

8

9

10

Karageorghis et al. (1999) turned this four-factor theoretical structure into a 13-item scale they called the Brunel Music Rating Inventory (BMRI). It has been the most widely used method to date of determining the extent to which a piece of music is motivational within the context of exercise (Elliott et al., 2004, 2005). The BMRI has been used by both researchers and exercise professionals (i.e., aerobics instructors) alike. Recent revisions (now called the BMRI-2) have resulted in improved psychometric properties and easier application by nonprofessional exercisers (Karageorghis, Priest, Terry, Chatzisarantis, & Lane, 2006) and colleagues. Table 7.1 presents the original BMRI along with instructions, and Table 7.2 presents the revised BMRI-2 version.

In sum, researchers such as Karageorghis and colleagues have concentrated on developing methods of systematically categorizing musical pieces to facilitate empirical research examining the effects of music on exercise. Additional empirical methods may help to disambiguate the equivocal results of previous investigations. Other efforts have been directed at understanding why music has an impact on exercise.
### **Theories of the Relationship of Music to Exercise **

Both empirical and anecdotal evidence indicated that fast, loud music seems to enhance exercise performance. Why might this be so? Several different theories have been proposed over the years for the positive influence of music on physical performance. One leading theory is that listening to music simply takes one’s mind off the negative aspects of the work (Anshel & Marisi, 1978; Karageorghis & Terry, 1997). More specifically, the act of listening to music requires part of the person’s limited attentional capacity, leaving fewer resources to be devoted to paying attention to signs of physical exertion. As discussed in previous chapters, the idea that humans have a limited amount of attentional resources is long standing and widely held (Broadbent, 1958; Kahneman, 1973; Wickens, 1984, 2002). The attentional resource approach suggests that listening to music has an indirect benefit by literally taking the focus off physical activity, leaving less time for the participant to devote resources to think about and therefore experience feelings of fatigue and exertion. **TABLE 7.2 Revised BMRI-2 **

**Strongly Disagree **

**In Between **

**Agree **

1. The rhythm of this music would motivate me during exercise

1

2

3

4

5

6

7

2. The style of this music (i.e., rock, dance, jazz, hip-hop, etc.) would motivate me during exercise

1

2

3

4

5

6

7

3. The melody (tune) of this music would motivate me during exercise

1

2

3

4

5

6

7

4. The tempo (speed) of this music would motivate me during exercise

1

2

3

4

5

6

7

5. The sound of the instruments used (i.e., guitar, synthesizer, saxophone, etc.) of this music would motivate me during exercise

1

2

3

4

5

6

7

6. The beat of this music would motivate me during exercise

1

2

3

4

5

6

7

A second theoretical explanation is that music directly affects psychomotor arousal level (Karageorghis & Terry, 1997). Music may be used to increase the intensity and tempo of the arousal system in preparation for exercise or calm an overanxious person before an important competition. Karageorghis and Terry pointed out that while this appears to be a popular position held by sports psychologists, there has been relatively little empirical attention given to it. The perception that music has an impact on arousal level may result primarily from learned associations between a particular piece of music and a feeling state.

A third explanation for the influence of music on physical state is that people have a natural tendency to respond to the temporal characteristics of the music and thereby synchronize their movement with the temporal beat of a piece. This position is also lacking empirical research, according to Karageorghis and Terry (1997).

It has been suggested that people tend to prefer musical tempi within the range of their heart rate (see discussion in Karageorghis, Jones, & Low, 2006). However, empirical research indicated that the relationship between musical tempo and preference is somewhat more complex. Karageorghis, Jones, and Low (2006) observed that people’s preference for tempo depends on their heart rate at any given time. When they are engaged in low or moderate rates of exercise (with corresponding low to medium heart rates), they exhibit a preference for medium- and fast-paced music.

When they are engaged in more vigorous exercise resulting in a fast heart rate, they prefer fast-paced music.

Edworthy and Waring (2006) observed that people exhibited faster treadmill speeds and faster heart rates when listening to fast (200-bpm) versus slow (70-bpm) music. However, for treadmill speed this effect interacted with loudness of the music over time. That is, listening to fast music had little effect on treadmill speed over time if the music was played quietly (60 dB). But, if the fast music was played loudly (80 dB), treadmill speed increased over a 10-min period, although loudness had no impact on heart rate.

In summary, music is generally found to enhance physical performance during exercise either directly or indirectly. Indirectly, music may help keep an exerciser’s mind off the physical and often-negative aspects of the workout, thus leading to increased work output at lower subjective levels of perceived exertion. Directly, fasttempo music may help an exerciser achieve a quicker pace and sustain that pace longer. Next, we focus on the impact of music on cognitive task performance.
## **MUSIC AND COGNITION **

The long history of research into the impact of background music on the performance of cognitive tasks has met with contradictory and often-controversial results. Music has been found to improve and decrease performance; to distract, annoy, and enhance affective mood; along with a host of other contradictory conclusions. Music can arouse or calm the central nervous system, helping to keep people awake or lull them to sleep. It seems that the impact of music on performance depends on a number of factors, including the type of music (i.e., fast tempo or slow), whether it is played at high or low intensities, the type of task being performed, as well as individual differences in listener characteristics (Day, Lin, Huang, & Chuang, 2009).

In general, music is often found to improve performance of monotonous tasks with low cognitive demands—helping people perform these tasks for longer durations and to feel better about them while doing them (Furnham & Stephenson, 2007). These results are in line with the theory that music facilitates arousal and thus facilitates short-term increases in attentional resources. Conversely, for more cognitively demanding tasks, like reading comprehension and prose recall, music has more often been found to decrease performance, to increase feelings of distraction, or both (Furnham & Strbac, 2002). Historically, interest in the effects of background music centered around productivity and job satisfaction among factory workers (Kirkpatrick, 1943). This area of research is still important today as the use of personal electronics increases, making music accessible in nearly any environment and as a new generation enters the workforce who are used to music on demand (Day et al., 2009).
### **Music in Industry **

Music in industry is, in fact, the title of one of the articles Kirkpatrick published in 1943. There is a strong relationship between music and work that pre-dates electronic forms of music. In preindustrial times, workers sang as they performed their tasks, and the pace of the work informed the pace of the song (Korczynski & Jones, 2006). As Korczynski and Jones pointed out in their review of the history of music in British factories, as work was brought into factories, singing was generally discouraged, and sometimes workers were even fined or otherwise punished for it. Singing was categorized as a leisure-time activity, and work time was sharply distinguished from it. World War II changed this view.

During World War II, efficiency and satisfaction among factory workers was of paramount importance. Fredrick Taylor’s Hawthorne studies in the 1920s had demonstrated that subjective factors and perceptions on the part of workers contributed significantly to worker productivity (Kirkpatrick, 1943). A 1943 report in the London journal, *Conditions for Industrial Health and Efficiency *, indicated that worker absenteeism had increased dramatically during the war, nearly tripling among women. Providing music to alleviate fatigue and boredom was seen as one way of combating the absenteeism issue (Unknown, 1943).

Kirkpatrick (1943) and others would attempt to demonstrate empirically that providing music in the workplace could improve morale, relax tensions, and reduce boredom.

Wyatt and Langdon (1938, as cited in Kirkpatrick, 1943) had reported the results of a quasiexperimental design demonstrating productivity increases of up to 6% among factory workers when listening to music for 30- and 45-min periods relative to their normal sound conditions. Other accounts of increased productivity resulting from piped-in background music were even higher.

In this spirit, the British Broadcasting Company (BBC) began airing a program called *Music While You Work *. The program was intentionally aimed at being intellectually accessible to the general public and to provide aesthetically pleasing background music for workers engaged in factory jobs that were all too frequently tedious and monotonous (Korczynski & Jones, 2006). BBC research indicated that by 1945 at least 9,000 factories were broadcasting the program. The music had to be rhythmical and nonvocal; could not be interrupted by announcements; and had to maintain a consistent volume to overcome the noise present in the factory environments (Korczynski & Jones, 2006).

Research available at the time suggested that this type of music did indeed improve productivity and morale among factory workers with largely monotonous, cognitively simple jobs (Kirkpatrick, 1943). The question remained: What impact would music have on more cognitively complex tasks?

Young and Berry (1979) examined the impact of music along with other environmental factors, such as lighting, noise, and landscaping, on performance of realistic office tasks. The office tasks were much more complex than those used in many previous studies. They involved decision making, design, and forms of creative work. Although it was difficult to assess performance outcomes quantitatively, workers expressed a definite preference for music versus no music. The one exception to this is when music was combined with loud background noise. Adding music to the already-noisy background accentuated the undesirable effects of the noise.

Similarly, Oldham, Cummings, Mischel, Schmidtke, and Zhou (1995) found that employees who were allowed to use headsets during a 4-week quasiexperimental investigation improved their performance and had higher organizational satisfaction and better mood states. These findings were particularly strong for workers in relatively simple jobs. However, it is important to note that participants in the Oldham et al. investigation all came from a pool of employees who had expressed an interest in listening to personal music systems at work. In a more controlled laboratory study, Martin, Wogalter, and Forlano (1988) found that background music consisting of an instrumental jazz-rock piece did not improve or disrupt reading comprehension.

A wealth of research has now been conducted to examine the impact of music on cognitive performance. At least two dominant but conflicting theories have been proposed to explain the results of these investigations. Focusing on the influence that music has on attention, Day et al. (2009) aptly referred to these different postulated roles as the distractor versus the arousal inducer. Basing these positions on the classic model of attention presented by Kahneman (1973), music could play the role of distractor, requiring mental resources to process in an obligatory way and thus have a tendency to be a detriment to performance. Conversely, but using the same model, music could induce arousal and therefore temporarily increase the level of available resources, thus having a tendency to improve performance. Both of these views have received support in the literature (Beh & Hirst, 1999; Crawford & Strapp, 1994; Furnham & Strbac, 2002; Jefferies, Smilek, Eich, & Enns, 2008). As mentioned previously, the impact of music appears to depend on a number of factors, including characteristics of the music itself (i.e., tempo and intensity); the listener (i.e., introvert vs. extravert); and the task (i.e., type of working memory resources required and difficulty level). A complete list or discussion of this area of research is well beyond the current scope. Instead, illustrative examples of some of the key findings are presented.
### **Intensity **

Music presented at relatively low-intensity levels (i.e., ~55–75 dBA) has been shown to have a positive impact on nonauditory tasks requiring sustained attention or vigilance, such as visual detection tasks (Ayres & Hughes, 1986; Beh & Hirst, 1999; Davies, Lang, & Shackleton, 1973). For example, Beh and Hirst investigated the impact of low- (55-dBA) and high- (85-dBA) intensity background music on a visual reaction time task, a central and peripheral visual vigilance task, and a tracking task designed to simulate some aspects of driving. Participants performed all three tasks at the same time (the high-demand condition) or individually (low-demand condition). Relative to no music, low-intensity music improved performance of the visual reaction time task and responses to both central and peripheral targets in the vigilance task while having no impact on the tracking task. The pattern of results for the higher-intensity music was more complex.

The higher-intensity music in Beh and Hirst’s (1999) investigation improved performance in the visual reaction time task, relative to the no-music condition, to the same degree as the low-intensity music. That is, both low- and high-intensity music reduced response time in the visual reaction time task. High-intensity music also reduced response time for central targets in the vigilance task, again similar to the low-intensity music and relative to the no-music condition. Performance differences between high- and low-intensity music, however, were found for the peripheral targets in the vigilance task. In both single- and dual-task conditions (referred to as low and high demand, respectively, by Beh and Hirst), participants demonstrated a vigilance decrement over the 10-min task interval in both the no-music and the high-intensity music condition. Only the low-intensity music condition resulted in stable performance across the task interval.

Using similar task combinations, Ayres and Hughes (1986) found that neither low-intensity (70-dBA) nor high-intensity (107-dBA) music affected performance on a visual search task and a pursuit tracking task. However, the high-intensity music impaired performance on a visual acuity task. From these results, and those of Beh and Hirst (1999) and others, we can tentatively conclude that music presented at low intensities will generally facilitate performance on most tasks. But, higher intensities facilitate performance for some tasks, but for other types of tasks, high-intensity music has either no impact or a detrimental impact on performance. Contradictory results have been found, even when using similar tasks. For example, Turner, Fernandez, and Nelson (1996), using a visual detection task resembling a parttask driving simulation, found that background music played at 70 dBA improved detection performance, while music played at a lower intensity (60 dBA) or higher intensity (80 dBA) decreased detection performance. The equivocal results of these investigations may be due to the use of music with different tempos.
### **Tempo **

The tempo of a musical piece, often measured in beats per minute (bpm), has an impact on heart rate (Bernardi, Porta, & Sleight, 2006), motor behavior, and cognitive performance and may interact with intensity. North and Hargreaves (1999) compared fast, loud music (140 bpm at 80 dB) to slow, low-intensity music (80 bpm at 60 dB). Performance on a visual motor speed task (simulated racing) was impaired by both a concurrent task load (counting backward by threes) and by listening to fast, loud music. They found that the combination of having to perform the concurrent tasks while listening to the fast, loud music was particularly detrimental to speed maintenance, relative to performance when listening to the slow, low-intensity music. From this, North and Hargreaves concluded that listening to the music and completing the concurrent tasks competed for a limited processing resource.

North and Hargreaves (1999) also noted that musical preference was correlated with performance in the tasks. People liked both pieces of music more when they were not also performing the backward counting task (which also corresponded with when their speed maintenance performance was best). People who listened to the fast, loud music while performing in the demanding concurrent task conditions reported liking it the least, and their performance tended to be the worst. Highest musical preference ratings were obtained in the conditions for which performance was best (the slow, soft music in the single-task condition). These findings corresponded with previous work by North and Hargreaves (1996) and provided support for a link between context and musical preference (Martindale, Moore, & Anderson, 2005; Martindale, Moore, & Borkum, 1990; North & Hargreaves, 1996; Silvia & Brown, 2007). However, because North and Hargreaves manipulated both intensity and tempo at the same time, it is impossible to tease out the relative contribution of each factor.

Brodsky (2001) examined heart rate and performance of participants on a simulated driving task (presented via a commercially available racing game) while they listened to music of three different tempos. Music at a slow tempo (40–70 bpm), medium tempo (85–110 bpm), and fast tempo (120–140 bpm) was played at a consistent intensity level (85 dBA) while participants drove their simulated vehicle through a 90-min course of daylight driving conditions involving both municipal and interstate roadways. In Brodsky’s first experiment, no effects were observed as a function of music condition for average heart rate or heart rate fluctuation or for average speed or lane deviations. However, significantly more participants ran through red lights while listening to the fast music (55%) relative to the no-music or slow music conditions (20% and 35%, respectively). In a second experiment in which Brodsky took the speedometer display away from the participant’s view, driving speed also increased with increases in music tempo. Faster music was also associated with greater lane deviation, red light violations, and collisions in this second experiment. (Anecdotally, I can relate to this result. My first and only speeding ticket was received in my early 20s just after I had purchased a new CD of music involving a fast tempo. I will not mention its name, but I was playing it quite loudly and apparently not paying much attention to my speedometer when I saw the flashing red lights behind me.)

Relatively few studies have reported examinations of the impact of music tempo on cognitive performance. Those that have contained equivocal results, sometimes within the same investigation (Mayfield & Moss, 1989). For example, Mayfield and Moss reported two investigations in which undergraduate business majors performed mathematical stock market calculations while listening to either no music, slow- tempo music, or fast-tempo music. In their first experiment, they found no effects of music condition on either the accuracy or the speed at which the mathematical calculations were performed. In their second study, using a larger sample, they found that fast-tempo music resulted in a higher level of performance in the calculation task but was also perceived as more distracting than the slow-tempo music.

In a controlled field study, musical tempo had no effect on the time supermarket shoppers took to select and purchase their groceries or on how much money they spent (Herrington & Capella, 1996). However, this study found that the more shoppers liked the music, the longer they tended to shop and the more they tended to spend. Musical preference was not related to either tempo or intensity in their investigation. Milliman (1982) used a more stringent pace criterion involving determining the time it took supermarket shoppers to move from Point A to Point B under the conditions: no music, slow-tempo music, or fast-tempo music. Shoppers exposed to the slow-tempo music exhibited a slower pace than those shoppers exposed to fast-tempo music. A nonsignificant trend ( *p *<. 079) indicated that the slow-tempo music tended to result in a slower pace than the no-music condition as well. The slow-tempo music also resulted in significantly higher sales figures relative to the fast-tempo music. Interestingly, Milliman observed these results despite the fact that the majority of a random sample of shoppers leaving the store indicated they were not even aware of the music. Of those who did notice the music, no differences in preference were observed between the two types of music.
### **Individual Differences **

There also appear to be differences in the effects of music on cognition that are based on individual characteristics. For example, Furnham and Stanley (2003) observed performance impairments when introverts performed a phonological task and a reading comprehension task while listening to instrumental music. Similar results were obtained for extraverts in the phonological task. That is, instrumental music significantly degraded performance on the phonological task for both introverts and extraverts. However, for the reading comprehension task, extraverts performed just as well in the presence of instrumental music as in a silent condition, whereas vocal music significantly disrupted their performance.

In another study, introverts and extraverts performed an immediate or delayed recall task and a reading comprehension task in the presence of either silence or pop music (Furnham & Bradley, 1997). The music had a detrimental effect on immediate recall in both groups, but the introverts experienced significantly more performance impairment relative to extraverts on both the delayed memory and reading comprehension tests.

People can be aware of the impact of music on their performance. When asked about how they used music in their everyday lives, people with different personality types reported using music in different ways (Chamorro-Premuzic & Furnham, 2007). Introverts were more likely to report using music to regulate their mood, while extraverts were more likely to report using it as background. This observation was true for a sample of participants taken from British and American universities (Chamorro-Premuzic & Furnham, 2007) as well as a sample of students from Barcelona (Chamorro-Premuzic, Goma -i-Freixanet, Furnham, & Muro, 2009). In the Barcelona sample, Chamorro-Premuzic et al. also noted that extraverts were more likely to report using music as a background to other activities than were introverts.

It remains to be seen whether people have learned to use music to enhance their mood states, arousal levels, and overall performance or whether their existing states and levels simply reflect both their underlying personality and musical preferences. The current understanding indicates that the impact of music on performance is a complex interplay between the characteristics of the music, such as loudness and tempo, as well as both the activity being performed and characteristics of the listener. What about the impact of different types of music on specific abilities? Is there any validity to the idea that listening to certain types of music, like Mozart compositions, can improve specific forms of reasoning or intelligence?
## **THE MOZART EFFECT **

In 1993, Rauscher, Shaw, and Ky reported the results of a study in the journal *Nature *that sparked considerable discussion among scientists and educators and in the popular media. They had college students listen to 10 min of Mozart’s sonata for two pianos in D major (K488) (see Figure 7.6 ), 10 min of a relaxation tape, or silence. Results of the study by Rauscher and colleagues are shown in Figure 7.7 . As illustrated, those who listened to the Mozart piece scored several standard age points higher on a spatial-reasoning component of the Stanford-Binet intelligence test, relative to the other two listening groups (Rauscher et al., 1993). This increase in IQ was roughly equivalent to an increase of 8–9 standard IQ points.

Rausher and colleagues observed that the effect only lasted for the 10–15 min that it took to complete the spatial task. The story was picked up by a *Boston Globe *reporter, who termed it the “Mozart effect,” which has since been become a household name and widely known phenomenon, although largely a misunderstood one (Rauscher & Hinton, 2006). It has often been confused with the general statement that “music makes you smarter” (Schellenberg, 2003, p 432).

**FIGURE 7.6 **Opening bars of Mozart’s K488.

**FIGURE 7.7 **Mozart effect IQ results. (From Rauscher, F. H., Shaw, G. L., & Ky, K. N. Music and spatial task performance. *Nature, 365 *, 611, 1993.)

As Rauscher and others have since argued, (Nantais & Schellenberg, 1999; Rauscher & Hinton, 2006), several points of clarification are warranted. First, the improvement in spatial-temporal abilities reported by Rauscher and her colleagues was considered to be very short lived (on the order of 10–15 min) and resulted from passive listening. This is far different from the long-lasting effects attributed to early music instruction that are sometimes mistakenly referred to as the Mozart effect. In addition, Rauscher and colleagues made no claims that listening to any other type of classical music would improve spatial-temporal skills, and they did not claim that listening to the Mozart piece would improve any other type of ability.

Regardless, publication of Rauscher and colleagues’ brief correspondence in 1993 received considerable attention. Shortly thereafter, the governor of Georgia, Zell Miller, in a move that reflected the typical widespread misunderstanding of the findings, funded a program to provide a CD or cassette of classical music (including Mozart’s) to all the infants born in the state. Miller is quoted in the January 15, 1998, issue of the *New York Times *as saying, “No one questions that listening to music at a very early age affects the spatial, temporal reasoning that underlies math and engineering and even chess.” Despite criticism in the same news article by noted scientist Sandra Trehub, the popular myth that early exposure to classical music could result in long-term boosts in intelligence continued.

Attempts to replicate the findings of Rauscher and colleagues (1993) have met with mixed results (Chabris, 1999; Steele, Bass, & Crook, 1999). Chabris conducted a meta-analysis of 16 studies examining the Mozart effect and concluded that the small amount of enhancement observed in some specific spatial types of tasks was likely due to general arousal, particularly of the right cerebral hemisphere—a brain region critical to the control of both arousal and spatial task performance. Chabris concluded that, regardless, the effect was much smaller (on the order of 2.1 IQ points) than originally reported and was well within the range of average variation for any given individual.

Later researchers suggested that the results might be due, at least in part, to the use by Rauscher and colleagues of a between-subjects design, rather than a within- subjects design. The between-subjects design leaves open the possibility that people were actually experiencing the results of different levels of arousal or changes in mood. Rauscher and colleagues had compared performance after listening to a very lively piece of music to performance after listening to a relaxing piece or complete silence. Differences in general arousal level and mood are quite possible across these different conditions and are a plausible explanation for Rauscher et al.’s (1993) results.
### **Arousal and Preference **

Some researchers noted that preference for the auditory material (Nantais & Schellenberg, 1999), whether it be Mozart’s music or a story, appeared to be the cause of the spatial-temporal benefit. For example, when ratings of enjoyment, arousal, and mood were statistically controlled, the performance benefit of listening to Mozart went away (Thompson, Schellenberg, & Husain, 2001). In other words, listening to Mozart’s music only appeared to benefit people who preferred and enjoyed listening to that type of music.

The impact of musical preference has recently been confirmed with a study examining performance on a computer game (Cassity, Henley, & Markley, 2007). They compared performance in a commercially available skateboarding game that involved complex spatial-temporal skills when listening to the regular sound track of the game ( *Fight Like a Brave Man *by the Red Hot Chili Peppers) or Mozart’s K488. Following performance in the game, they asked participants to rate their preference for different musical genres (two of which used the Red Hot Chili Peppers and Mozart as examples of the genre). They found no evidence for the traditional Mozart effect. In fact, there was a nonsignificant trend in the opposite direction

when comparing performance while listening to Mozart versus the Chili Peppers. However, they did observe a significant effect for musical preference, with people tending to perform better when listening to music they liked, a result they noted that was particularly strong for males listening to and preferring heavy metal music. After taking musical preference into account, gender differences in musical impact are seldom observed (Elliott et al., 2005).

Thus, as disappointing as it may be, there is no evidence for an easy shortcut for making our children (male or female) smarter by simply playing them pieces of classical music. However, there is empirical support for the idea that sustained musical experience (in the form of physical and mental practice) can affect neuronal activity patterns.
## **MUSICAL TRAINING **

Extensive musical training may lead to both specialized behavioral and neural processing capabilities. For example, musical conductors appear to have enhanced spatial location capabilities (Munte, Kohlmetz, Nager, & Altenmuller, 2001; Nager, Kohlmetz, Altenmuller, Rodriguez-Fornells, & Munte, 2003), and musical experience results in enhanced memory for musical sequences (Nager et al., 2003; Palmer, 2005). In addition, notable differences in key cortical areas have been observed between musicians and nonmusicians (Ebert, Pantev, Wienbruch, Rockstroth, & Taub, 1995; Pantev et al., 1998). Evidence that musical training can result in changes in cortical functioning (auditory neuroplasticity) comes from an investigation with monkeys (Recanzone, Schreiner, & Merzenich, 1993). Over the course of several weeks, the monkeys were trained to recognize fine-grained changes within a narrow frequency band in tonal patterns. Individual neuronal responses before and after training in the experimental monkeys and control monkeys were compared. The trained monkeys had better behavioral discrimination abilities and significantly greater cortical response for the frequencies involved in the training. Importantly, the extent of their behavioral improvement correlated with both the number of neurons that responded to those frequencies after training and sharpness of their tuning. This provided clear evidence for the potential for musical training to alter cortical functioning, at least in animals. Evidence for auditory neuroplasticity in humans has also been found. Individuals with extensive musical training demonstrated more neuronal activity across a larger cortical area relative to nonmusicians for piano tones but not pure tones (Pantev et al., 1998).

Evidence is mounting to support the position that neuroplasticity resulting from musical training during childhood may significantly affect aspects of cognitive functioning outside the realm of musical processing (Schellenberg, 2005). For example, Patston, Corballis, Hogg, and Tippett (2006) examined visuospatial abilities of musicians and nonmusicians using a line bisection task. (The task requires participants to draw a line through the point they perceive as directly in the middle of a line.) Patston and colleagues found that musicians were significantly more accurate on the task than nonmusicians. Further, while right-handed nonmusicians tended to be more accurate when performing this task with their right hand, Patston’s righthanded musicians did not demonstrate this asymmetry. The authors suggested that spatial attention may be more symmetrical in the brains of musicians. However, the results of such studies cannot unequivocally demonstrate that the performance differences are a result of musical training. It could be argued that they result from preexisting differences that might make it more likely that individuals would have the opportunity for extensive musical training—the classic, “which came first, the chicken or the egg” problem.

Evidence that musical training actually changes neural structure and functioning is found in studies involving measurement before and after training among one group with comparison to a control group that receives no musical training. These investigations have been conducted. For example, Pascual-Leone (2001) utilized transcranial magnetic stimulation (TMS) to examine the neural circuitry involved in playing a musical pattern on a keyboard before and after extensive training. TMS involves applying a brief, transient magnetic pulse to a specific cortical area. Depending on the rate at which the pulse is delivered, it can either activate or deactivate the underlying neural circuitry. Deactivating neural signals functionally results in a “temporary lesion.” This technique can be used to map precisely which cortical areas are involved (and at what time) in a particular cognitive or motor process.

Pascual-Leone (2001) had adults with no prior musical training practice a sequence of manual keyboard presses with one hand (similar to practicing a scale). One group of participants practiced for 2 h a day for 5 days. Another group continued practicing for an additional length of time. Pascual-Leone compared functional and structural changes in these groups from their initial state to when they had achieved near-perfect levels of performance (i.e., meeting specified sequence and timing criteria). Pascual-Leone interpreted an initial pattern of cortical changes to an unmasking of existing patterns (recruiting existing cortical networks to perform this task, a pattern that was present immediately after practice for the first 3 days). These cortical changes gradually tapered off and were replaced by an emerging pattern that occurred much more slowly (building over the course of several weeks in the group who continued to practice). Pascual-Leone interpreted this slower emerging pattern as the formation of new cortical networks for performing the task. Granted, as exciting as these results are, they primarily relate to the formation of new motor pathways critical to performing complex musical pieces. They do not suggest that simply listening to complex musical pieces can influence the development of cortical pathways that will assist us with other types of complex tasks, as the Mozart effect had suggested. Next, we look at the performance effects of a type of sound that is increasingly present and generally much less pleasant.
## **NOISE **

Examination of the effects of noise on human performance has a long history (Broadbent, 1958, 1971; Gawron, 1982; Kryter, 1960, 1985, 1994; Poulton, 1976, 1977, 1978; Smith & Jones, 1992). The systematic study of the effects of noise on human performance began during World War I (see reviews in Kryter, 1985, and, Matthews et al., 2000) and World War II (Broadbent, 1978; Poulton, 1977). An extensive body of literature pertaining to the effects of noise on human performance now exists. It is beyond the current scope to fully review this work. The interested reader is referred to more comprehensive works that provide broader, more in-depth coverage of the effects of noise (Gawron, 1982; Kryter, 1985, 1994; Smith & Jones, 1992; Tempest, 1985). However, several important aspects of this work, particularly those that have relevance to understanding the effects of noise on mental workload, are discussed.

First, the effects of noise on performance are not straightforward. Noise has been found at times to both improve and interfere with performance. Simple solutions to this puzzling picture (e.g., by examining either the absolute level of noise or the type of noise) have proven elusive (Hygge & Knez, 2001; Smith & Jones, 1992). First, some definitions of noise and key factors influencing its effects are presented. In the simplest sense, noise can be defined as any unwanted sound. However, the term *noise *has also been used to describe random and aperiodic sounds varying in intensity and frequency as well as any sound that interferes with (masks) a desired sound (Smith & Jones, 1992). This means that a sound can be a noise in one situation or environment but not in another. In other words, what constitutes noise, much like what constitutes a weed, is in the eye (or ear) of the beholder.

Two primary factors characterizing noise and its potential effects are intensity and frequency. As discussed in Chapter 2 , these factors interact psychophysically. Another key point to be made is that the effects of noise can be discussed in terms of subjective assessments (psychological reactions such as annoyance) and objective assessments (behavioral consequences and physiological responses). Smith and Jones (1992) pointed out that these two assessments may diverge. In other words, a person may find a particular noise highly annoying and yet its presence does not necessarily disrupt performance. Conversely, a person may be enjoying a particular noise (e.g., music), and yet his or her performance may be impaired by the noise. Considerable evidence suggests that subjective and objective assessments often interact as well. In particular, the listener’s perceived control over the noise appears to have substantive performance effects. That is, noise is less detrimental to performance if the person perceives that he or she has control over its presence or intensity (Smith & Jones, 1992).

People commonly complain of the detrimental effects of noise on performance in everyday tasks (Smith & Jones, 1992). These complaints are commonly waged against moderately noisy environments (70–90 dBA). However, despite numerous empirical investigations, the precise effects of noise on mental workload remain unclear. Noise has variously improved, impaired, or left unchanged participants’ performance on a variety of tasks (Bell, 1978; Davies & Jones, 1975; Gawron, 1982; Hygge & Knez, 2001; Kryter, 1985; Sanders & McCormick, 1993; Smith & Jones, 1992; Smith, Jones, & Broadbent, 1981; Weinstein, 1974, 1977).

In confirmation of the equivocal findings pertaining to the effects of noise, Gawron (1982) reviewed previous research and cited studies with many conflicting results. Two examples cited in the Gawron (1982) report are as follows: Obata, Morita, Hirose, and Matsumoto (1934) found decreased performance speed in the presence of noise, while Davies and Davies (unpublished report cited in Davies, 1968) found increased performance speed in the presence of noise; Hamilton, Hockey, and Rejman (1977) found that noise decreased the number of correct responses, while Park and Payne (1963) found no difference in this measure with a similar noise intensity. Investigators have attempted to explain these contradictory results in numerous ways.

Sanders and McCormick (1993) and others (Smith & Jones, 1992) proposed that these equivocal results can be explained in part by the wide variability of conditions tested in noise experiments. They pointed out that these test conditions have varied regarding intermittent or continuous noise and have used such varying noise sources as tape-recorded machine noise, street sounds, rocket noise, and gibberish. In addition, they pointed out that the tasks used to measure performance have differed dramatically in terms of the relative demands they place on the perceptual, cognitive, memory, and motor capabilities of the participants.

Grant and colleagues (1998) observed that recall of newly studied material was better if the recall situation (noise vs. quiet) matched the study condition. That is, those who studied material in a condition involving background noise recalled more if they were tested in a condition involving background noise relative to quiet. Conversely, those who had studied in quiet performed better if tested in the same quiet conditions, relative to noise. The implications for this form of state-dependent learning are rarely considered in empirical investigations and may be another reason why different studies often report equivocal results.

Kryter (1985) listed 11 different ways that noise has been purported to affect performance. One was the position of Poulton (1976) that noise masked internal speech and other important acoustic cues. The other 10 ranged from noise causing distraction, competing for psychological attention, or causing confusion by conveying irrelevant information to physiological changes and preemption of auditory neural pathways.

Matthews and colleagues (2000) discussed three main ways that noise can affect performance: (a) the disruption of auditory perception (i.e., through either masking or hearing impairment); (b) disruption of postperceptual processing (reduction of attentional resource availability); or (c) indirect stress-related effects such as irritation or annoyance. Each of these avenues is addressed to a limited extent here. However, for the current purposes, emphasis will be placed on further examination of the second causal explanation, that is, that noise may directly affect the level of attentional resources required or available to carry out specific tasks.

Noise is generally thought to have adverse effects on performance. However, in certain circumstances, noise can enhance performance. Enhanced performance in conditions of noise can generally be attributed to physiological changes such as increased arousal levels when operators are fatigued. A brief history of noise research follows.
### **Early Noise Research **

#### **The Broadbent-Poulton Debate **

During World War II, a number of researchers began to investigate the mechanisms behind the detrimental impact of noise on performance, particularly in conditions of sustained attention. Broadbent was a key figure in this early work ( Figure 7.8 ). Broadbent reasoned that noise could directly disrupt performance. According to this position, noise had a distracting effect, which he termed the *internal blink *, that temporarily took attention away from other tasks (Broadbent, 1958; and see discussion in Matthews et al., 2000). This view was later challenged by Poulton and others, who reasoned that noise affected performance by masking acoustic cues and inner speech in particular. Poulton (1976, 1977, 1978) argued that continuous noise and articulatory suppression both prevented listeners from engaging in subvocal rehearsal. Thus, each had similar performance-degrading effects because in both cases they disrupted echoic memory processes.

**FIGURE 7.8 **Donald Broadbent.

Poulton followed in the tradition of S. S. Stevens (1972). From a series of experiments conducted during World War II, Stevens had concluded that the negative effects of noise were primarily due to the masking of auditory cues. Poulton (1977) supported the position that the effects of noise were primarily due to masking— the fact that noise covers up important environmental sounds that normally provide informative cues and can further block inner speech (i.e., subvocal rehearsal) that normally would aid performance by supporting verbal working memory. Poulton argued vehemently in published literature against Broadbent’s position. Poulton argued that Broadbent had not adequately accounted for the effects of masking in his experiments throughout the 1950s, and his frustration with the lack of acknowledgment of this position was summed well in the last lines of an abstract he published in a review paper in 1977:

Here, the noise can be said to interfere with or mask inner speech. Yet current explanations of the detrimental effects of continuous intense noise usually follow Broadbent and ignore masking in favor of nonspecific concepts like distraction, the funneling of attention, or overarousal. ( p. 977)

Broadbent (1978) responded to Poulton’s (1977) review and position statement by pointing out that Poulton had in some cases confused the unit of measurement used (i.e., confusing dB SPL [sound pressure level] with dBA or dBC or vice versa). In others, Broadbent asserted that Poulton had ignored or discounted critical controls that the researchers had put in place to ensure that the effects of masking could be ruled out as the primary cause of noise effects.

Thus, despite some support for Poulton’s position (see discussion in Kryter, 1985), the mechanisms behind the impact of noise remained equivocal. Further evidence that contradicts Poulton’s argument continued to be found. For instance, Jones (1983) demonstrated that when acoustic cues were eliminated from the task (participants were wearing sound attenuating headphones), the impact of noise was still the same.

The performance effects of noise have also been explained within the context of arousal theory (Broadbent, 1971). If noise increases arousal, then the beneficial performance effects of noise under conditions of sleep deprivation can be explained since the arousal stemming from the noise would serve to help counter low levels of arousal due to sleep deprivation. However, when arousal levels were already too high, then any additional noise would be expected to impair performance. Conversely, noise would also be expected to improve performance in other situations of low arousal (i.e., from extended time on task). However, this hypothesis has not been supported in the literature. As discussed by Matthews et al. (2000) the performance impact (in terms of increased errors) of high noise levels tends to increase with increased time on task.

Arousal theory also leads to predictions of individual differences in the effects of noise. The theory indicates that extraverts, who are generally thought to be underaroused, might benefit from the presence of noise, while introverts, believed to be overaroused, might experience performance decrements from similar levels of noise. Smith and Jones (1992) provided a detailed discussion of this issue.

Broadbent (1979) summarized the results of numerous investigations, concluding that noise affects performance beyond what can be explained by the effects of masking. The precise nature of the effect will depend on a combination of the type of noise and the task being performed—as well as potentially the individual listener (see also Smith, 1989). Broadbent concluded that noise has little or no effect on basic sensory and low-level processes (i.e., visual acuity, contrast sensitivity, eye movements, and simple reaction time). Further, continuous nonverbal noise appears to have little effect on basic working memory tasks such as mental arithmetic. More complex tasks, such as choice serial response tasks, are generally sensitive to the effects of noise, with response times becoming more variable perhaps due to periods of inefficiency or distraction.

The Broadbent-Poulton debate carried on for some time in the early literature until it began to become apparent that neither position adequately explained the equivocal effects of noise on performance. The complex pattern of results across numerous studies indicated that the effects of noise cannot be explained simply by assuming that noise interferes or masks acoustic cues or inner speech, at least not in a passive or mechanistic way (Smith, 1989; Smith & Jones, 1992). In summary, the impact of noise depends not only on the type of noise but also on the task performed. In general, noise often reinforces adoption of a dominant task performance strategy and decreases the efficiency of control processes used to monitor task performance.

Thus, we see that some generalizations pertaining to the effects of noise can be made, although specific effects will depend not only on the task and type of noise but also potentially on characteristics of the individual listener. The next section summarizes and discusses some of the major findings pertaining to the impact that noise may have on mental workload within a resource theory framework.
### **Resource Theory Framework **

Viewing the impact of noise within a resource theory, or mental workload, framework can assist in understanding the effects of noise on performance. For example, Finkelman and Glass (1970) suggested that previous equivocal results pertaining to the effects of noise on performance may be due to the way in which earlier investigators measured performance decrements. Using a mental workload framework, Finkelman and Glass reasoned that when demands imposed by the task and concurrent environmental stress are within the operator’s processing capacity, the task can be performed substantially without errors. In fact, some researchers (e.g., Yeh & Wickens, 1988) have suggested that under certain circumstances performance may actually improve as workload increases. For example, when examining the processing demands of relatively easy tasks, an increase in workload may be associated with the operator increasing his or her attention to maintain current levels of performance. This notion would be in line with the classic performance-arousal curve cited in numerous investigations. However, when the limited capacity of an operator’s processing resources is exceeded, performance degradation will occur. They termed this the *overload notion *.

Finkelman and Glass (1970) investigated their overload notion by means of a subsidiary task involving delayed digit recall in combination with a primary tracking task involving a vehicular steering simulation. Participants were 23 undergraduate volunteers. Random bursts of 80-dB white noise were presented in either a predictable pattern (9-s burst interpolated with 3-s intervals of silence) or in an unpredictable pattern (bursts of random duration varying in ten 1-s steps between 1 and 9 s). The total duration and ratio of sound/silence was the same for predictable and unpredictable noise conditions.

Finkelman and Glass (1970) found that unpredictable noise had no effect on the primary tracking task but did cause statistically significant performance decrements in the subsidiary recall task. Specifically, the mean number of errors on the subsidiary task was twice as great in the unpredictable noise compared to the predictable noise, suggesting that the unpredictable noise required greater mental effort to ignore relative to the predictable. Time-on-target (TOT) measures of tracking were virtually the same in each noise condition. This suggests that participants were able to concentrate their attentional resources to preserve one task—although possibly at the expense of the second task. Finkelman and Glass concluded that the subsidiary task appeared to be a superior method of detecting performance deficits resulting from environmental noise and interpreted their results within a resource framework. The subsidiary or dual-task technique has also been widely used to examine the impact of noise on task prioritization.
#### **Task Prioritization **

The presence of loud noise may cause people to concentrate their attention on tasks of high priority—potentially allowing them to maintain performance on some tasks— while disregarding tasks perceived as less important. Broadbent (1971) explained this by suggesting that noise increases the probability that people will sample information from dominant sources at the expense of nondominant ones.

For example, Hockey and Hamilton (1970) gave participants a short-term word recall task in quiet (55 dB background) and noise (80 dB). Words were presented in one of each of the four corners of the screen. Participants were asked to recall all the words in the correct order; after this, they were unexpectedly asked to recall which corners of the screen the word had been presented. Those performing the task in the presence of noise recalled as many words as those performing the task in quiet. In fact, a marginally significant trend was observed for the noise group to correctly recall more words in the correct order relative to those in the quiet group. However, when asked to provide the location where the word was presented, the quiet group provided the correct location information significantly more often than those in the noise group. Hockey and Hamilton concluded that the noise had induced a high arousal level that resulted in task prioritization. That is, the noise group paid more attention to the most important aspect of the task (word recall in the correct order) but consequently processed less-relevant aspects of the task (the location) to a lesser degree.

In a series of three experiments, Smith (1982) used a paradigm similar to Hockey and Hamilton’s (1970) and found supporting results. In his second experiment, Smith extended Hockey and Hamilton’s results by manipulating which of the two aspects of the task were given priority—word order or location. An interaction was observed between noise condition and task prioritization such that participants in the noise condition performed better for whichever task was prioritized (order or location), indicating that the previous results could not be explained by something inherent in the task structure but rather relied on prioritization instructions. Smith’s third experiment provided further support for this conclusion, indicating that the same pattern of results was observed even when the prioritized task was performed after the less-relevant task.

Together, these findings provide strong support for the position that moderate-to- loud noise results in the focusing of limited resources on the most relevant aspects of a task at the expense of less-relevant aspects—or task prioritization. The next section addresses the impact of lower-intensity noise on cognitive performance.
### **Everyday Noise Levels **

Numerous laboratory studies have demonstrated that acute noise degrades complex task performance (Smith & Jones, 1992). A limited number of studies have demonstrated that even noise present at much lower levels, such as those commonly found in everyday situations, affect performance. The investigation of Weinstein (1974) illustrated this as well as another very important observation. That is, that people may be poor judges of the impact of noise on their performance.

Weinstein (1974) questioned the application of previous studies of the effects of noise on complex activities, noting that past research has most often utilized rather exotic tasks and high noise levels (often above 90 dB). Based on these observations, Weinstein conducted a study that (a) investigated the effects of realistic noise and noise levels (unamplified sounds of a teletype machine) on performance of a familiar task with some real-life significance (proofreading printed material); and (b) examined participants’ knowledge of the effect of the noise on their performance.

In line with a resource framework, Weinstein (1974) reasoned that noise would interfere with tasks that were intellectually demanding (such as identifying grammatical errors) while leaving performance on tasks with little cognitive demand (such as identifying spelling errors) unchanged. Participants were 43 undergraduates in an introductory psychology class randomly assigned to either the noise or the quiet condition. Participants were required to proofread texts that were prepared from nonfiction sources and to detect errors introduced at irregular intervals. Errors were of two types (contextual or noncontextual). Contextual errors included errors in grammar, missing words, and incorrect or inappropriate words (e.g., the word *an *instead of the word *and *). The second type of error was noncontextual errors, which included misspelled words and typographical errors. Weinstein reasoned that contextual errors place greater cognitive demand on participants since they could not be detected by examining individual words.

Weinstein (1974) demonstrated that realistic noise levels (66 dB and 70 dB bursts against a background of 55 dB) degraded intellectually demanding tasks. Perhaps even more important is that, when questioned, participants were unaware of the negative impact the noise had on their performance. Overall, participants reported that they felt they had been able to adapt to the noise and thus that it had not affected their performance after its initial onset. Weinstein’s behavioral data contradicted this subjective report. These findings underscore the observation yet again that people are often not able to give accurate subjective estimates of the impact of irrelevant sounds on their performance. Now, although the focus of this chapter is on nonverbal sound, it is nevertheless important to include at least some discussion on a type of “noise” that can have particularly detrimental effects on many types of performance. That noise is irrelevant speech.
### **Irrelevant Speech and Visual-Verbal Processing **

Examining the affects of irrelevant speech on information processing sheds light on the attentional processes required for storage and transformation of verbal information in memory (Jones & Morris, 1992). Irrelevant speech is the most detrimental form of noise to performance, at least when the task involves processing language in some form or other. Most of the research in this area has examined the impact of irrelevant or unattended speech on performance of tasks requiring the recall of some type of visually presented verbal stimuli, such as digits or words (Jones & Morris, 1992). As an early investigation by Colle and Welsh (1976) demonstrated, irrelevant phonological material, even if it is in a language that is unfamiliar to the listener, disrupts recall of visually presented material. Colle and Welsh also observed that the disruptive effects of irrelevant speech were greatest when the to-be-remembered letters were phonologically distinct (i.e., F, W, L, K, and Q) versus phonologically similar (i.e., E, G, T, P, and C). They reasoned that maintenance of an auditory code would be more helpful for the phonologically distinct letters. Thus, irrelevant speech (or anything else that disrupted the auditory code) might have less of an effect on the phonologically similar letters as people are relying on the auditory code less for these lists.

Salame and Baddeley (1982, 1987) demonstrated that both irrelevant spoken words and nonsense syllables disrupted memory for visually presented digits. As in the experiment by Colle and Welsh (1976), participants were told to ignore the irrelevant sounds. Salame and Baddeley reasoned that the phonological information from the irrelevant material is automatically encoded into the phonological store, where it is represented at a phonemic level rather than a word level. Therefore, phonologically similar material can be nearly as disruptive as actual words. As discussed further in Chapter 9 , speech (unlike text and other nonverbal sound) is thought to gain automatic obligatory access to the phonological loop, a prelexical processing component of working memory. This may explain why irrelevant speech is so disruptive to performance of other verbal tasks.

The disruptive effects of irrelevant speech appear to be limited to tasks that require processing of some type of verbal stimuli. Irrelevant verbal material has little to no effect on recall of pitch, as in a tone recall task (Deutsch, 1970).

Until this point, the focus has been on the effects of relatively short periods of noise. What impact does long-term or chronic noise exposure have on performance? This is the focus of the next section.
## **CHRONIC NOISE EXPOSURE **

### **Workplace Noise **

Noise has also been implicated as an antecedent factor in workplace accidents, both directly—through distraction and fatigue inducement—and indirectly by resulting in both temporary and permanent hearing loss (Picard et al., 2008). Hearing loss, whether temporary or permanent, can prevent perception of important environmental cues to safety (i.e., localizing objects and potential hazards) as well as blocking or reducing perception of warning sounds and speech communications. The need to block unwanted sound can increase cognitive demand in the short term and can exacerbate workplace fatigue in the longer term. Picard et al. conducted an investigation that included over 50,000 males in various occupations between 1983 and 1998 for whom both hearing assessment data and workplace injury records were available. An association was found between accident risk and hearing sensitivity, with increased hearing threshold associated with increased risk of accident exposure even after controlling for age and occupational noise exposure at the time of the hearing test (see discussion in Smith & Jones, 1992). Other investigations have observed associations between workplace noise exposure and increased risk of accidents (Kerr, 1950; Noweir, 1984; Poulton, 1972). However, separating out the higher risk associated with certain types of occupational exposures from noise exposures and other socioeconomic factors can be problematic. For example, in addition to high noise levels, Kerr observed significant correlations between workplace accident rates and several other factors. High accident rates were associated with low intracompany transfer mobility rates, small numbers of female employees and salaried employees, and low promotion probability rates. The difficulty of separating socioeconomic and other confounding variables from the impact of noise reoccurs pertaining to the impact of chronic noise on school-age children, discussed next.
### **Aircraft Noise and Children **

Because short-term noise exposure has been found to disrupt performance on many different cognitive tasks, great concern has been directed at the potential consequences of chronic exposure to aircraft noise among school-age children. Compared to age-matched controls attending schools in quiet neighborhoods, children who live near airports have been shown to have higher blood pressure (Cohen, Evans, Krantz, & Stokols, 1980; Evans, Bullinger, & Hygge, 1998), to have reading comprehension difficulties (Cohen et al., 1980; Green, Pasternack, & Shore, 1982; Stansfeld et al., 2005), to be more likely to fail on cognitive tasks, and to be more likely to give up prematurely (Cohen et al., 1980). Increased noise exposure is associated with a higher probability of reading at least 1 year below grade level (Green et al., 1982).

Negative performance associations are consistently found between chronic noise exposure and cognitive performance. However, it is often difficult to determine the extent to which these negative performance outcomes (i.e., poor reading comprehension abilities and reading below grade level) are a result of the noise exposure, per se, or other socioeconomic factors that are associated with living in high-noise areas (i.e., next to airports, subways, and highly congested roadways). For example, Haines, Stansfeld, Head, and Job (2002) observed significant associations between noise levels and performance on reading and math tests among school-age children. Their sample included approximately 11,000 children from 123 schools. They observed significant associations in chronic noise exposure in a dose-response function (the higher the noise exposure, the greater the performance detriment) to national standardized tests of reading comprehension and mathematics. However, these associations were confounded by socioeconomic factors. Once Haines and colleagues adjusted their model for a measure of social deprivation (the percentage of children in the school eligible for free lunches), the association between noise level and test performance was greatly reduced and no longer statistically significant.

In a more recent cross-national, cross-sectional investigation, Stansfeld and colleagues (2005) controlled many more sociodemographic factors and still observed a significant association between chronic aircraft noise exposure level and reading comprehension. Their sample consisted of nearly 3,000 children from the Netherlands, the United Kingdom, and Spain.

Sociodemographic variables in this investigation included employment status, housing tenure, crowding (the number of people per room in the home), and maternal education level. Both before and after adjusting for each of these variables, significant impairments in reading comprehension scores were obtained as a function of exposure to chronic aircraft noise. The effect sizes of the impact of noise exposures for measures of reading did not differ across countries or socioeconomic status. Specific effects include such observations as a 5 dB increase in noise exposure associated with a reading delay of between 1 and 2 months among 9- and 10-year-old children in the United Kingdom and the Netherlands. Exposure to the aircraft noise was not associated with measures of working memory, prospective memory, or sustained attention.

A unique opportunity to examine the impact of chronic noise exposure on reading comprehension in children was utilized by Hygge, Evans, and Bullinger (2002). They conducted an investigation involving data collection waves before and after the opening of the new Munich International Airport and termination of the old airport. Such a design allowed for a direct experimental test of the effects of airport noise. The sample consisted of children at both airport sites and a control group closely matched for socioeconomic status. They were able to test children in each group individually on an array of cognitive tests, including measures of reading, memory, attention, and speech perception. They obtained measures on each of these tests once before the airport moved and twice afterward. After the airport moved, reading and long-term memory scores declined in the newly exposed group and improved in the formerly exposed group, providing strong causal evidence of the negative but potentially reversible effect of chronic noise on school-age children’s cognitive performance.

The impact of noise on performance may not always be detrimental. As discussed in the following section, noise can actually improve performance of certain types of vigilance tasks.
## **AUDITORY VIGILANCE **

The term *vigilance *refers to the ability to sustain attention for an extended period of time (Davies & Tune, 1970; Parasuraman & Davies, 1984; Warm & Alluisi, 1971; Warm, Matthews, & Finomore, 2008). With the increased use of automation in many modern work environments, maintaining vigilant monitoring of system states and controls is an essential part of many jobs. Air traffic control and baggage screening are two commonly cited tasks that require vigilance. It is well documented that maintaining vigilance is hard work (see Grier et al., 2003; See, Howe, Warm, & Dember, 1995; Warm, Parasuraman, & Matthews, 2008). Performance tends to degrade after a relatively short period of time (i.e., after 15–20 min), resulting in increased response time, errors, or both. This performance decline is referred to as the *vigilance decrement *, and it is particularly evident in tasks that have high event rates and that require the operator to maintain an example of the target or signal in memory (Parasuraman, 1979). Performance decrements are generally accompanied by increases in subjective workload ratings as well as physiological changes, such as decreased cerebral blood flow velocity (Hitchcock et al., 2003; Shaw et al., 2009) and increased activation of areas in the right cerebral hemisphere (see review in Parasuraman et al., 1998).

Auditory tasks are less susceptible to the vigilance decrements than visual tasks, but performance declines are still evident (Warm & Alluisi, 1971). For example, Szalma et al. (2004) compared performance and subjective workload ratings on comparable versions of a visual and auditory temporal discrimination task. Participants had to watch or listen for signals of a shorter duration. Despite equating for initial task difficulty using a cross-modal matching technique, performance in the visual version of the task was much worse than for the auditory version. A vigilance decrement was observed for both tasks over the course of four consecutive 10-min vigils, and participants rated the two tasks as equally demanding in terms of workload.

Using the same temporal discrimination task, Shaw et al. (2009) observed similar performance declines for both the auditory and visual versions. Shaw et al. noted that the performance reductions for both modalities were accompanied by similar reductions in cerebral blood flow velocity, particularly in the right hemisphere. They concluded that a supramodal attentional system is responsible for sustained attention, such that neither the visual nor auditory modalities escape the consequences of maintaining vigilance.

Still, the majority of studies that have compared vigilance performance for tasks presented in the auditory versus visual modalities showed at least a slight performance advantage for the auditory task (for a meta-analysis and review, see Koelega, 1992). Further, introverts showed a slight advantage over extraverts in tasks requiring sustained attention, and these performance differences were greater with visual relative to auditory tasks (see Koelega, 1992).
### **Noise and Vigilance **

A further observation that deserves mention is that mild noise seems to improve vigilance performance, at least in some tasks. This has been attributed to the fact that some amount of stimulation in the environment (i.e., instrumental music) seems to be preferable to sterile environments. Pickett (1978, as cited by Parasuraman, 1986) likened it to being in a “coffee shop atmosphere” rather than a quiet library, where there is some stimulation but little distraction. Background music may aid performance, particularly in jobs that require sustained attention and repetition (Fox, 1971). As discussed previously, noise in the form of music has been shown to improve performance in a visual vigilance task when combined with a simulated driving-like tracking task (Beh & Hirst, 1999). The picture is complex, depending on both the level of noise and individual differences. For example, introverts, hypothesized to have higher base levels of arousal, generally demonstrate performance advantages in vigilance tasks, relative to extraverts, who are hypothesized to have lower base levels of arousal (see Geen, McCown, & Broyles, 1985; Koelega, 1992). Geen et al. found that low levels of noise (tape recordings of a food blender played at 65 dB) improved detection rates among introverts, while the same noise played at a higher intensity level (85 dB) degraded performance. Conversely, extraverts performed worse in the low-intensity noise relative to their performance in the high-intensity noise. In the high-intensity noise, their performance was equal to that of the introverts, but the introverts outperformed the extraverts in the lower-intensity noise levels. In general, mild-to-moderate noise is often found to improve or have no effect on vigilance performance (see reviews in Davies & Tune, 1970; Mirabella & Goldstein, 1967). However, in a more recent review of this area, Koelega (1992) pointed out that these effects are not consistently observed.
## **SUMMARY **

Increased noise exposure is also commonly positively associated with increased annoyance (Clark & Stansfeld, 2007; Stansfeld et al., 2005), which can compound the negative cognitive impact on both children and adults. Given the prevalence of environmental noise in our modern world, its impact on public health, cognition, and quality of life deserves further attention.

In summary, nonverbal sounds—both wanted and unwanted—surround us in the modern world. Regularities in the acoustical signal such as the mathematical relationships between spectral components or harmonics of a sound, as well as changes in stimulus onset time and location, are some of the many cues we use to segment this symphony of sound in the process called auditory scene analysis. Once auditory streams are segregated, sounds appear to be stored based on the objects that created them in a process analogous to visual object-based coding. Our remarkable ability to recognize objects and characteristics of those objects has been explained by the competing theories emphasizing either an information-processing or ecological approach. An extensive body of literature now exists examining the impact of both music and noise on physiological and cognitive performance. Music appears to enhance many types of performance—increasing the effectiveness of fitness workouts while decreasing feelings of fatigue. Music may at times improve productivity and job satisfaction at work and even improve performance on some cognitive tasks, particularly if those tasks are long and repetitive, like vigilance tasks. However, if the music is too loud or the tempo too fast, it is likely to detract from performance, and people will tend to view it as unpleasant. At this point, music can cross the line to become noise—or unwanted sound. Noise has generally been found to have a negative impact on performance on a wide range of cognitive tasks. Workplace noise may increase accident risk, and chronic noise exposure has a detrimental impact on reading comprehension and memory performance among school-age children.
## **CONCLUDING REMARKS **

Many different types of nonverbal sounds form the soundscape around us and have the potential to have an impact on mental workload and performance either positively or negatively. An extensive amount of attention was given to the impact of noise on performance in earlier years. Today, that focus has shifted toward the impact of music on performance. With both, a complex mosaic is found depending on characteristics of the individual, the sound, and the task or tasks to be performed. In the next chapter, attention is directed toward speech processing and the factors that have an impact on the mental effort of this everyday task.