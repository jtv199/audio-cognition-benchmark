# Unknown Chapter

*Source: Auditory Cognition and Human Performance by C. L. Baldwin (2012)*

---

# 11 Auditory Display Design

## **INTRODUCTION **

The preceding 10 chapters in this book have discussed how aspects of different forms of auditory stimulation—speech, music, noise, nonverbal sounds, and so on— influence cognition and performance in a wide variety of tasks. A common theme that ran through many of the chapters is that while our phenomenal experience views auditory processing as seemingly effortless, in fact many aspects of auditory cognition pose significant demands on attentional resources and thus on mental workload, particularly in challenging listening environments. Accordingly, the design of auditory displays that minimize workload demands so that people can use them effectively and safely represents an important practical issue. This chapter examines the implications of the findings on auditory cognition discussed previously for the design of auditory displays.

Technological advances in recent years have provided engineers and designers the ability to display large amounts of information to human operators in many work environments. In some cases, the volume of data is greater than any human could possibly process at any given time. For example, the modern aircraft cockpit is replete with visual displays. In this environment of high visual demand, auditory and haptic displays are increasingly being used to provide information to the aircrew.

An auditory display is any device or interface in the environment that provides information to a human user in an auditory format. Defined this way, auditory displays can be discrete or continuous sounds that are either inherent or designed. They can also range from simple to complex. Inherent auditory displays are those that are an intrinsic part of some system and that sound automatically when the system is operational. The sounds of a computer operating, a printer in use, or the hum of a running engine in a car are examples of inherent displays. Changes in the auditory characteristics of the machines we interact with provide important cues regarding how the system is operating, such as when it is time to shift gears or when a machine may be malfunctioning. Recognizing the wealth of information these inherent displays can provide, designers sometimes artificially mimic them, such as implementing a key clicking sound in a touch screen display to provide feedback to the user. The focus of this chapter, however, is on designed rather than inherent auditory displays. Coverage of inherent displays is limited to the observation that they can inform ecological design.

Designed auditory displays include the more obvious sources, such as alarm clocks, telephones, and doorbells, as well as sophisticated sonifications or auditory graphs. They can be nonverbal, as the previous examples illustrate, or verbal, such as the in-vehicle routing and navigational directions informing us in our choice of voice (from a generic female avatar to the voice of the Mr. T. character) to “Turn right in two blocks on Ash Avenue.” Such auditory displays vary tremendously in complexity and format, which largely influences the mental workload required to comprehend them.

Auditory displays can vary from simple beeps and buzzers to complex sonograms that require extensive training before they can be interpreted. Intermediate levels of complexity include auditory displays used in a wide variety of operational settings, including aviation, surface transportation, and medical facilities. Many operational environments utilize multiple auditory displays, including a range of inherent continuous and designed discrete categories as well as different complexity levels. For example, in the modern cockpit, continuous auditory information is available from running engines, while auditory alerts are used to signal changes in automation mode. Auditory warnings are used to signal system malfunctions, and speech interfaces are used for communication with air traffic controllers and other ground personnel.

Several important quality reviews of auditory displays have previously been compiled (Edworthy & Adams, 1996; Edworthy & Stanton, 1995; Kramer et al., 1999; Stanton & Edworthy, 1999b; Walker & Kramer, 2004). Their main conclusions are described, supplemented by a discussion of empirical findings of newer studies conducted since these reviews. Furthermore, this concluding chapter emphasizes the application of this existing body of knowledge for understanding and predicting the mental workload requirements of auditory displays in operational environments. First, some of the advantages and challenges with using auditory displays are presented, including guidelines for when they are preferred to other forms of displays. Next, key psychoacoustic factors are discussed, including such topics as how to ensure that auditory displays and warnings are detectable and distinguishable. The next section is devoted to auditory warnings, a specific type of auditory display. From there, auditory displays are discussed within the context of three specific application areas: aviation, surface transportation, and medicine. First, some important general considerations pertaining to auditory displays in work environments are discussed.

The next section provides answers to a number of key questions concerning the use of auditory displays in the workplace: When is it preferable to use the auditory rather than the visual channel to present information? How do the characteristics of auditory displays provide advantages for operational performance? How can this information be applied in different work settings?
## **ADVANTAGES OF AUDITORY DISPLAYS **

Auditory displays date to at least the Industrial Revolution and even much earlier if you include natural auditory warnings like the human cry (see review in Haas & Edworthy, 2006). Many characteristics of the auditory modality make it particularly well suited for providing alerting and warning information in most settings. Because of this important function, auditory warnings are discussed more fully in a subsequent section. The auditory channel is also an effective means of conveying status or system-level changes and representational encodings of critical and sometimes multidimensional information. Their use in these more complex information representational forms is on the rise. Table 11.1 summarizes a number of circumstances when auditory displays are preferable to visual displays that were identified by Sanders and McCormick (1993). **TABLE 11.1 Circumstances When Auditory Displays Are Preferable to Visual Displays **

Circumstance/Situation

When the origin of the signal itself is a sound

When the message is simple and short

When the message will not be referred to later

When the message deals with events in time

When warnings are sent or when the message calls for immediate action

When continuously changing information of some type is presented, such as aircraft, radio range, or flight path information

When the visual system is overburdened

When the speech channels are fully employed (in which case auditory signals such as tones should be clearly detectable from the speech)

When the illumination limits use of vision

When the receiver moves from one place to another

When a verbal response is required

*Note: *Data from Sanders, M. S., & McCormick, E. J. (1993). *Human factors in engineering and design *(7th ed.). New York: McGraw-Hill, p. 169.
## **Auditory Source **

The auditory modality is well suited when the information to be displayed is itself auditory. For example, the optimal means of conveying how something sounds is by demonstrating the sound. If you have ever had the experience of trying to imitate the unusual sound made by a car engine or other motor to a mechanic, you can appreciate this use of the auditory channel. Other instances include trying to convey how a professor’s accent or mode of speech has an impact on comprehension or presenting a display of engine noise to a remote unmanned ground or aerial vehicle operator. The auditory modality is also well suited when the message is short.
### **Brief Communications **

Long or extensive messages that exceed working memory capacity are generally best presented in a visual format so that they can be referenced, thus increasing comprehension. But, when the message is brief, the auditory modality is generally preferred. To make sure that working memory limitations are not exceeded, auditory messages should be limited to two or three terse messages. For example, Barshi (1997) showed that the ability to execute procedural commands correctly deteriorates with increases in the number of commands contained in a set, and that performance bottoms out at three commands. Exceeding three commands overloads working memory capacity, particularly when the auditory display must be time-shared with a concurrent task (Scerbo et al., 2003). If more than three messages must be presented or if the individual messages are complex, then visual displays are advantageous (Dingus, Hulse, Mollenhauer, & Fleischman, 1997; Srinivasan & Jovanis, 1997a, 1997b).

**FIGURE 11.1 **Seismic information.

A caveat must be noted regarding the length and complexity issue. Auditory displays are useful when messages are short and simple. However, auditory displays may also be used to present some types of information that are too complex to be easily interpreted visually. Consider seismic information like the sample illustrated in Figure 11.1 . This sample illustrates activity from only a few recorders. An operator may be asked to monitor information from seismographs with 15 or more recording sites and to integrate the information across multiple seismographs. The complexity of this information is often difficult to understand in the visual domain (Walker & Kramer, 2004). Capitalizing on the human ability to perceive complex acoustic patterns, seismic information is much easier to interpret and categorize when it is presented in auditory rather than visual format. Translating complex information into auditory form is referred to as *audification *, and it is one of the many recognized categories of nonverbal auditory displays (Walker & Kramer, 2004). Others include auditory icons, earcons, and sonifications. Each is discussed in greater detail in subsequent sections. The main distinction is that in these instances the information is nonverbal and multidimensional. Our auditory system possesses unique pattern recognition capabilities that are well suited for processing this type of complexity, as long as the acquired information does not need to be stored for an extended period of time or referred to at a later point in time.
### **Short Retention Intervals **

Visual displays may be preferable to the auditory channel when the acquired information must be retained over extended periods of time. For example, one of the advantages of a text-based data-link interface over traditional voice communications for providing air traffic control (ATC) information involves this issue. Text-based commands can be stored and referred to later. Therefore, they reduce working memory demands and decrease the chances of misinterpretation or forgetting. Conversely, pilots may attempt to write down spoken ATC commands, thus detracting from the benefit of eyes-free transmission. In fact, Helleberg and Wickens (2003) found that pilots spent more time with their head’s down when using a voice ATC transmission system relative to a visual data-link system due to their attempt to write down spoken commands. Such headdown time disrupted both traffic monitoring and flight path tracking performance.
### **Time-Critical Signaling **

Auditory displays are well suited for presenting time-critical information as well as information that requires temporal sequencing. Auditory signals can be superior to visual signals in terms of their attention-getting capabilities and often lead to faster response times (Liu, 2001). At one time, it was commonly believed that auditory signals resulted in about a 40-ms reaction time advantage over visual stimuli. This conclusion has subsequently been questioned, and the issue of modality superiority remains unresolved (see reviews in Kohfeld, 1971; Niemi & Naatanen, 1981). Equivocal results have been obtained and appear largely to do with different methods of equating stimulus intensity in each modality, as well as the duration and timing of stimulus intervals. Despite the debate, it is clear that auditory signals of sufficient intensity to be discerned attract attention even under conditions of distraction and therefore are well suited for presenting time-critical information. In the case of text-based controllerpilot communications discussed previously, for example, an auditory signal is likely to be necessary to alert the pilot to look at the text display. Because auditory signals are omnidirectional, they can be processed even when a person is not looking at them. For this reason, they may often be used in conjunction with visual displays to initiate and direct attention to time-critical information in a visual display (Chan & Chan, 2006).

An additional factor that influences how quickly people respond to auditory signals is the type of information presented and how it is portrayed to the listeners. Auditory signals, particularly speech, are processed slightly faster when presented to the right than to the left ear, a phenomenon referred to as the right ear advantage (REA) (Hiscock, Inch, & Kinsbourne, 1999). A REA is consistently observed for most verbal tasks (Hugdahl, Bodner, Weiss, & Benke, 2003; Shtyrov, Kujala, Lyytinen, Ilmoniemi, & Naatanen, 2000; Voyer & Boudreau, 2003), except when emotional or prosodic cues need to be processed (Sim & Martinez, 2005). As discussed in Chapter 3 , the right hemisphere is specialized for processing prosodic cues. Hemispheric specialization for nonverbal tasks is not as consistent (Hiscock, Lin, & Kinsbourne, 1996). Nonverbal tasks such as melodic pattern recognition (Cohen, Levy, & McShane, 1989) or pitch identification (Itoh, Miyazaki, & Nakada, 2003) often show a left ear advantage. Providing auditory information to the ear best suited for processing that type of information could facilitate speed of processing.
### **Continuously Changing Information **

The auditory modality is also ideal for presenting continuously changing information, such as flight path information in aviation (Lyons, Gillingham, Teas, & Ercoline, 1990; Simpson, Brungart, Dallman, Yasky, & Romigh, 2008; Veltman, Oving, & Bronkhorst, 2004) or the status of a patient’s vital functions to a nurse or physician (Watson & Sanderson, 2004). The auditory modality is ideal for presenting continuously changing information alone or to augment visual displays. Auditory presentation is particularly advantageous in environments where heavy demands are placed on the visual system.
### **Under High Visual Load **

Many activities and occupational settings require a heavy processing load on the visual system. For instance, it has been estimated that about 90% of resources required to drive are from the visual channel (Dingus et al., 1998). Clearly, there is little left to spare. Consequently, auditory displays can be used in these environments to avoid visual overload or provide additional information that would not otherwise be possible through the conventional visual channel. Heavy visual demands are also imposed on aircraft pilots during particular flight segments, such as taxiing, takeoff, final approach, and landing. Alerting pilots visually to potential hazards in the environment may add to such demands. Since many aircraft accidents and near misses occur on the airport runway rather than in the air, auditory alerts to hazards during taxiing or landing can help reduce the incidence of such “runway incursions.” Squire et al. (2010) reported such an auditory alerting system design based on a cognitive task analysis of pilot activities during taxiing and landing.

Auditory displays have also been found to be particularly beneficial in other contexts, such as for older drivers (Baldwin, 2002; Dingus, Hulse, et al., 1997; Dingus et al., 1998). Older adults take longer to extract information from a visual display and longer to shift their attention from one visual area to another (Greenwood, Parasuraman, & Haxby, 1993; Parasuraman & Haxby, 1993). Providing information through the auditory modality can help offset some of the visual-processing load in these environments. Similarly, when visual processing cannot be accomplished or ensured, either due to poor illumination or because the operator is moving about and therefore unlikely to notice changes in a visual display, the auditory modality may be a suitable alternative choice for presentation of information.
### **Displays Requiring Verbal Responses **

## The auditory modality is also preferable in many situations for which a verbal or vocal response is required. Stimulus-response (S-R) compatibilities between pairings of visual-manual and auditory-vocal tasks are well documented (Hazeltine, Ruthruff, & Remington, 2006; Levy & Pashler, 2001; Proctor & Vu, 2006; Stelzel, Schumacher, Schubert, & D’Esposito, 2006). Visual-spatial tasks (i.e., indicating which direction an arrow is pointing) are carried out faster when a manual response mode is used. Auditory-verbal tasks (i.e., classifying tones as high or low) are carried out faster and more efficiently when a vocal response is used. Hazeltine et al. found that incompatible mappings (visual-vocal and auditory-manual) were particularly disruptive in dual-task trials. They computed response cost as time needed to perform in dual-task trials minus single-task performance time. Incompatible pairings in dual-task trials generally resulted in more than twice the response cost of compatible pairings.

**AUDITORY DISPLAY CHALLENGES **

Despite the many potential benefits that auditory displays can provide, they are not a panacea and pose some challenges in design. Auditory displays can be masked when background noise is high or if listeners have hearing impairment. They can be distracting and annoying (particularly if they are poorly designed or present information that is not useful). There is also a potential to overload the auditory channel, much like the visual overload that such displays may be designed to combat.
### **Auditory Overload **

Numerous examples of the distraction that auditory signals impose in high-workload, high-stress situations have been cited in the literature. One of the best-known cases of this (at least in the human factors community) is probably the tragedy that occurred at the Chernobyl nuclear power plant in April 1986. It was not uncommon at the time for nuclear power plants to have as many as 100 or more auditory alarms that could potentially all go off simultaneously (Medvedev, 1990). It was such a cacophony of simultaneous alarms that Chernobyl plant workers faced while trying to diagnosis the crisis at hand.

Another vivid example of auditory overload was provided by Patterson (1990a). Some of the aircraft involved in accidents at the time had as many as 15 auditory warnings that potentially could all come on at the same time, some at intensities of over 100 dB. The number of auditory displays has increased dramatically since that time. According to Stanton and Edworthy (1999b), there were over 60 different auditory warnings simultaneously sounding during the Three Mile Island nuclear plant accident. In such situations, auditory warnings tend to distract attention and increase mental workload rather than reduce workload and guide effective action. This potential for auditory information to detract from visual or attentional processing rather than enhance it is closely related to another challenge for auditory displays briefly discussed in Chapter 9 .
### **Auditory Preemption **

Cross-modal task pairings are generally preferred to within modality pairings. As previously discussed, this was predicted by Wickens’ (1984, 2002) multiple resource theory (MRT) and a considerable body of evidence supporting it (Derrick, 1988; Horrey & Wickens, 2003; Klapp & Netick, 1988; Risser et al., 2006; Wickens & Liu, 1988). MRT leads to the prediction that, all other things being equal, a visual task will generally be time-shared more efficiently with an auditory task than with a visual task. However, as Wickens and Liu (1988) pointed out, there are exceptions to this general rule. Sometimes, a discrete auditory signal (particularly if it represents a task that is not extremely time critical) can disrupt a continuous visual task more than would a discrete visual task (Latorella, 1998; Wickens & Liu, 1988). Auditory stimuli have a tendency to capture or preempt attention. Providing a nonurgent secondary task in the visual modality can assist the operator in carrying out his or her own attention allocation strategy. Conversely, an auditory signal tends to divert attention and consequently may disrupt ongoing visual task performance at inopportune times.

Visual displays allow monitoring of the secondary task during periods deemed optimal by the operator rather than on the demand of an auditory signal.

In sum, complex patterns are frequently more easily discerned in auditory relative to visual format, and auditory information can be detected quickly from any direction—regardless of where the operator’s visual attention is focused. Using auditory displays in operational environments that have high visual workload can facilitate performance provided the information is designed to match the capabilities of the human operator and auditory overload is avoided. Attention is now turned to the psychoacoustic characteristics that have an impact on the effectiveness of auditory displays.
## **PSYCHOACOUSTICS OF AUDITORY DISPLAYS **

Effective auditory displays share some basic characteristics with displays in any other modality. To be effective, they must be detected and identified, allow discrimination, and sometimes be localized (Bonebright, Miner, Goldsmith, & Caudell, 2005; Sanders & McCormick, 1993; Walker & Kramer, 2004). Detection involves making sure that an operator or user is aware that a signal has been presented. Presuming the display is detected, it must also be identified and distinguished or discriminated from other sounds. For example, hearing a sound and knowing that it is an indicator that a machine has finished its series of cycles is important, but knowing which machine is finished is also important. Relative discrimination refers to differentiating between two or more signals presented close together. These first two aspects (detection and identification) are determined largely by the sensory characteristics of the signal, such as its intensity, frequency, and duration. Absolute identification involves identifying the particular class or categorization of the signal. Localization, as the function implies, involves determining where the signal is coming from. The process of locating the display or its referent may or may not be essential in all situations and is thus left out of some categorization schemes. Similar stages or processes have been discussed in reference to a particular class of auditory displays: warnings (Stanton & Edworthy, 1999a). Auditory warnings simply add an element of time urgency to the equation that may not be present with other forms of auditory display. Factors that have an impact on each of these key aspects are discussed.
### **Detection **

Any auditory display that cannot be heard, or for which important patterns of fluctuation cannot be perceived, can be considered completely ineffective. Designing auditory displays that are detectable without being disruptive or annoying is an ongoing ergonomic challenge (Edworthy & Adams, 1996). Early implementation of auditory alarms often took an approach of “better safe than sorry” (Patterson, 1990a). Detection was ensured by creating alarms with high intensities. If alarms or displays are made loud enough, detection is virtually guaranteed. However, as Patterson pointed out, high intensities also cause a startle response and can prevent communication in time-critical situations.

Startle responses can be elicited from sudden acoustic stimuli of at least 85 dBA (Blumenthal, 1996). In laboratory settings, the startle response is most frequently associated with the eye-blink response as measured by electromyographic (EMG) activity. However, in tasks outside the laboratory, such as during driving, a startle effect may result in not only an eye-blink response but also the prepotent response of grabbing the steering wheel tightly and quickly applying the brakes. This is obviously not always the optimum response, and any time spent recovering from the startle effect is time lost for responding to a critical event.

Detection and perception of the display depend on a complex interaction between the acoustic characteristics of the sound, the listening environment, and the hearing capabilities of the listener (Walker & Kramer, 2004). As Edworthy and Adams (1996) pointed out, in quiet environments an auditory signal can easily be too loud, while in noisy environments signals can be inaudible and therefore missed. Therefore, the key to whether an auditory display can be perceived depends on its masked threshold rather than its intensity (Stanton & Edworthy, 1999a).

Recall from Chapter 3 that any background noise or unwanted sound can obscure or mask a target sound. Because of upward spread of masking, the impact of background noise will tend to be greatest on frequencies at and above the spectral makeup of the noise. Since engine noise (i.e., jet engines) tends to have a high concentration of low-frequency noise their potential to mask target sounds is particularly high.

Compounding the situation is that typical work environments often have unpredictable levels of noise; therefore, the optimum appropriate amplitude level for an auditory display or warning may vary from moment to moment. (See Edworthy & Adams, 1996, for a discussion of remedies for fluctuating background noise levels.) Patterson (1982) developed a set of guidelines for auditory warning systems in aircraft that have had widespread application in a number or occupational environments, including aviation, surface transportation (Campbell, Richman, Carney, & Lee, 2004), and medical facilities (Edworthy & Hellier, 2006a; Mondor & Finley, 2003; Sanderson, 2006). The aim of his guidelines was to ensure detection and recognition of warning signals while reducing disruption to cognitive performance and flight crew communication. A computerized model called Detectsound has also been developed (Laroche, Quoc, Hetu, & McDuff, 1991). An additional feature of the Detectsound program is that it takes into account the reduced hearing sensitivity common among older adults. Improving detection while minimizing distraction and annoyance remains an ongoing challenge in many operational environments.

To make sure that auditory displays (and warnings in particular) are audible, Patterson (1982, 1990a) recommended that they contain four or more spectral components that are at least 15 dB above the ambient background spectral frequencies. Figure 11.2 provides an example of the application of Patterson’s (1982) method of assessing background frequencies in 0.01-kHz increments—using the spectral components of a Boeing 727 in flight—to assess the optimal range for auditory warnings.

Additional standards and design guidelines for ensuring alarm detection include international standard ISO (International Organization for Standardization) 7731, “Danger Signals for Work Places—Auditory Danger Signals,” (ANSI, 2003) military standard 1472C, “Human Engineering Design Criteria for Military System, Equipment, and Facilities” (DOD, 1981), and IEC (International Electrotechnical Commission) 60601-1-8, 2006, which provides guidelines for alarm systems in medical equipment. In general, these guidelines recommend that auditory warnings should be 15–20 dB(A) above ambient background noise levels, with ISO 7731 considering the spectral content of the background noise and recommending levels of 13 dB or more above the masked threshold in one or more octave bands (see Begault, Godfroy, Sandor, & Holden, 2007). These guidelines, although primarily aimed at auditory warnings, can be used to ensure that other forms of auditory displays are audible as well.

**FIGURE 11.2 **Patterson detectability.

A report compiled by the Federal Highway Administration (FHWA) has also composed guidelines for ensuring that auditory displays are both detectable and identifiable when used in in-vehicle automotive displays (Campbell et al., 2004). The interested reader is referred specifically to Chapter 6 of the report, “The Auditory Presentation of In-Vehicle Information.” Recommendations found in this report are based largely on empirical results with detectability recommendations from Patterson (1982).

In summary, to promote detection, critical display sounds should contain four or more spectral components at least 15 dB above the background auditory levels while avoiding sounds louder than 90 dBA.
### **Identification and Discrimination **

When an operational environment includes numerous auditory displays or alarms, identifying what any given sound means can be onerous, if not impossible. Time spent trying to recognize what a sound is designed to represent detracts from effectiveness by adding to the time necessary to respond to the sound. Therefore, the number of time-critical displays or alarms should be limited to a small, finite set. In fact, the potential advantage of signifying different states with multiple unique alarms may be offset by the increased time spent in identifying what each alarm is meant to represent. For example, master alarms (one alarm signifying a number of different critical states) may be preferable to multiple individual alarms in some settings (Cummings, Kilgore, Wang, Tijerina, & Kochhar, 2007). One method of combating the recognition/identification issue that is increasingly being considered is mapping the display or alarm sound to something that represents the natural or inherent sound that the situation or failure is designed to represent. This method, involving the use of auditory icons such as screeching brakes or breaking glass to represent the sound of an impending crash, has been shown to improve alarm response in a number of settings. For example, such auditory icons improve crash avoidance when used in invehicle collision avoidance systems (Belz et al., 1999; Graham, 1999).

In an aviation context, the use of auditory icons (i.e., such as coughing to indicate carbon monoxide exposure) resulted in faster learning of alarm associations and resulted in greater identification accuracy during a test phase in both high- and low-workload situations relative to abstract alarm sounds (Perry, Stevens, Wiggins, & Howell, 2007). Earcons—melodic sequences that must be learned—may not be beneficial, particularly if they signal infrequent events or if many different states must be discriminated.

Attempts to use earcons to aid learning and recognition of alarms among nurses in a medical context met with relatively little success (Wee & Sanderson, 2008). Recognizing numerous multiple alarms by learning a specific melodic pattern resulted in error-prone and slow performance, particularly when tested under divided-attention conditions.

Patterson (1990b) pointed out that immediate action warnings should be kept to a maximum of about six in any given work environment. These immediate action warnings are ones such as fire alarms that will gain attention through “sheer brute force.” They should each be distinct in both melody and pattern so that confusion does not ensue. Prior to the establishment of Patterson’s guidelines for auditory warnings, pilots apparently complained—and rightfully so according to Patterson—that there were too many existing alarms, they were too loud (as high as 100 dB or more), and too confusing. Further, there seemed to be no coherent ordering of the sounds used, such that two or more could come on simultaneously, with the combined sound preventing identification of either individual warning. Patterson noted that a similar situation plagued operating rooms at the time. Often, patients could be linked to as many as 10 different systems, each with its own auditory warning sounds. The result was a discordant symphony of sound that, as discussed further in this chapter, nurses often cannot discriminate.
### **Localization **

Auditory displays can be used to assist operators with tasks requiring localization of targets or other objects or to keep track of where things are in operational space. Tremendous progress has been made in understanding how the use of spatial and three-dimensional (3-D) audio displays can have an impact on performance. The auditory system is intrinsically suited for localizing and perceptually grouping objects in the environment. This ability can be capitalized on in designed auditory displays to present location information pertaining to critical and potentially interacting or conflicting elements. Auditory displays providing location information for real-world sound sources can be of benefit in a wide range of occupational

settings, including ATC, traffic collision avoidance for pilots and drivers, telerobotics in hazardous environments, and any other environment where visual cues are limited or overly taxed (Wenzel, Wightman, & Foster, 1988). In addition to location information, auditory spatial cues enhance the feeling of presence in virtual reality (VR) displays. Auditory cues may even enhance neurobiological presence—such as increased hippocampal activity (previously shown to be related to both spatial navigation and learning in general) when viewing scenes of moving through space and locating objects relative to viewing the scenes without auditory information (Andreano et al., 2009).
#### **Spatial Audio Cues **

Spatial audio or 3-D audio cues can aid localization in a number of different situations (Begault, 1991; Begault et al., 2007; Begault & Pittman, 1996; Bolia, 2004; MacDonald, Balakrishnan, Orosz, & Karplus, 2002). For example, 3-D audio significantly reduces the time needed to acquire visual targets (Begault & Pittman, 1996; Veltman et al., 2004), improves speech intelligibility in noise, and enhances realism in virtual environments (Begault, 1993). Spatial audio makes use of the sound localization cues discussed in Chapter 3 . Namely, these are interaural time differences (ITDs) and interaural level differences (ILDs) as well as cues provided by the outer ear (called the pinna) and the head. These direction-dependent acoustic cues can be used to compute a head-related transfer function (HRTF). HRTFs simulate the acoustic effects of the listener’s head, pinnae, and shoulders, and they can be used to synthesize location information through speakers or headphones. Generally, 3-D communication systems make use of air conduction (sound waves traveling through air), but bone conduction spatial audio transmission has also been considered since it has some advantages in certain environments (MacDonald, Henry, & Letowski, 2006). For example, wearing extensive hearing protection (i.e., insert devices in combination with earmuffs) makes sound localization poor (Brungart, Kordik, Simpson, & McKinley, 2003). Bone conduction transmission localization interfaces can be used in extremely noisy environments and when hearing protection devices are being worn, conditions often found in military field operations (MacDonald et al., 2006). However, air transmission is still more prevalent, and therefore unless specified otherwise, that is the form referred to here.

The ability to localize real versus virtual sounds was explicitly studied by Bronkhorst (1995). Although localization accuracy is better for real versus virtual sounds, accuracy improves if individualized HRTFs are used and for left-right judgments relative to up-and-down discriminations. Allowing listeners to move their heads and making sure that the sound source is of sufficient duration also increases the localization of virtual sound sources (Bronkhorst, 1995).

Several experiments investigated 3-D audio as a means of improving collision avoidance and other target detection tasks in visually demanding flight tasks. For example, Veltman et al. (2004) found that providing 3-D audio cues assisted pilots with an aircraft pursuit task that involved following the flight path of a target jet while maintaining a specified distance. 3-D audio cues can improve accuracy and response time in aircraft target detection tasks and can decrease reliance on headdown displays (Oving, Veltman, & Bronkhorst, 2004; Parker, Smith, Stephan,

Martin, & McAnally, 2004), resulting in decreased mental workload and improved situation awareness. Oving and colleagues observed that the combination of semantically informative (e.g., “up”) 3-D spatial cues resulted in particularly dramatic improvements in detection times relative to a visual-only display and either semantically informative or 3-D cues alone.
#### **Spatial Auditory Alerts **

Spatial auditory alerts have met with less success as an efficacious method of decreasing crash rates in in-vehicle warning systems (Bliss & Acton, 2003; Ho & Spence, 2005). It is reasonable to think that spatially predictive alerts might guide visual attention, which could potentially aid appropriate crash avoidance responses. However, Ho and Spence (2005) observed no benefit in either response time or accuracy for making a collision avoidance response from a spatially predictive nonverbal audio cue in a driving simulation task. Conversely, nonspatial but semantically informative (e.g., front, back) verbal warnings did produce significantly faster response times than the spatial audio cues. Further, similar to the results of Oving et al. (2004), Ho and Spence observed additive benefits by providing the semantically informative verbal warning from a spatially predictive location. Verbal directional words presented from a spatially congruent direction produced even faster response times than nonspatial verbal directional warnings.

Bliss and Acton (2003) observed that reliable spatially predictive cues increased the likelihood that drivers would swerve in the correct direction but appeared to have no effect on their ability to avoid having a collision. However, in their simulated driving paradigm, the spatial predictability of the cue may have been confounded with overall alarm reliability. They manipulated alarm reliability for both a spatial and a nonspatial alarm. In two experiments, they observed the best collision avoidance behavior when the alarm was reliable only 50% of the time, compared to 75% or 100% reliability. This surprising result may have stemmed from the fact that participants were told ahead of time the reliability of the alarm system and experienced either 6, 9, or 12 collision events (in the 50%, 75%, and 100% reliability conditions, respectively) over the course of a 20-min drive. The frequency of the collision events (participants had to look in a rearview mirror and determine which direction to swerve for a car approaching from the rear) is rather unrealistically high and may partially explain why participants in the 100% reliable alarm conditions (regardless of whether the alarm provided spatial information) experienced more collisions than those in the 50% reliable condition. In the 100% reliable condition, participants would have been experiencing a potential collision event approximately once every 100 s relative to once every 200 s in the 50% reliable condition.

Alarm reliability has been shown to have a significant impact on driver collision avoidance response (Cummings et al., 2007). However, Cummings et al. did not find spatially predictive audio warnings to result in improved accuracy or response time over a nonspatially predictive auditory warning in their simulations. So, to date research regarding the potential benefits of spatially predictive auditory collision avoidance alarms has been equivocal at best, with the strongest support at present being for nonspatial but semantically informative cues. Barrow and Baldwin (2009) also found semantically informative directional cues to aid response time.

Importantly, though, since warning systems will not always be reliable, when the semantic directional word guided attention in an incorrect direction (e.g., the word “left” was presented when a response from stimuli to the right was required), the incongruity led to a significant increase in response time relative to a congruent semantic word or a neutral word. The incongruent semantic word resulted in greater response time detriment than an incongruent nonverbal spatial cue. This observation was replicated for a majority of participants in a subsequent study. However, Barrow and Baldwin (in preparation) noted strong individual differences in the impact of incongruity in the two conditions. That is, some individuals experienced differentially greater disruption from incongruent semantic information, while others were more disrupted by incongruent spatial information. This pattern of individual differences could be predicted based on a wayfinding strategy questionnaire developed by Kato and Takeuchi (2003). Further research is needed to determine whether spatially predictive semantic cues of imperfect reliability would improve or hinder effective collision avoidance behaviors or if such systems should ideally be adaptable based on individual differences in spatial orientation.

Spatial audio can also be used to assist in monitoring the activity of several different people in real or virtual space. For example, separating voices along the azimuth improves both detection and recognition of the speech signal relative to diotic (binaural) listening conditions or when all the signals are presented from the same location (Brungart & Simpson, 2002, 2005; McAnally & Martin, 2007). Brungart and Simpson (2005), for instance, found that listeners were over three times as likely to accurately identify which of seven different virtual speakers had presented a verbal instruction when the virtual speakers were presented in 3-D audio rather than nonspatialized audio.

In sum, auditory displays have the advantage of reducing the frequency with which visual attention must be diverted from more primary tasks (such as monitoring the position of an aircraft). In addition, sound naturally captures attention, and auditory displays in general have the advantage of being able to be presented simultaneously in different directions, freeing the operator from remaining in a stationary position to extract the signal. Auditory displays have strong potential for reducing visual and mental workload and for improving situation awareness if well designed. So far, we have been discussing all forms of auditory displays, although much of the discussion has included examples of auditory alerts and alarms. Now, attention is directed specifically to this important category of auditory display.
## **AUDITORY WARNINGS **

Auditory alerts and warnings are frequently used to present time-critical information during periods of high workload and stress. The alerting capabilities inherent in the auditory modality as well as the omnipresent (vision-free) nature of audition make it well suited for these types of time-critical situations. Unfortunately, however, many past applications have added auditory alerts and warnings one at a time as needed, with little attention paid to the effective integration of auditory warnings and alerts into the existing system. Lack of integration can result in auditory displays having the exact opposite effect for which they are intended, thus increasing workload by causing startle responses, distracting the operator from concentrating on time-critical tasks in the event of an emergency situation, and simply being too numerous for the operator to effectively interpret in a time-pressured situation.

An examination of key principles of auditory warnings design is presented in the following section. First, a general introduction to warning design theory is presented. This is followed by an examination of both verbal and nonverbal auditory alerts and warnings in complex operational environments, such as aviation and medicine.
### **Design of Warnings **

Warnings can be viewed as an extension of human sensory systems, providing an additional venue for obtaining system state information (McDonald, Gilson, Mouloua, Dorman, & Fouts, 1999). In this sense, warnings enhance or supplement sensory input. Warnings have the greatest potential benefit in high-workload, high-stress environments in which immediate response is critical, such as on the flight decks of aircraft and in surgical operating rooms. In complex multitask environments such as these, the efficacy of warnings and alerts extends beyond mere detectability. In these environments, operators are required to process and respond to simultaneous competing sources of information from several sensory modalities. The level of technological complexity present in many modern operational environments results in the need for warning systems that extend beyond error detection aids toward integrated displays that support error recovery, error management, and team/ crew decision making (Noyes & Starr, 2000).

The warning process can be said to involve detection, encoding, comprehension, and compliance (Rogers, Lamson, & Rousseau, 2000). In more specific terms, effective warnings are compatible with human sensory, attentional, and cognitive capacities and are mapped to represent an appropriate level of urgency for the system states they represent. At a minimum, effective warnings are detectable and understandable, convey appropriate levels of urgency, and induce appropriate levels of compliance while minimizing attentional resource demands.

Ineffective warnings are inappropriately intrusive or not easily detectable, are difficult to interpret, or are improperly mapped to represent an appropriate level of system urgency. Further, ineffective warnings may require excessive attentional resources or invoke an unnecessary state of physiological stress that may interfere with decision making and corrective response. Alarms that are excessively redundant, have high false-alarm rates, or are perceived as representing an unrealistic state of urgency will promote noncompliance by increasing the likelihood that the operator will ignore the alarm, disable the alarm system, or in the event of a system state emergency, expend critical time and attentional resources to disable the warning system prior to responding to critical system components. Current warning design theory cautions against the overuse of redundant or unnecessary alarms (Rogers, et al., 2000).

Numerous investigations have established empirical support for the implementation of human factors guidelines for the design of auditory warnings, particularly for operational environments such as aviation and more recently in the area of in-vehicle systems. Unfortunately, a wide gap persists between scientific theory and application in this area. Acoustic characteristics such as intensity, frequency, pulse rate, and onset-offset time affect important warning parameters, such as perceived urgency, alerting effectiveness, and noise penetration (Baldwin, 2011; Baldwin & May, 2011; Edworthy et al., 1991; Haas & Edworthy, 1996; Hellier et al., 1993; Patterson & Datta, 1999). However, many auditory alerts and warnings are still installed in a piecemeal fashion with little attention to how the sounds will affect operator detectability, intelligibility, and workload. This can lead to numerous problems, contributing to distraction, alarm mistrust, and general ineffectiveness. Well-designed auditory warnings are capable of telling what the problem is, where the problem is, and when to expect the problem—the urgency of the situation (Catchpole, McKeown, & Withington, 2004). Ensuring detection, identification, and discrimination and potentially localization were discussed in this chapter, and these issues are particularly relevant to auditory warnings.

As discussed previously in this chapter in reference to auditory displays in general, ensuring that auditory warnings contain several spectral components (at least four) within a range of 15–25 dB above background noise levels (Patterson, 1982) is critical to ensure detection. Intensity levels lower than this may result in the warning not being heard, while intensity levels higher than this are annoying and irritating and run the risk of being disabled. Warnings need to include periods of silence to promote communication and because periodic sounds are more attention getting than continuous sounds. Pulses of sound lasting 200–500 ms with onset and offset times of 20–30 s will reduce startle while preventing unduly long sound periods. Pulses of sound of different frequencies can be combined to produce recognizable patterns of sounds in repetitive “bursts” lasting 1–2 s. Bursts of sound can be repeated, perhaps increasing somewhat in intensity if not attended for as long as necessary. Adhering to these psychoacoustic design principles can ensure detection of auditory alarms. However, once detected the sound should be recognized and, as discussed in the next section, matched to the hazard level of the situation it is designed to represent. Verbal warnings convey relative hazard levels and can easily be made recognizable but can be more difficult to make detectable in environments with high ambient noise levels. Ensuring that warnings are designed to convey an appropriate level of urgency for the situation they represent cannot be overemphasized. Fortunately, a considerable body of literature has been constructed that can inform this aspect of design.
### **Urgency Mapping **

Warning design research has emphasized the need for consideration of urgency mapping between the perceived urgency of the warning and the situational urgency of the condition represented by the warning (Edworthy et al., 1991). Considerable research has been conducted regarding the signal characteristics associated with levels of perceived urgency, particularly for design of auditory warnings. For example, the parameters of pitch, speed, repetition rate, inharmonicity, duration, and loudness have been shown to affect the perceived urgency of auditory warnings (Edworthy et al., 1991; Haas & Casali, 1995; Haas & Edworthy, 1996; Hellier & Edworthy, 1999a; Hellier et al., 1993; Momtahan, 1990; Wiese & Lee, 2004). Other things being equal, sounds that are higher in pitch, faster in rate, louder, and contain more random mismatch in harmonics are perceived as more urgent. Hellier and Edworthy presented a compelling case that existing research pertaining to these parameters and others can be used to design sounds with varying degrees of urgency for multiple-priority alerting systems. In addition to providing a solid design example, they discussed the results of two investigations that validated the use of guidelines from the existing literature (Edworthy et al., 1991; Patterson, 1982).

Hellier and Edworthy (1999a) constructed a set of nine auditory sounds, which they labeled “attensons.” The set was constructed to be similar, but not identical, to warning sounds in actual use in work environments. Using the design guidelines, they constructed three attensons at each of three urgency levels, representing the most critical (Priority 1), moderate urgency (Priority 2), and least urgent (Priority 3). They then validated the newly constructed attensons by obtaining urgency ratings for each. Results indicated that attensons ranged in urgency according to the priority levels they were designed to represent, providing strong support for the use of the guidelines in achieving appropriate urgency mappings.

Edworthy and Adams (1996) pointed out that urgency mapping is particularly critical in high-workload environments in which a multitude of alarms may be present. With appropriate mapping, the apparent urgency of the situation may be assessed instantaneously, thus providing a decisional aid for determining how quickly the operator must divert attention to the malfunctioning system.

In general, a relationship between perceived urgency and response time has been observed such that as perceived urgency increases, response time decreases (Haas & Casali, 1995).

However, the evidence for this relationship is far from unequivocal. For example, when auditory warnings of different designed urgency level were presented in combination with automated or manual tracking, Burt, Bartolome-Rull, Burdette, and Comstock (1999) found no differences in reaction time between different levels of warning urgency. Interestingly, subjective ratings of perceived urgency obtained from participants before the experiment followed the expected pattern, with higher urgency ratings given for warnings designed to signify higher priority and correspondingly lower ratings for moderate- and low-priority levels. However, following participation in the dual-task warning response and tracking trials, no differences in urgency ratings were obtained across the three priority levels. Participants appeared to reassign urgency ratings based on task demands. This suggests that urgency levels achieved in work environments after exposure to contextual factors may not always match those obtained in laboratory rating tasks.

Other investigations have found the expected relationship between urgency ratings and response time. For example, decreasing the number of pulses or bursts per second increases the perceived urgency of a sound (Edworthy et al., 1991; Hass & Edworthy, 1996). Suied, Susini, and McAdams (2008) found that this same parameter (the interval between two pulses of sound) decreased response time to the sounds. Further, the association between urgency level and response time was more pronounced in divided-attention conditions.

In addition to burst rate, a number of other acoustic parameters have been shown to affect perceived urgency. Fundamental frequency and pitch range are two important parameters (Edworthy et al., 1991). Additional psychoacoustic properties that have an impact on perceptions of the urgency of a sound include its harmonic series, amplitude envelope, and temporal and melodic parameters, such as speed, rhythm, pitch range, and melodic structure (Edworthy et al., 1991). These psychoacoustic parameters can be combined to construct nonverbal auditory warnings with predictable levels of urgency. Less attention has been systematically placed on the influence of alarm intensity, perceived as loudness, on perceptions of urgency. In one notable exception, Momtahan (1990) systematically manipulated intensity in conjunction with interpulse interval (IPI), number of harmonics, spectral shape, fundamental frequency, and frequency glide. While IPI, spectral shape, frequency glide, and the number of harmonics all had an impact on urgency, loudness independently influenced perceived urgency. Specifically, sounds presented at 90 dB were rated as significantly more urgent than sounds presented at 75 dB, regardless of how the other sound characteristics were manipulated. Loudness also interacted with a number of other sound characteristics, a result that has been supported in other recent investigations (Baldwin, 2011; Baldwin & May, 2011). Although there is some evidence indicating that loudness has an impact on the perceived urgency and annoyance of an auditory alarm (Haas & Edworthy, 1996; Momtahan, 1990), loudness is generally examined in conjunction with other parameters (i.e., loudness and pulse rate are both varied simultaneously) rather than being systematically varied while holding other parameters (i.e., pulse rate) constant.

The interaction between individual acoustic parameters (i.e., frequency and pulse rate) is one of the many issues that make designing sounds so that their perceived urgency matches the actual hazard level a very complex process (see reviews in Edworthy & Adams, 1996; Haas & Edworthy, 2006; Stanton & Edworthy, 1999b). An additional challenge that has received considerably less attention is the potential influence of contextual factors on perceptions of urgency. Research has established that when an auditory warning is presented in the absence of appropriate context and hazard level, highly urgent sounds are also highly annoying (Marshall, Lee, & Austria, 2007; Wiese & Lee, 2004).
#### **Urgency in Context **

An example of this contextually based urgency-annoyance trade-off is provided by a study by Marshall et al. (2007). They obtained ratings of the urgency and annoyance of warnings as a function of their auditory characteristics after providing listeners with a description of a driving context. Listeners were asked to imagine that sounds were coming from one of three different in-vehicle systems ranging in urgency: (a) a collision avoidance system, (b) a navigational system; or (c) an e-mail system. Marshall and colleagues examined numerous acoustic parameters (i.e., pulse duration, IPI, alert onset and offset, sound type) and found a relatively consistent pattern indicating that as sounds were rated as more urgent, they were also rated as more annoying. At the same time, listeners utilized the contextual descriptions indicating that highly urgent sounds were more appropriate in highly urgent driving scenarios, and they provided higher annoyance ratings for urgent sounds in the low-urgency driving scenario. The psychoacoustic parameters of pulse duration, IPI, alert duty cycle, and sound type influenced perceived urgency more than annoyance. Nevertheless, they found a strong relationship between ratings of urgency and appropriateness in the collision avoidance scenario and between ratings of annoyance and appropriateness in the low-urgency e-mail scenario. These results confirmed that both psychoacoustic properties and context will have an impact on perceptions of auditory warnings.

Baldwin and May (2011) examined the impact of performing a contextually related task on ratings of the perceived urgency and annoyance of verbal collision avoidance warnings. A low-fidelity simulated driving task was used to provide a limited situational context. The task provided limited situational context because the warnings were not actually related to events within the driving scene, but participants were encouraged to prioritize the driving task at all times and to respond and rate warnings only when they could do so without disrupting their driving performance. Within this context, louder warnings (+10 dB signal/noise [S/N] ratio vs. -2 dB S/N ratio) were rated as more urgent and were responded to faster but were also rated as more annoying, thus confirming previous results obtained with ratings only.

Baldwin and May (2011) used a high-fidelity driving simulator to examine the impact of acoustic and semantic warning parameters on collision avoidance behavior. The signal word *notice *or *danger *was presented at one of two loudness levels (70 or 85 dBA) immediately prior to the onset of an event with a high probability of a crash. An interaction between acoustic and semantic characteristics was observed. The two combinations designed to be of intermediate urgency (danger at 70 dB and notice at 85 dB) resulted in significant reductions in crash probability. Neither the low-urgency (notice at 70 dB) nor the most urgent (danger at 85 dB) warning reduced crash probability. Results supported the importance of both appropriate hazard matching and the need to avoid startling operators in contextual settings. Cognitive issues, including contextual factors, also play a significant role in the learning and recognition of auditory warnings.

Auditory warnings come in many types. The type implemented will have ramifications for cognitive factors such as how easily the warnings are learned (e.g., see a review in Edworthy & Hellier, 2006b) as well as recognized after initially being learned (Petocz, Keller, & Stevens, 2008). One broad classification that will be used here involves distinguishing between nonverbal and verbal auditory warnings. Nonverbal warnings can then be further divided into categories such as auditory icons versus abstract sounds.
### **Warning Types **

Nonverbal warnings can be classified in several ways other than the verbal-nonverbal distinction. As with any auditory display, a warning sound may be intentional or incidental. Incidental sounds that accompany a critical system state may not be considered warnings at all in some classifications (Edworthy & Adams, 1996; Haas & Edworthy, 2006). Intentional warnings are those that are designed. They consist of sound added to signify critical states or changes in state to a system or piece of equipment (Haas & Edworthy, 2006). Intentional warnings are the focus of the present discussion. Auditory warnings, like visual warnings, may be abstract or representational (Blattner, Sumikawa, & Greenberg, 1989; and see discussion in Stanton & Edworthy, 1999a). Traditionally, warnings have tended to be abstract sounds (i.e., bells, buzzers, sirens, and klaxons). However, representative sounds are gaining increasing attention, if not implementation.

Warning sounds can further be classified into symbolic, nomic, and metaphoric (Gaver, 1986). Symbolic sounds are abstract, that is, they are arbitrary learned mappings (abstract symbols). Nomic and metaphoric are categories of representational sounds. Nomic sounds are derived from the physics of the object they represent, much like an auditory photograph. Metaphoric sounds are somewhere in between the symbolic and nomic categories. These types of sound refer to metaphorical similarities of the physics of the object, rather than completely arbitrary mappings.

Another classification system includes four main categories: (a) conventional nonverbal sounds; (b) earcons, which are sounds having musical qualities and learned associations (Blattner et al., 1989; Brewster, Wright, & Edwards, 1995); (c) auditory icons, which are environmental or representational sounds (Belz et al., 1999; Graham, 1999); and (d) speech (Edworthy & Hellier, 2006b; Hellier et al., 2002). See the work of Edworthy and Hellier (2006b) and Petocz et al. (2008) for reviews of several other classification schemes. Edworthy and Hellier pointed out that the lack of an established taxonomy of auditory warnings is an indication of the relative infancy, compared to visual cognition, of the field of auditory cognition. More research and further refinement of classification taxonomies and methods are needed. However, as discussed in the following sections, a number of strides in this direction are currently under way.

Auditory icons and speech warnings are learned, recognized, and retained more easily than conventional nonverbal sounds and earcons (Keller & Stevens, 2004; Petocz et al., 2008). The more direct the relationship between the warning sound and its referent, the more easily the warning is both learned and retained. Additional characteristics of both nonverbal and verbal warnings are provided in the following sections.
#### **Nonverbal Warnings **

To date, the overwhelming majority of auditory warnings have been nonverbal. Traditionally, these tended to be bells, buzzers, and sirens. More recently, new classes, including designed attensons, auditory icons, and earcons, have received considerable attention, research, and application.

*Attensons *

Edworthy and Hellier (1999) have used the term *attenson *to refer to short, attentiongrabbing sounds. A well-established body of literature now exists for guiding design of attensons (Edworthy & Adams, 1996; Edworthy et al., 1991; Patterson, 1982, 1990b). As discussed, varying nonverbal parameters such as the fundamental frequency of a sound, the overall loudness (often measured in root-mean-square [RMS] intensity), and the burst rate. Edworthy and Hellier recommended using a dominant frequency range of 500–3,000 Hz to capitalize on human hearing capabilities (we are most sensitive in this range) and to decrease the probability that the noise will be startling or annoying. As discussed in several reviews (Edworthy & Hellier, 2006b; Hellier & Edworthy, 1999a), attensons can be effectively designed to be detectable and recognizable and to convey an appropriate level of urgency in a wide variety of work and leisure settings.

Auditory icons, the second type of nonverbal warning to be discussed, have received considerably less attention until quite recently.

*Auditory Icons *

Auditory icons consist of representational nonverbal sounds, or those that sound like the object, machine, or event they represent. Examples include screeching tires and breaking glass for in-vehicle collision warning systems (CWS) or a cash register sound on a Web site to indicate that an electronic payment has been processed. Auditory icons were first systematically examined for use in computer interfaces by Gaver (1986, 1989). Since that time, they have received considerable attention for use as in-vehicle auditory warnings (Belz et al., 1999; Graham, 1999). Auditory icons use representations of natural sounds to represent different types of objects or actions (Gaver, 1986). As pointed out by Graham (1999), they differ from earcons in several key ways.

*Earcons *

Earcons are abstract representations of synthetic tones. Earcons have a musical connotation not associated with a defined meaning. Therefore, the relationship between earcons and their referents must be learned (Graham, 1999). There are several notable sounds that might be associated with either complex attensons or earcons. For example, the French police siren, known to anyone who has either visited France or watched the movie of Dan Brown’s book, *The da Vinci Code *, makes a repeating two-chord sound that certainly grabs attention but has nearly a musical quality to it. More complex earcons are not in common use as warnings presently, but they have potential, particularly in medical environments, where they might be used to convey time-critical information to trained medical staff without alarming patients and family members.

The final category of warnings discussed here is verbal warnings.
#### **Verbal Warnings **

Verbal warnings have the advantage of being able to both alert and inform without imposing a heavy memory load on the part of the receiver. Further, as foreshadowed in science fiction films such as *2001: A Space Odyssey *and *The Andromeda Strain *, verbal warnings can use semantics rather than acoustics to convey the urgency of the warning. That is, synthesized voices can provide time-critical information aimed at avoiding impending disaster in a calm, monotone voice rather than in high-pitched, high-rate tones that are likely to startle the listener (Edworthy & Adams, 1996). However, there is a paucity of research regarding the use of verbal warnings in operational environments. This may be due in large part to technical challenges.

Voice synthesis technology has only recently begun to achieve acceptable designlevel standards. Natural speech presented in digitized form has several important methodological issues that must be considered for it to be utilized in operational environments. Edworthy and Adams (1996) noted that, relative to synthetic speech, natural speech is more likely to be masked by other concurrent communications present in the environment, and issues of intelligibility and detectability need further research.

*Masking of Speech Signal *

As previously discussed, for an auditory display or warning to be audible in noisy operational environments, it should contain multiple spectral components with intensity levels in the range between 15 and 25 dB above ambient background noise levels. The dominant frequencies for speech recognition (~400–5,000 Hz) include the frequency range at which humans hear best. This frequency range is also one that is the least susceptible to age-related threshold increases. The positive side of this feature is that, in favorable listening conditions (i.e., low-noise conditions), the auditory signal is presented within a listeners’ most sensitive range, but the downside is that design flexibility is consequently reduced. A speech signal cannot be altered significantly to contain additional frequencies outside the normal range to accommodate a noisy environment without significant distortion to the signal, potentially rendering it unrecognizable.

Older individuals and people with hearing loss are particularly susceptible to the difficulties of perceiving speech in the presence of noise. Frequencies at the high end of the speech range (>~3,500 Hz) are critical for distinguishing between consonants, a task that is critical to recognition (Botwinick, 1984; Villaume et al., 1994). Recall that due to upward spread of masking, frequencies slightly to moderately above the frequency range of the noise are more susceptible to masking. This makes recognition of consonant sounds—which are critical to speech recognition—more susceptible to masking than lower-frequency sounds. Not surprisingly, people with high-frequency hearing loss have more difficulty understanding speech in the presence of noise (Amos & Humes, 2007; Helfer & Wilber, 1990).

Depending on the type and level of background noise, some potential masking issues can be countered by using synthetic speech in a slightly different frequency range. However, there are limits to what can be achieved since altering the frequency range too much would unduly distort the speech signal. In addition, there is a potential trade-off in recognition accuracy.

*Synthesized and Digitized Speech *

Digitized natural speech is easier to recognize and comprehend than synthesized speech (Simpson & Marchionda-Frost, 1984; Simpson & Williams, 1980). The trade-off is that synthesized speech requires less data storage than digitized speech and may be generated on an as-needed basis. System capabilities and requirements will thus dictate the use of each. For verbal displays requiring an extensive vocabulary (i.e., in-vehicle route guidance systems [RGSs]), synthesized speech displays will be required. Use of a limited vocabulary set, which is more indicative of verbal warnings, may benefit from the use of digitized natural speech. Limited use of key signal words such as *danger, caution *, and *notice *or terse advisory commands in aviation such as *pull up *to signal critical low-altitude states are examples of where it may be possible to use digitized rather than synthesized speech.

Both digitized natural and synthetic verbal warnings have the advantage of being able not only to alert an operator but also to convey information. Verbal warnings reduce learning time relative to nonverbal warnings, and an extensive body of literature now exists to facilitate designing verbal warnings that vary in perceived urgency level.

*Verbal Urgency Mapping *

The importance of urgency mapping was discussed previously in this chapter. We return to this important topic and discuss specific empirical findings and guidelines relevant to verbal urgency mapping. One of the first systematic examinations of hazard level and verbal warning parameters was conducted by Wogalter and colleagues (Wogalter, Kalsher, Frederick, Magurno, & Brewster, 1998). Wogalter had listeners judge the connoted hazard level of various signal words presented under one of three voice styles (monotone, emotional, and whisper) at one of two presentation levels (60 and 90 dBA) spoken by a male or female. The signal word, *danger *received a higher hazard rating than *warning *and *caution *, which did not differ. *Notice *received the lowest hazard rating. The hazard level (also termed the perceived urgency) of spoken signal words has been confirmed by subsequent research (Baldwin, 2011; Hellier et al., 2002; Hellier, Wright, Edworthy, & Newstead, 2000).

Hellier et al. (2002) examined the relationship between semantic and acoustic parameters of spoken words. Male and female actors spoke signal words in an *urgent, nonurgent *, and *monotone *style and listeners rated the perceived urgency of each of 10 warning signal words, including *deadly, danger, warning, caution, risky, no, hazard, attention, beware *, and *note *. Hellier found that speaking style, signal word, and gender of the speaker all significantly affected ratings of perceived urgency. Words spoken in the urgent style were rated as more urgent than those spoken in the monotone style. In rank order, *deadly *was perceived as more urgent than *danger *, followed by *warning *. No differences were found between the perceived urgency of the signal words *warning *and *caution *. Further, words spoken by female speakers were rated as more urgent relative to words spoken by males. These results were consistent across both male and female listeners.

Hellier et al. (2002) did not specifically manipulate the presentation level in their investigation. However, they did implement a careful procedural control during the recording of stimuli to ensure that the presentation levels, expressed in peak SPL (sound pressure level) were consistent across male and female speakers. The mean presentation level for both male and female actors was 80 dBA in the urgent style. Slight differences in presentation level were observed for male and female speakers in the other two conditions, for which presentation levels of 62 dBA for the male and 64 dBA for the female were recorded in the nonurgent style, and presentation levels of 58 dBA for the male and 64 dBA for the female were recorded in the monotone style.

It is possible that the slightly louder and higher-frequency characteristics of the female voice contributed to higher ratings of perceived urgency. However, from these data alone one cannot rule out the possibility that some other factor (i.e., learned associations) accounted for the different urgency ratings of male and female voices.

In sum, ample empirical evidence from psychoacoustic investigations now exists to guide the design of both nonverbal and verbal warnings, so that they are audible and can be distinguished from each other. After ensuring that auditory warnings are audible, it is essential to consider whether the warnings are meaningful to their intended recipients. That is, do such warnings convey meaningful information to allow their recipients to understand what they are designed to represent, and are they presented at an appropriate urgency level—one that matches the actual hazard level of the situation? Considerable information is currently available on the interaction of acoustic and semantic factors that have an impact on ratings of urgency. Considerably less is known regarding how different types of sounds (i.e., artificial, environmental, and verbal) will have an impact on signal-referent relationships and perceptions of urgency and appropriateness in different operational contexts. This last area is awaiting additional research.

Next, a discussion of an emerging area of auditory research is presented: sonography or data sonification.
## **DATA SONIFICATION **

Representing continuous information (such as system state) or data relations through sound is referred to as *sonification *. Data sonification is particularly useful for presenting information to the visually impaired, when visual attention must be devoted somewhere else, or when representing multiple complex relationships that are not easily amenable to visual graphical depiction (Flowers, Buhman, & Turnage, 2005). The “science of turning data into sound” (Edworthy, Hellier, Aldrich, & Loxley, 2004, p. 203) is a burgeoning area of applied auditory research efforts. Data sonification has strong potential for application in both medicine and aviation.

One of the most important factors in sonification involves parameter mapping, or determining which specific data dimensions should be mapped to an acoustic characteristic (Edworthy & Hellier, 2006b). An example would be identifying which sound parameters (i.e., pitch, loudness, tempo) should be used to represent specific data dimensions (i.e., temperature, pressure, rate, etc.).

Effective parameter mapping involves finding out which sound parameters are best for representing different types of dimensions. This area has proven more challenging than was originally thought. Intuitive guesses for parameter mapping do not generally lead to the most effective performance outcomes (Walker, 2002). A related and equally important issue is determining the most effective polarity for the mapped parameters. That is, should increases in the sound parameter be associated with increases in the dimension (i.e., increasing pitch to represent increased temperature). This positive mapping (Walker, 2002) can be contrasted with a negative mapping in which decreases in the acoustic dimension are associated with increases in the perceptual dimension it is designed to represent. For example, a decrease in pitch might be used to represent increases in size, which would have natural representational mappings in many contexts.

Psychophysical scaling is a third important issue to consider in the parametermapping relationship (Walker, 2002). This involves determining how much the acoustic parameter should change to connote a level of change in the data dimension. Magnitude estimation has become the accepted standard for determining psychophysical scaling for data sonification (Edworthy et al., 2004; Walker, 2002).

Considerable work is currently under way in the area of data sonification (e.g., Anderson & Sanderson, 2009; Harding & Souleyrette, 2010; Pauletto & Hunt, 2009). This promises to be a significant new area of application in areas as diverse as determining suitable highway locations (Harding & Souleyrette, 2010), physical therapy and rehabilitation (Pauletto & Hunt, 2009), as well as aviation and medicine. Several reviews discussing the state-of-the-art work in this area have been published (see Flowers et al., 2005; Kramer et al., 1999; Walker & Kramer, 2005).

In the remainder of this chapter, applications of auditory displays are discussed within the context of three environments: aviation, surface transportation, and medicine.
## **AUDITORY DISPLAYS IN COMPLEX ENVIRONMENTS **

Aviation, surface transportation, and medical operations are three complex environments that have been the focus of considerable human factors work. The issues pertaining to auditory displays that have been discussed so far in this chapter are applicable, for the most part, in each of these environments. Yet, each has some unique characteristics along with the commonalities, and each environment has been the focus of considerable research and application in the area of auditory cognition.
### **Aviation **

Dramatic changes in cockpit displays and warning systems have taken place over the last several decades. Advances in avionics capabilities have transformed both the display format and the quantity of information provided to today’s flight crew. Considerable research has been conducted regarding the efficacy of the warning and alerting systems currently in existence. Human performance capabilities and the mental workload requirements of these systems have been examined both in the laboratory and in the field.

Advances in technology and the advent of the “glass cockpit” have led to increasingly complex interacting systems in the cockpits of modern aircraft ( Figure 11.3 ). Cockpit automation requires pilots to divide attention between traditional flight tasks and monitoring the system states of automated functions, often with the aid of complex multimodal displays. Numerous visual and auditory displays and warnings provide invaluable information regarding system states, automation mode changes, and traffic management. Careful consideration of the attentional workload involved in utilizing auditory displays is critical to their integration into existing cockpit designs. At present, new visual and auditory cockpit displays are often added in piecemeal fashion with inadequate emphasis placed on the overall attentional impact of their integration with other system components. What follows is a discussion of some examples of flight deck warnings that highlight some of the key issues. It is by no means a comprehensive review of the literature on auditory displays in aviation environments.

**FIGURE 11.3 **Modern glass cockpit.
#### **Flight Deck Warnings **

With the increasing sophistication and technological advances in display technologies, the number of flight deck warnings has also risen sharply. As Noyes et al. (1995) pointed out, “Over the last several decades, civil aircraft warnings systems have gradually evolved from little more than a fire bell and a few lights to highly sophisticated visual warning displays accompanied by a cacophony of aural signals” (p. 2432). Noyes and colleagues also reiterated that, unlike machines, which have the potential to have their capabilities increased through technological advances, humans are limited in the extent to which their human information-processing capabilities can be enhanced.

Auditory alerts and warnings are used extensively in the modern cockpit to present time-critical information to pilots. For example, traffic collision avoidance systems (TCASs) and cockpit displays of traffic information (CDTIs) both supplement a visual display with an auditory warning if time-critical traffic situations are detected. Wickens and Colcombe (2007) demonstrated that even when these types of systems were prone to false alarms (presenting alerts when no real danger was present), they tended to improve overall flight control performance. The alerts decreased the mental workload of the traffic-monitoring tasks so that pilots could devote more resources to primary flight control tasks.

Another key area in which auditory cognition plays a critical role is in communication between the flight crew and ATC personnel. Considerable attention has been placed on reducing mental workload and errors in this important area.
#### **ATC Communications **

Communicating with and remembering ATC commands is a cognitively demanding task for both the pilot and the air traffic controller. The ATC communication task can be particularly challenging either in high-workload conditions or for older pilots (Morrow, Wickens, Rantanen, Chang, & Marcus, 2008). The importance of understanding various signal quality dynamics and their effects on speech processing and comprehension is illustrated by a tragedy in the Canary Islands that cost the lives of 583 individuals. The accident resulted from poor verbal communication between the pilot and the air traffic controllers (see discussion in Wickens, 1992). Rerouted, tired, and trying to navigate a crowded airport through dense fog, a KLM pilot misinterpreted ATC instructions. Believing it was safe to initiate takeoff, the KLM initiated takeoff and had just reached lift speed when it clipped a Pan American plane that was crossing the runway. Of the 249 people aboard the KLM flight, there were no survivors, and the crash is one of the deadliest on record to this date.

Despite an extensive review of ATC phraseology and new standards for restricting the use of certain terms such as *clear/clearance *and *takeoff *, auditory communication remains a pervasive safety issue in aviation settings (Hawkins, 1987). According to Hawkins (1987), an analysis in 1986 of more than 50,000 aviation accidents stored in the Aviation Safety Reporting System (ASRS) databank revealed that about 70% involved some kind of oral communication problem related to the operation of the aircraft. Despite potential for error, the auditory modality has several advantages, and speech communication continues to be an integral and perhaps irreplaceable part of many complex aviation environments.

Efforts to replace speech-based ATC communications have met with a number of hurdles. Even as technological advances are made through efforts such as nextgeneration (NexGen) aviation operations that make alternative forms of communication possible, speech-based communication within the cockpit will continue to play an important role in aviation safety. Speech-based communications will be used between personnel on the flight deck as well as to confirm nonroutine and poorly understood communication with other aircraft and air traffic controllers.

In an effort to understand some of the many challenges to speech communication in the cockpit, speech intelligibility has been investigated using a variety of methods of degradation. For example, the impact of intermittent speech disrupted by a chopping circuit on speech intelligibility has been examined (Payne et al., 1994), as well as comparisons of natural versus synthetic speech (Simpson & Williams, 1980), fast versus time-compressed speech (Adank & Janse, 2009), and after sustained periods of time on task and background noise (Abel, 2009). These are just some of the many factors that threaten aviation communications.

The NexGen air transportation system is the strategy of the Federal Aviation Administration to maintain safety and efficiency through 2025 as the national air space (NAS) becomes increasingly congested. Several new cockpit technologies have been designed in accordance with this aim. Controller-pilot data-link communications (CPDLC or data link/data com) and the CDTI are aimed at reducing congestion of overburdened radio-frequency channels, reducing communication errors, and offsetting pilot working memory load.

Pilots must often perform multiple tasks that require competing attentional demands, such as monitoring and interpreting displays, monitoring traffic, as well as interpreting information from ATC. Aviation prioritization strategies emphasize the need for pilots to aviate, navigate, and communicate (Helleberg & Wickens, 2003; Jonsson & Ricks, 1995; Schutte & Trujillo, 1996). Thus, while communication is an essential component of the piloting task, it is not the highest-priority task. Information communicated to pilots from ATC, regardless of modality, may be subjected to interference from multiple sources within the flight deck, potentially affecting the recall and execution of ATC commands.

CPDLC or data-link text communication was developed in an effort to reduce some of the errors associated with current voice communications, to reduce radiofrequency congestion, and as an initial step toward greater pilot autonomy in support of free flight. However, research to date indicates that rather than reducing communication errors, data link appears to introduce errors of a different type (Dunbar, McGann, Mackintosh, & Lozito, 2001; Issac & Ruitenberg, 1999; Kerns, 1999; McGann, Morrow, Rodvold, & Mackintosh, 1998).

*Data-Link and Voice Communications *

Data link, as an alternative to traditional voice communication, has been in existence for at least three decades and has been implemented in several air systems, particularly for transoceanic flights (Kerns, 1999). Data link is a predominantly visually based communications format that allows messages to be typed for visual display between pilots and controllers.

Essential to understanding the implications of redistributing communication workload is an examination of the interaction between voice and visual communications (or mixed). Currently, implementations of the data-link system result in pilots and controllers using both systems for different communications rather than exclusively relying on the visual-manual system. Even when visual displays are the dominant form of ATC-to-pilot communication, an auditory alert is used to signal the presence of a new visual message, and radio/speech channels continue to be used to supplement and clarify visually presented information. Such “mixed” communication environments appear to place even greater demand on attentional resources than either format alone demands.

Another complex environment that shares a number of the same communication challenges as aviation is surface transportation. While the focus of the following discussion is on one component—automobile driving—it is important to keep in mind that there are other important areas of surface transportation, including railways and runways. For the purposes of brevity, the coverage here focuses on emerging in-vehicle displays that capitalize on the auditory modality.
### **Surface Transportation **

Driving is primarily a visual-motor task. Due to the heavy visual demands placed on the driver, use of the auditory modality for presenting time-critical or supplementary information may be advantageous. Advanced in-vehicle information systems (AVISs) are rapidly making their way into the modern automobile. Many of these systems provide information through both visual and auditory displays. Three systems that rely on auditory displays as a major form of presentation are CWSs, RGSs, and lane departure and fatigue detection systems. Each of these emerging areas is discussed in turn.
#### **Collision Warning Systems **

Many automobile models currently come equipped with some form of auditory CWS. Commercially available systems currently most often utilize a visual alert in combination with an auditory alert, although previous research indicated that drivers may have lower levels of reliance on multimodal systems relative to single-modality systems (Maltz & Shinar, 2004). In simulation studies, CWSs have shown great potential for decreasing both the severity and the rate of occurrence of motor vehicle collisions (Brown, Lee, & McGehee, 2001; Graham, 1999; Maltz & Shinar, 2004). As discussed, a wide variety of auditory warnings has been investigated in this context, ranging from nonverbal attensons (Brown et al., 2001; Lee et al., 2002; Siuru, 2001) to auditory icons such as breaking glass and screeching tires (Belz et al., 1999; Graham, 1999). Research is currently under way to determine if these systems will actually reduce roadway collisions outside the laboratory. But, preliminary work is promising, particularly for certain categories of drivers, such as older drivers (Dingus, McGehee, Manakkal, & Jahns, 1997; May, Baldwin, & Parasuraman, 2006). At present, there is still great concern, and some evidence (Maltz & Shinar, 2007), that some younger drivers may misuse CWSs by overrelying on them and maintaining closer vehicle headways without sufficient visual attention. On the other hand, there is some evidence that providing an auditory alert to middle-aged drivers can be used to teach safe driving behaviors. Shinar and Schechtman (2002) provided an auditory alert to drivers whenever their temporal headway reduced to unsafe levels. Drivers learned to maintain safer following distances, and the beneficial effects observed during the initial experimental session remained at a session administered 6 months later.

Another emerging in-vehicle system that can be a great transportation aid to all, and particularly to older drivers, are the RGSs. Although they nearly always have both a visual and an auditory display, it is the auditory guidance that is particularly helpful to older drivers and, in fact, all drivers in high-workload situations. The auditory interface of these systems is the focus here.
#### **In-Vehicle Route Guidance Systems **

In-vehicle RGSs are one of the forms of complex verbal displays rapidly increasing in prevalence. Several high-end automobile manufacturers include RGSs as part of their standard option package. Optional RGSs are available in virtually all vehicles, if not from the factory, then from one of the off-the-shelf stand-alone systems widely commercially available.

These systems use global positioning satellite (GPS) technologies to track the location of the automobile and then provide navigational guidance in the form of visual maps and terse auditory commands. Drivers have demonstrated a preference for auditory commands relative to visual maps for navigation tasks in a number of investigations (Dingus, Hulse, et al., 1997; Streeter et al., 1985). Auditory guidance instructions have the advantage of allowing drivers to keep their eyes on the road. Switching visual attention back and forth from the roadway to a visual map incurs increased executive processing requirements to control task switching. The increased demand for executive processing results in switching costs normally manifested by increased response time and errors (DiGirolamo et al., 2001). Using functional magnetic resonance imaging (fMRI), DiGirolamo and colleagues have shown that task switching involves greater recruitment of the medial and dorsolateral frontal cortex. Interestingly, unlike their younger counterparts, older individuals demonstrated increased recruitment of these areas continuously during dual-task performance rather than just when switching between tasks. This increased use of executive processes during concurrent performance may be particularly problematic for older drivers, who require more time for task switching (Parasuraman & Haxby, 1993).

*Navigational Message Complexity *

Empirical evidence indicates that auditory navigational commands must be kept short to avoid overloading the driver (Srinivasan & Jovanis, 1997a, 1997b). If an auditory message is too long or too complex, it threatens to exceed the working memory capacity of the driver and not be retained (for a review see Reagan & Baldwin, 2006). Several investigations indicated that auditory route guidance messages should contain no more than three or four informational units or propositional phrases and should take no longer than 5–7 s to present (Barshi, 1997; Green, 1992; Kimura, Marunaka, & Sugiura, 1997; Walker et al., 1990). When procedural or navigational commands contain more than three propositional phrases, execution errors increase sharply (Scerbo et al., 2003), drivers make more navigational errors (Walker et al., 1990), and driving performance becomes less stable.

Srinivasan and Jovanis (1997a) suggested that to prevent excessive load on a driver’s information-processing resources, auditory directional information be provided in the form of terse commands, such as, “Turn left in 2 blocks onto Park Avenue.” In the systems currently available on the market, these types of terse commands are generally followed by even shorter commands just prior to the turn, such as, “Turn left.” Reagan and Baldwin (2006) found that including one additional piece of information, either a salient landmark or cardinal direction, could be accomplished without noticeable negative impact on simulated driving performance or memory for the instructions. In fact, including a salient landmark in the guidance statement improved navigational performance and decreased ratings of subjective mental workload relative to the standard guidance command without this information.

The FHWA guidelines, based largely on empirical research, have been established to aid designers in determining the appropriate complexity level of in-vehicle displays (Campbell et al., 2004). According to these guidelines, complexity can be thought of as falling along a continuum from high to low. For auditory displays, high-complexity displays contain over nine information units and take over 5 s to process. Conversely, low-complexity displays contain three to five information units and require less than 5 s to process.
#### **Fatigue and Lane Departure Detection Systems **

In addition to forward CWSs and navigational route guidance systems, a number of other driver assistance devices are making their way into the modern automobile. Detection systems have also been devised to detect potentially dangerous driver states, like inattention, distraction, and fatigue. A number of fatigue alerts designed to warn drivers of safety-critical sleep episodes are commercially available. Figure 11.4 illustrates an example of one of these devices. Note that it can be mounted in the vehicle. These devices may monitor eye closure (or the percentage of time the eye is closed, called *perclos *), changes in lane position variability, or even the driver’s rate of physical movements.

**FIGURE 11.4 **Artistic illustration of the Eye Alert™ Fatigue Warning System 2011.

Auditory warnings that alert drivers to inadvertent lane departures may benefit both fatigued and distracted drivers. Auditory warnings simulating the sound of rumble strips have been shown to decrease the duration of lane departure events and speed corrective steering behaviors when departures occur (Navarro et al., 2007). It should be noted that Navarro et al. found that steering wheel movements designed to prime appropriate motor behaviors were even more effective than auditory warnings alone.

The number of in-vehicle driver assistance and information systems is on the rise, and further research into how best to provide drivers with this information will continue to be of importance for many years to come (Baldwin, 2006; Baldwin & Reagan, 2009; Cummings et al., 2007; Maltz & Shinar, 2007; Reagan & Baldwin, 2006; Shinar, 2008). Another environment replete with auditory alarms is the modern medical center.
### **Medical Environments **

Medical environments are the third specific work domain that are considered with respect to auditory displays. Auditory alerts and displays are common in many health care systems. As in other high-workload environments, such as the aviation cockpit and the modern automobile, auditory interfaces allow surgeons, anesthesiologists, and other medical personnel to monitor critical information (patient physiology) while performing other tasks (Watson & Sanderson, 2004). Medical patient-monitoring equipment generally provides patient status information to medical personnel in both visual and auditory formats. Auditory displays may provide continuous information regarding patient status or intermittent alerts to signal that operators should focus attention on the visual displays.

Watson, Sanderson, and Russell (2004) described three categories of auditory information found in the surgical operating room and discussed the impact of each on attention, mental workload, working memory, and expectations. The first category is the continuous auditory display; Watson et al. (2004) provided the pulse oximeter as an example of this category. The pulse oximeter provides continuous information on a patient’s status, generally in the form of a two-dimensional auditory display, with one dimension (pulse rate) mapped to heart rate and another (pitch level) mapped to oxygen saturation.

A second category, the informative alarm signal, is designed to capture or direct the operator’s attention to a discrete event, such as an unexpected change in patient status or equipment functioning. As described in the model (Watson et al., 2004), the presence of informative alarms will decrease attentional costs and mental workload (presumably because they aid the operator in patient monitoring) but increase working memory demands since they interrupt ongoing activities. However, since they are often used to direct attention to life-threatening patient states, the added working memory demands can be viewed as worth the cost. In addition, Watson and colleagues pointed out that the auditory alarms on various medical devices are sometimes tailored by changing system settings so that they can be used as reminders to check patient status information and offset the workload of continuous monitoring.

The final category in the model of Watson and colleagues (2004) is called “alarm noise and other auditory distractions” (p. 277). Included in this category are false alarms or nuisance alarms that do not convey important patient or equipment information. Auditory information in this category has a negative impact on attention, workload, and working memory without any operator benefit. They pointed out that this category includes auditory distractions unrelated to patient status or current operating situation. Any auditory information, including continuous displays and informational displays, can be distracting during some procedures and to some individuals or work teams. Turning off all auditory alarms continues to be the most common form of alarm tailoring in the operating room (Watson et al., 2004). As is discussed in the next section, because the prevalence of nuisance alarms is so high, it is not surprising that health care workers might be tempted to turn off auditory warnings altogether.
#### **Nuisance Alarms **

Nuisance alarms continue to be pervasive in medical environments. Edworthy and Hellier (2005) illustrated this with the example of a workstation used in anesthesia. An average anesthetic workstation might have several pieces of equipment, each capable of producing 20 or more different alarm sounds. Now, multiply this by the number of pieces of equipment in an intensive care unit, and the resulting number of alarms is staggering. Alarmingly, up to 90% of these are false or nuisance alarms (Imhoff & Kuhls, 2006).

Gorges, Markewitz, and Westenskow (2009) provided a sobering real-life example of this phenomenon. They conducted an observational study of alarm sounds and behavioral responses in an intensive care unit. During their 200-h observational study, 1,214 alarms went off. This is an average of 6.07 alarms per hour directly observed in this one unit. They observed that only 23% of these alarms were effective. Thirty-six percent were ineffective (meaning no response was taken by medical personnel), while the remaining 41% were consciously ignored or disabled.

To make matters worse, Patterson (1990a) pointed out that for reasons of economy, auditory alarms too often consist of high frequencies and vary only in intensity and frequency. High frequencies are much more difficult to localize than lower speech frequencies, and high intensities are associated with greater annoyance and stress. The poor design of some auditory warnings only increases the negative consequences of nuisance alarms.

Watson and Sanderson (2004) noted that the most commonly cited problem with auditory alarms in the medical environment is that they too often present less-than- useful information at inappropriate times. Notably, they pointed out that alarms present critical state information rather than trend information, and this information generally comes at an inopportune time (i.e., high-workload times occurring as a result of multiple problems or failures). Then, to make matters worse, the anesthesiologist must waste valuable time shutting off the auditory alarm when he or she could be attending to the patient. For these reasons in particular, Watson and Sanderson (2004) suggested that a continuous auditory display presenting patient status trend data (i.e., a sonograph) may be a more effective means of presenting patient physiological data, one that allows medical personnel to maintain eye contact with the patient and perform other tasks without losing awareness of patient status. Sonographic displays, discussed in this chapter, have many applications in medical environments.
## **SUMMARY **

Auditory displays are preferable to other types of displays in a number of circumstances. For example, when the information to be conveyed is auditory in nature, when the operational environment places heavy visual demands on the operator, and when the receiver may not be able to see a visual display either because he or she is moving around or because of low visibility, auditory displays are preferable to visual displays. Despite these advantages, including too many auditory displays can result in auditory overload: Critical information is masked, and the soundscape becomes so annoying that attention and communication are disrupted. Auditory information also has the tendency to preempt visual processing—a double-edged sword. On the positive side, this auditory characteristic makes it well suited for presenting time-critical alerting information. On the negative side, presenting auditory information may disrupt performance of an ongoing visual task at inopportune times. Therefore, caution should be exercised in both the type and the timing of auditory displays. Alerting an individual or operator to time-critical information is often best carried out through an auditory display. Appropriate hazard matching between the acoustic and semantic characteristics of the sound and the threat level of the situation it represents is essential to the design of effective auditory warnings. Auditory displays are increasingly being used in such arenas as aviation, surface transportation, and medical facilities. Each of these environments not only shares some commonalities but also exhibits some unique aspects that warrant careful attention to the design and implementation of new auditory displays.
## **CONCLUDING REMARKS **

The design and implementation of auditory displays across domains represent the culmination of a considerable body of literature in the burgeoning field of applied auditory cognition. In this book, I attempted to discuss not only the importance of continued research aimed at uncovering the mysteries of human auditory processing but also what can be understood from the existing literature.

Many auditory tasks are commonly used to investigate cognitive and neuropsychological functioning, in both healthy and neurological populations. The wrong conclusions can be drawn from the results of their use unless careful attention is paid to both the stimulus quality of the material presented and the hearing capabilities of the listener. Older adults, for example, may experience communication and even cognitive difficulties that can be attributed at least partly to declining hearing capabilities. Degraded listening conditions and hearing impairment make speech understanding and other complex auditory processing tasks more effortful, which can in turn compromise performance on other important tasks, such as remembering, driving an automobile, or performing a complex surgical procedure.

Presenting redundant information through the visual channel can offset some of the load associated with processing auditory information. However, in many of the complex operational environments in existence today, rather than presenting redundant information, visual and auditory displays are used simultaneously to present independent streams of information. In complex domains such as aviation, surface transportation, and medicine, understanding principles of auditory cognition and their implications for auditory display design is essential for facilitating and maintaining effective, efficient human performance capabilities.

Sounds are everywhere in the modern world. The complex soundscape that we live in influences where we direct our attention, how we communicate with each other, and how we interact with the technological systems prevalent in modern life. As I hope the material presented in this book has illustrated, we know increasingly more of the mechanisms of auditory cognition, knowledge that can be put to good use to better design our auditory world. Yet, at the same time, much remains to be learned about auditory cognition. The future promises to reveal more discoveries concerning this essential human faculty.