# Unknown Chapter

*Source: Auditory Cognition and Human Performance by C. L. Baldwin (2012)*

---

# 4 **Auditory Cognition *****The Role of Attention and Cognition in Auditory Processing ***

## **INTRODUCTION **

In this chapter, the discussion of auditory processing begun in the last chapter is continued by examining the role that attention plays in helping to identify and select an acoustic pattern of interest. Next, higher-order processes involved in audition, which can be viewed as the real heart of auditory cognition, are discussed. Some would go as far as to call these abilities *auditory intelligence *(de Beauport & Diaz, 1996), suggesting that as humans we vary in our ability to interpret and find meaning in the sounds we hear. Viewed in this way, auditory intelligence involves the degree to which we are able to go beyond merely hearing sounds to higher-order processing—involving not only taking in sounds, but also “words, tones of voice, and arriving at a sophisticated or comprehensive meaning … and connecting inner meaning to a sound received from the outer environment” (de Beauport & Diaz, 1996, p. 45). Individuals may differ in their ability to construct meaning from sound because of talent or experience, but regardless, the same basic processes are used by all listeners.

Auditory cognition begins with our attending to an acoustic stimulus. Without such attention, further cognitive processing is unlikely. Therefore, we begin our discussion by examining attentional mechanisms in selective listening and the role that studies of selective listening have played in the development of theories of attention and information processing.
## **ATTENTION **

It should be clear by now that listening requires more than simply passively receiving an auditory stimulus. A host of complex processes is involved in extracting and making sense of the acoustic environment. Selective attention plays one such important role in auditory pattern perception. Since we are constantly being bombarded by a wide variety of sounds, more stimuli than we could ever begin to process, it is fortunate that humans are capable of selectively attending to certain acoustic sources while ignoring others. Our ability to selectively attend to one source of sound in the midst of competing messages was examined extensively during the 1950s and 1960s using the selective listening paradigm. More recently, the role of attention in auditory processing has been examined using a variety of brain imaging techniques. This topic is discussed further in this chapter.

The selective listening paradigm, or dichotic listening task as it is more commonly referred to today, involves presenting two competing messages to the two ears. Most commonly, a different message is presented to each ear. The listener is generally required to attend to one of the two messages, often being asked to repeat aloud (shadow) the contents of the attended message. Investigations utilizing the dichotic listening paradigm made major contributions to the development of early theories of attention and information processing (Broadbent, 1958). The paradigm was used to systematically examine such issues as the aspects of the sound stimulus that help in auditory stream segregation and whether and to what extent one can attend to more than one input source at a time. The study of attention remains an integral part of modern experimental psychology. However, our discussion primarily focuses on what early research on auditory attention revealed about the processes involved in auditory pattern perception.
## **EARLY ATTENTION RESEARCH **

The study of attention has a rich history. Dating at least as far back as William James (1890/1918), examinations of the role of attention in processing have been the focus of considerable theoretical and empirical research (Anderson, Craik, & Naveh- Benjamin, 1998; Ball, 1997; Barr & Giambra, 1990; Broadbent, 1982; C. Cherry, 1953a; Moray, 1969; Norman, 1976; Treisman, 1964c). It is well beyond the scope of the current discussion to review this long and fruitful history. The reader interested in more detailed coverage of research on attention is referred to one of the many excellent books on the topic (see, for example, Parasuraman, 1998; Parasuraman & Davies, 1984; Pashler, 1998b). However, a brief overview of the development of various theories of attention is relevant to the current aims. In a broad sense, this history can be divided into three primary phases: (a) filter theories, (b) capacity or resource theories, and (c) cognitive neuroscience models.

Early models of attention postulated the existence of a filter or bottleneck where parallel processing changed to serial processing. The primary distinction and point of debate between these early models was where in the processing chain the filter occurred. Early selection models (i.e., Broadbent, 1958) postulated that the filter occurred in preattentive sensory processing. Treisman’s (1964c) attenuation theory suggested that rather than an early switch, unattended information was attenuated or processed to a limited extent. Late selection models (Deutsch & Deutsch, 1963) proposed that essentially all information reaches long-term memory (LTM), but we are unable to organize a response to all of the information. Use of the dichotic listening task was instrumental in the development of these early models of attention and shed light on a number of interesting features of auditory processing.
### **Dichotic Listening Tasks **

Initial evidence for placing the filter early in the information-processing chain was demonstrated through the dichotic listening paradigm. As previously stated, this task involves simultaneous presentation of two distinct messages, usually a separate message to each ear. In the typical study, the listener was asked to answer questions about or shadow (repeat out loud) the message played to one of the two ears (Cherry, 1953; Moray, 1969). For example, Cherry (1953) implemented a dichotic listening paradigm and found that with relatively little practice participants could easily shadow one of the two messages. However, Cherry observed that participants generally had little recollection of the content of the unattended message. This finding indicated that we really cannot attend to and understand two messages at the same time. If you are listening to a lecture and the person sitting next to you begins a conversation, you must choose the information you want to understand. If you choose the neighbor’s conversation, you will miss what is being said in the lecture and vice versa. Additional key generalizations arose from these early dichotic listening investigations.

In general, it was observed that the more two messages resembled each other, the more difficult they were to separate (Moray, 1969). Differences in the acoustical or physical characteristics of the messages (such as loudness, pitch, gender of speaker, or spatial location) assist listeners in distinguishing between and thus separating messages. When two simultaneous messages are presented and listeners are provided no instructions regarding which to pay attention to, louder messages are reported more accurately (Moray, 1969). However, if listeners are given instructions, they are able to attend selectively to one message at the expense of the other (Moray, 1969). These early investigations demonstrated that selective attention is a “psychological reality, not merely a subjective impression” (Moray, 1969, p. 19). The results of these studies were applied in settings such as aviation, in which it was determined that adding a call sign (i.e., the flight number or a person’s name) could greatly assist listeners in selectively attending to one of two competing messages.

Dichotic listening investigations typically have used speech presentation rates approximating 150 words per minute (Moray, 1969). People can learn to shadow (verbally repeat) one of two competing messages rather easily, and performance improves with practice. Listeners are generally only able to report overall physical characteristics of the unattended message. For example, listeners are able to report whether the unattended message was speech or given by a male or female speaker, but generally they are not able to report the semantic content of the message or even the language of the spoken message.

Moray (1959) observed that even if a word was repeated 35 times in the unattended message, listeners did not report the word as having been present when given a retention test 20 s after the end of shadowing. Norman (1969) found that if the retention test was given immediately, listeners were able to report words presented within the last second or two. This finding provided initial evidence for storage of the unattended message in a brief sensory store.

Mowbray (1953) observed that when participants listened to one story while ignoring the other, comprehension of the unattended story was at chance level. Several researchers (C. Cherry, 1953b; Treisman, 1964c) investigated a listener’s ability to separate two identical messages presented to each ear across different presentation lags; two identical messages were presented but one began at a temporal point different from the second. With temporal lags of approximately 5 s, listeners readily identified the two messages as separate. With shorter delays, an echo-like effect was produced. For the messages to be fused (or heard as stemming from a single source), the temporal lag could be no longer than 20 ms. These findings provided additional evidence for the existence of a brief sensory store capable of maintaining an echoic trace for a period of several seconds (Moray, 1969). Findings such as these led Donald Broadbent (1958) to propose that attention was subjected to a “filter”- type mechanism resembling a bottleneck.
### **Filter Theories **

The first formal theories of information processing in modern psychology suggested that some type of structural bottleneck keeps people from being able to process all available sensory information at any given time. That is, while we are constantly bombarded by a simultaneous cacophony of sounds, we can only process some limited set of these at any given time. A structural bottleneck or filter prevents all information from being processed. The early filter theories differed in terms of where in the information-processing stream this filter or bottleneck occurred.
#### **Broadbent’s Early Filter Model **

Broadbent (1958) presented his classic bottleneck or filter theory of attentional processing in his seminal book *Perception and Communication *. The model essentially proposed that all sensory stimuli enter the sensory register and then are subjected to an attentional filter or bottleneck based on certain physical characteristics. Broadbent noted that in a dichotic listening paradigm, physical characteristics such as the ear of presentation or pitch could be used to allow one of many auditory messages selectively through the filter.

However, subsequent research provided evidence that under some circumstances more than one auditory message could be processed, at least to some extent. For instance, Moray (1959) found that sometimes when attending to one of two dichotically presented messages people were able to recognize their name in the unattended message. Moray reasoned that only information that was very important to the listener was able to “break through the attentional barrier” (p. 56).

Treisman (1960) found that when attending to one of two dichotically presented messages, subjects would sometimes switch their attention to the unattended ear to maintain the semantic coherence of the original message. That is, if the messages were switched to the opposite ear midsentence, listeners would sometimes repeat a word or two from the unattended ear, thus following the semantic context of the sentence for a short period of time. The semantic content of the “unattended” message appeared to influence the attended message for a period of time corresponding to the duration of the echoic trace (Treisman, 1964b).
#### **Treisman’s Attenuation Theory **

Treisman (1964a) proposed that rather than a filter or all-or-none switch, attention functioned like an attenuation device. Sensory stimuli would pass through the sensory register, where only limited processing occurs, and then be passed on for further processing through a hierarchical progression of sets of gross characteristics.

The physical aspects of the stimulus were the most likely to be passed through the attenuation device and could be used to select the message to be attended. This would explain why listeners are able to report gross physical characteristics of an unattended message such as the speaker’s gender. The semantic content of the unattended message would be attenuated but not completely lost.
#### **Late Selection Models **

Late selection models such as those of Deutsch and Deutsch (1963) and Norman (1968) proposed that filtering occurred much later in the information-processing stream, after information had passed on to LTM. According to late selection models, it is not so much that information is not processed but rather that we are limited in our ability to organize and select from the many representations available in LTM. So, in other words, all incoming sensory stimuli are matched with representations in LTM. The bottleneck occurs because we are incapable of choosing more than one representation at a time.

Although constructing experiments to conclusively decide between the different models has proven difficult, the investigations used to develop and examine these early theories of attention provided considerable knowledge regarding our ability to process multiple auditory messages. Perhaps as is often the case in psychological literature, the reality may be that some combination of the multiple theories explains the role of attention. Cognitive neuroscience data have recently added more to this debate; therefore, the early/late discussion is picked up in that subsequent section. Specifically, cognitive neuroscience data have provided converging evidence on when early selection is used and when late selection is possible. That is, perhaps the various filter-type (early or late) or attenuation-type mechanisms play a more dominant role in differing contexts and in different situations. Lavie’s (1995) “perceptual load” theory may provide a reconciliation of the early/late debate. For example, late selection is typically used, and both relevant and irrelevant sources are processed through to LTM, except under high perceptual load (e.g., dichotic listening with fast rates, many messages), in which case early selection is used. But, it should be noted that Lavie’s theory has mainly been validated with visual, not auditory, attention tasks (but see Alain & Izenberg, 2003).

Regardless of where the bottleneck occurs, if there is such a thing, it remains clear that humans can only respond to a limited amount of information. This notion became the cornerstone in the next stage of theories of attention.
### **Capacity Models **

Evidence that humans have a limited capacity for processing multiple sources of information was discussed extensively in a seminal article by Moray (1967) and later elaborated in a now-classic book by Daniel Kahneman (1973), *Attention and Effort *. Kahneman’s limited-capacity model suggested that attention was essentially the process of allocating the available resources from a limited pool. This pool of resources was not thought to have a fixed capacity but rather to change as a function of a person’s overall arousal level, enduring disposition, momentary intentions, and evaluation of the demands of the given task. Since the concept of a limited supply of processing resources is a cornerstone of mental workload theory, we return to a discussion of capacity theories in the next chapter in our discussion of theories of mental workload and assessment techniques. However, for now we continue with a look at the third phase in the development of attention theories, which relies on cognitive neuroscience.
### **The Cognitive Neuroscience of Attention **

The perspective gained from application of the cognitive neuroscience approach to studies of attention, auditory processing in particular, has been quite remarkable. The techniques now available for recording the electrical activity of the brain and for brain imaging provide a window into the processes and structures involved in information processing that earlier theorists like William James could only dream about. That is not to say that much cannot still be learned from use of the earlier behavioral techniques. In fact, a powerful method of exploration involves combining established behavioral methods with measures of brain activity and brain imaging. For example, the dichotic listening task is frequently used in a number of different clinical assessments, including neurological assessment for central auditory processing disorder (Bellis, Nicol, & Kraus, 2000; Bellis & Wilber, 2001). However, the behavioral results can now be complemented and extended by techniques such as recording event-related potential (ERP) components of electroencephalographic (EEG) recordings to track the time course and level of brain activity. Brain imaging techniques such as functional magnetic resonance imaging (fMRI), positron emission tomographic (PET) scans, and magnetoencephalography (MEG) have also been powerful tools of investigation. The reader interested in detailed examination of the brain mechanisms of attention is referred to the work of Parasuraman (1998). Here, the discussion is limited to a few of the topics germane to auditory attention.
#### **Event-Related Potential Indices of Auditory Attention **

Auditory attention has been investigated with neurophysiological recordings. Evoked potentials (EPs) and other ERPs of brain activity generally show increased amplitude for attended auditory stimuli, relative to ignored auditory stimuli (Bellis et al., 2000; Bentin, Kutas, & Hillyard, 1995; Hillyard et al., 1973; Just, Carpenter, & Miyake, 2003; Parasuraman, 1978, 1980). Hillyard and colleagues (1973) conducted one of the first well-controlled examinations of the role of attention in affecting auditory EPs in a dichotic listening task. They had listeners perform a tone discrimination task while attending to stimuli presented to either the left or the right ear. Tones of low frequency (800 Hz) were presented to the left ear, while higher-frequency tones (1,500 Hz) were presented concurrently to the right ear. Tones were presented at a fast rate with a random pattern ranging from 250 to 1,250 ms between each tone. Roughly 3 of every 20 stimuli in each ear were presented at a slightly higher pitch (840 for the left and 1,560 for the right ear). Participants were instructed to count the deviant tones in the attended ear and to ignore all stimuli in the unattended ear. They observed a greater negative wave deflection occurring about 100 ms (N1) after each tone in the attended ear, relative to tones presented in the unattended ear. Specifically, N1 responses to tones presented to the right ear were higher than N1 responses to left ear tone when participants were attending to the right ear; conversely, N1 responses to left ear tones were higher than those to right ear tones when participants were attending to the left ear. Hillyard and colleagues noted that the fast event rate made the pitch discrimination task difficult, and that under these conditions, listeners were able to attend selectively to the relevant source while effectively ignoring the irrelevant source.

Extending the results of Hillyard et al. (1973), Parasuraman (1978) examined the influence of task load (i.e., mental workload) on auditory selective attention. Parasuraman presented a randomized series of tones to the left (1,000 Hz), right (500 Hz), or an apparent midway position between the left and right ears (2,000 Hz) at 50 dB above threshold. Tones were presented at either a fast rate (average of 120 tones per minute or less than 500 ms between each) or a slow rate (average of 60 tones per minute or less than 1,000 ms between each). Listeners were asked to monitor one, two, or all three auditory channels for the presence of targets (tones presented 3 dB greater in intensity). At the fast presentation rate, the amplitude of the N1 component was significantly higher in the attended channels relative to the unattended channels. Importantly, however, Parasuraman pointed out that this occurred only when the presentation rate was high, which can be attributed to a situation with high mental or informational workload (Parasuraman, 1978). It has been shown that auditory selective attention is enhanced by both learning to ignore distracting sounds and attending to sounds of interest (Melara, Rao, & Tong, 2002).

The results of these studies showed that selective listening paradigms played an integral role in the development of theories of attention. Further, from these early studies much was learned about the way people process information through the auditory channel. The reader interested in more discussion of the role of attention in information processing is referred to works by Moray (1969), Parasuraman (Parasuraman, 1998; Parasuraman & Davies, 1984), and Pashler (1998b). We turn our focus now to a general look at information processing beyond the level of attending to auditory stimuli. We see that information-processing models have evolved from largely a “boxes-in-the-head” modal model of memory proposed in the late 1960s and early 1970s (Atkinson & Shiffrin, 1968, 1971) to elaborate theories grounded in models of artificial neural networks (ANNs). The development of many of these information-processing models relied heavily on an examination of how people process language. Thus, as was the case with theories of attention we will see again that the study of auditory processing both contributed to and benefited from research on information processing.
## **THE INFORMATION-PROCESSING APPROACH **

Numerous models of cognitive processing have evolved over the last half century in an effort to explain and predict how humans process information in different situations. It is beyond the scope of the current text to review each of these models in depth, and the interested reader is referred to several more detailed descriptions of the development and characteristics of these many models. (See the work of Matthews, Davies, Westerman, and Stammers, 2000, for an excellent review.) Some common properties are worthy of discussion before presenting several influential models with particular relevance to auditory processing. First, a distinction must be made between processing codes and operations.

**Processing Codes **

Codes can be thought of simply as the format of the information that is presented to the observer. Information presented through any of the senses must be translated into an internal code or representation. On the surface, this process appears superficially simple. All five sensory systems have unique processing codes. However, closer inspection reveals that information presented in the same sensory modality can be coded in different ways depending on the nature of the task being performed or the stage of processing. Auditory information can be encoded in several different ways—acoustically, phonologically, lexically, semantically, or even spatially. Similarly, while visual information typically invokes visual codes, visual stimuli involving speech or alphanumeric stimuli are frequently coded in an acoustic or phonological code (Conrad, 1964).

Processing code is included as a major dimension in Wickens’s multipleresource theory (MRT) of information processing, which is discussed further in a subsequent section (Wickens, 1984; Wickens & Liu, 1988). In addition, the same information may be coded in several ways during different stages of processing. For example, in the case of speech, the sound stimulus will be initially coded in acoustic format and then will progress to a lexical and then semantic code as processing continues.
### **Processing Operations **

Processing operations are the actions or computations performed on the stimulus information. Several types of operations may be performed on stimulus information, and these operations are typically carried out by different processing components that are now thought to occur in separate neurophysiological structures. Encoding, storage, and retrieval of the information are each separate operations that can be performed on an internal code. Encoding can be further subdivided into different operational strategies depending on the type of task to be performed. Maintaining information in short-term memory (STM) or working memory is an operation (referred to as maintenance rehearsal in some models)* that differs markedly from elaborative rehearsal, which involves relating information to existing schemas or frameworks in an attempt to permanently store information.

A number of operations may be performed on auditory information. For example, both speech and music may evoke the operations of listening to melodic patterns, temporal segmentation, rhyming, or segmentation into processing streams either automatically or intentionally. Processing operations are influenced by factors that have been divided into two primary categories, bottom-up and top-down processing.
### **Bottom-Up and Top-Down Control of Processing **

Most models of information processing recognize the influence of bottom-up and top-down processing, although they may differ on whether these two influences are thought to be sequential or interactive (Altmann, 1990; Norris, McQueen, & Cutler, 2000). As discussed in Chapter 3 , bottom-up processing essentially refers to the influence of the direct stimulus input or sensory components of the stimulus. Bottom-up processing is therefore often referred to as data-driven or data-limited processing. Conversely, top-down processing refers to the influence of existing memories and knowledge structures (such as the use of context) and is therefore often referred to as conceptually driven or resource-limited processing (Norman & Bobrow, 1975). We turn now to a discussion of the information-processing model that was dominant throughout most of the latter part of the last century.
### **Atkinson and Shiffrin’s Modal Model **

Atkinson and Shiffrin’s (1968) modal model of information processing has generated a vast amount of research since its publication. According to this model, information processing occurs in a series of stages consisting of sensory memory, STM, and LTM (Atkinson & Shiffrin, 1968). Sensory memory is thought to contain separate storage systems for each sensory channel. Acoustic information is temporarily held in an echoic sensory store, while visual information is held in an iconic sensory store. Information in sensory memory is thought to be coded as a veridical or as exact replication of the form in which it was received. As discussed in Chapter 1 , considerably more attention has been focused on the characteristics of visual sensory memory, or iconic sensory memory. However, auditory sensory memory, or echoic sensory memory, is more germane to our current purpose. Echoic memory is thought to hold an exact replica (in the form of an auditory trace or echo) of information presented for a brief period of time. The capacity and duration of echoic memory were examined through several key experiments in the late 1960s (see a review of much of this early work in the work of Hawkins & Presson, 1986).
### **Echoic Memory Capacity **

Moray, Bates, and Barnett (1965) utilized Sperling’s (1960) partial report paradigm to investigate the capacity of echoic memory. Sperling developed his partial report paradigm to examine the capacity and duration of visual sensory traces. He realized that people were processing more information than they could recall, and that some of this information was lost during the short time it took them to report their memories. Therefore, his ingenious solution was a partial report paradigm in which only a small subset of the information had to be recalled. The critical aspect was that the subset that was to be recalled was not made known to the viewer until *after *the stimulus array had disappeared. This allowed researchers to get a better estimate of both the capacity and the persistence of this echoic trace.

Using an auditory analogue to Sperling’s partial report paradigm, Moray et al. (1965) found that when participants recalled information from only one of four locations, eight items could be recalled fairly consistently. This corresponded to approximately 50% of any given list when four letters were presented from four different locations. Similar to the analogous process in visual sensory memory, Moray’s findings suggested that two mechanisms were at play in the auditory recall paradigm. One mechanism involved the amount of information that could be perceived in a brief auditory glance, and the second involved the number of items that could be recalled immediately after presentation. Moray referred to this brief auditory storage as the “immediate memory span.” It is now more commonly referred to as echoic memory. Moray concluded that the recall limitations were most likely due to loss at the time of recall rather than limitations in the amount of information encoded. That is, its capacity is thought to be greater than STM or working memory. Moray’s findings lend support to the notion that we are able to encode, at least briefly, more auditory information than we are capable of attending to and storing for further processing. The temporal limits of this brief storage system were investigated subsequently and are discussed in the section on echoic persistence.
### **Echoic Persistence **

Current opinion on the topic of auditory sensory memory tends to suggest that two forms of precategorical acoustic storage exist (Cowan, 1984; Giard et al., 1995; Winkler, Paavilainen, & Naatanen, 1992). The first form lasts only a few hundred milliseconds, while the longer form lasts several seconds and is generally most synonymous with what is meant when using the term *echoic memory *.
#### **Short-Term Auditory Store **

The first form of auditory sensory memory is a short auditory store capable of retaining acoustic information for 200 to 300 ms (Giard et al., 1995). It is thought to occur in the primary auditory cortex. It begins within 100 ms following the presentation of an acoustic stimulus and decays exponentially over time (Lu, Williamson, & Kaufman, 1992). Investigations of the persistence of this short-term auditory store have been explored by varying the interval between successive acoustic stimuli through masking paradigms (see Massaro, 1972, for a review) and more recently through neurophysiological indices of auditory ERPs or mismatch negativity (MMN) paradigms (see Atienza, Cantero, & Gomez, 2000).

Naatanen and colleagues have demonstrated that several cortical areas may be responsible for the short-term storage of different aspects of the acoustic signal as well as conjunctions between aspects (Giard et al., 1995; Takegata et al., 2001). They have used the MMN paradigm with MEG to examine the location of neural traces for physical characteristics such as frequency, intensity, and duration. They observed that the MMN activity patterns observed on the scalp for each of the different auditory parameters differed. Further evidence that short-term auditory memory for frequency, intensity, and duration are at least partially a result of different underlying neural structures was seen in the results of their dipole model analysis. Next, the duration of the longer form of auditory sensory storage is discussed.
#### **Long-Term Auditory Store **

The longer auditory store is generally the one referred to when the term *echoic memory *is used. This convention is maintained here; thus, the term *echoic memory *is reserved from this point in reference to the long-term auditory store. It is found in the association cortex and is thought to hold information for several seconds (Lu et al., 1992). The duration of echoic memory was first examined using an auditory version of Sperling’s (1960) partial report paradigm discussed in the previous section.

Darwin et al. (1972) expanded on the investigations of Moray and colleagues (1965) by examining the effect of a 1-, 2-, and 4-s delay between presentation and poststimulus cueing. Significant differences were found in the amount of information participants could recall after each poststimulus delay condition. Darwin et al. concluded that the time limit for the auditory sensory store was greater than 2 s but less than 4 s. This temporal limit has important implications for communication. Because a veridical representation of the auditory information is available for 2–4 s, it is possible that if one is engaged in a concurrent task when auditory information is presented, for a brief period of time the information will still be accessible for attentive processing. That is, it will be stored for 2–4 s, which may allow sufficient time to change focus and access the information content postpresentation. An everyday example of this is when a person is reading a book or watching TV and someone walks up and begins a conversation. The distracted person may automatically ask, “What did you say?” but then begin replying before the speaker has had a chance to repeat. Although the speaker may be a bit surprised and wonder why he or she was asked to repeat the message if the listener actually heard it, it makes perfect sense when we consider that the information remains in the listener’s echoic memory for a brief period of time, thus allowing the listener to make an appropriate response to the speaker’s question or comment.

The potential for presentation intensity to affect either the strength (veridicality) or the duration of the echoic memory trace was not examined in these early investigations. However, recent evidence indicates that presentation intensity may affect both of these aspects (Baldwin, 2007).
### **Emerging Developments in Echoic Memory Research **

Although a considerable amount of early work on sensory memory in the visual realm examined the visual parameters that had an impact on its veridicality and duration, this research seems not to have extended to the auditory modality. For example, researchers examined the impact of stimulus intensity and contrast on the persistence of visual sensory traces. They often found equivocal results, with increased intensity sometimes increasing the duration of the iconic traces and sometimes decreasing it. Many of these equivocal results can now be rectified by considering the frequency of the visual stimulus, particularly whether it relies on photopic (cone receptor) or scotopic (rod receptor) vision (see review in the work of Di Lollo & Bischof, 1995).

Baldwin (2007) examined the impact of auditory intensity (i.e., subjectively perceived as loudness) on echoic persistence. She sought to determine if the echoic traces of sounds that were presented at higher intensities lasted longer. Specifically, Baldwin found that at the upper temporal limit of echoic memory (4 s), matching performance for auditory tonal patterns was directly affected by presentation amplitude. That is, after 4-s delays, louder presentation amplitudes resulted in greater accuracy in determining whether a second tonal pattern matched the one presented previously.
#### **Implications of Persistence **

The impact of intensity level on echoic persistence has important implications for numerous auditory processing tasks, the processing of speech in particular. As discussed in Chapter 8 , most contemporary models of speech processing assume that speech is processed in a series of stages. The initial stage begins with translation of acoustic signals into a pattern of abstract representations, followed by phonemic identification and then word or lexical processing utilizing higher-level representations constructed from contextual cues and knowledge of prior subject matter (Cutler, 1995; Fischler, 1998; Massaro, 1982; Norris, McQueen, & Cutler, 1995; Stine, Soederberg, & Morrow, 1996). Auditory memory is essential to this progression. Corso (1981) noted that initial stages of speech perception rely on the ability to discriminate between small changes in frequency or pitch. Later stages rely on the ability to integrate successively heard words, phrases, and sentences with previously stored information (Pichora-Fuller, Scheider, & Daneman, 1995).

Presentation conditions that facilitate echoic persistence have the potential to both facilitate auditory processing and decrease the mental resource requirements for the lexical extraction process. Imagine, for example, dual-task situations that require a person to perform an auditory and a visual task simultaneously. If the person is engaged in the visual task when the auditory task is presented, a long-duration echoic trace would assist the person in retaining the auditory information until he or she could shift attention toward the auditory stimulus. Most of us have had an experience for which this was useful. Imagine that you are reading a book when a buzzer sounds unexpectedly. Being able to retain the sound long enough to finish reading a sentence and then shift attention toward the processing of the sound aids in identifying the sound and taking the appropriate action. The particular role that echoic memory plays in speech processing is discussed further in Chapter 8 . We turn our attention now to the next stage in processing information, working memory.
## **WORKING MEMORY **

*Working memory *is a term given to describe a limited-capacity system that is used to hold information temporarily while we perform cognitive operations on it (Baddeley, 2002). This section provides a discussion of the role of working memory in auditory processing as well as its close link to mental workload. The construct now referred to as working memory stemmed from earlier depictions of an intermediate stage of information processing termed *short-term memory *(Atkinson & Shiffrin, 1968). STM was thought to play an integral role in storing new information transferred from the sensory register (i.e., echoic memory) for possible storage in LTM. Baddeley and colleagues (Baddeley, 1992; Baddeley & Hitch, 1974, 1994) introduced the term *working memory *as an alternative to the construct of STM. Unlike STM, working memory is thought to be a multidimensional transient storage area where information can be held while we perform cognitive operations on it. Working memory places less emphasis on storing information until it can be transferred into LTM, instead placing an emphasis on holding information while we engage in such operations as problem solving, decision making, and comprehension (Baddeley, 1997). Working memory can also be linked closely with the limited-capacity processing resources embodied in what we refer to as attentional resources or mental workload. Therefore, we examine the multiple dimensions of working memory in some detail in this chapter. However, a rich history of research utilizing the framework of investigations of STM provides important information regarding the nature of auditory processing. Therefore, this begins our discussion.
### **short-term memory **

Considerable research has been conducted on the role of STM in both auditory and visual information processing. Early models emphasizing the importance of both a temporary storage and processing system and a more permanent longer-term storage system were developed (Atkinson & Shiffrin, 1968, 1971; Phillips, Shiffrin, & Atkinson, 1967; Waugh & Norman, 1965). These early models typically emphasized that a primary role of STM was to control an executive system, which functioned to oversee the coordination and monitoring of a number of subprocesses. Examinations of the capacity, duration, and code of STM led to the establishment of several well- documented characteristics of information processing in STM.

Recall paradigms of visually and auditorily presented letters, words, and sentences were frequently used to investigate the characteristics of STM (Baddeley, 1968; Conrad, Baddeley, & Hull, 1966; Daneman & Carpenter, 1980; Engle, 1974; Yik, 1978) and in fact are still frequently used in cognitive research (Baddeley, Chincotta, Stafford, & Turk, 2002; Craik, Naveh-Benjamin, Ishaik, & Anderson, 2000; Risser, McNamara, Baldwin, Scerbo, & Barshi, 2002). Two well-established findings from this body of literature are the modality effect and the suffix effect.
#### **Modality Effect **

When recall for lists of words presented in visual and auditory formats are compared, recall is consistently higher for items presented in the auditory format (Conrad & Hull, 1968; Murray, 1966). The recall advantage for auditorily versus visually presented material has been termed the modality effect. The modality effect is most salient for items at the end of a presented list. That is, in serial recall paradigms recency effects (better recall for items at the end of the list rather than items in the middle of the list) are strongest for material that is heard versus read. The modality effect provides evidence that short-term retention of verbal material benefits from an acoustic or phonological code, a point we will return to in further discussions.
#### **Suffix Effect **

What has been called the suffix effect provides another consistent and characteristic finding in investigations of STM. In recall paradigms in which to-be-remembered information is presented in an auditory format, retention of items at the end of a list (recency effects) is disrupted if the end of the list is signified by a nonlist word or suffix. For example, if a spoken list of items to be remembered is followed by the word *recall *, recency effects are diminished (Crowder, 1978; Nicholls & Jones, 2002; Roediger & Crowder, 1976). The role of an irrelevant suffix in disrupting recency effects provides further evidence that temporary retention of information benefits from an acoustic or phonetic code. Even more dramatic evidence of the acoustic code is found in observations of the recall advantages provided by articulatory rehearsal and acoustic confusions.
#### **Articulatory Rehearsal **

The recall advantages of articulatory rehearsal are evidenced by the observation that recall is higher if participants are allowed to engage in auditory rehearsal, such as silent vocalization, whispering, or speaking out loud the to-be-remembered stimuli (Murray, 1965). This process, known as *articulatory rehearsal *, facilitates memory in serial recall tasks (Larsen & Baddeley, 2003).
#### **Acoustic Confusions **

Acoustic confusions are instances when an acoustically similar item is substituted for a presented item during recall. Letters, due to their greater similarity, are more prone to acoustic confusion than are digits. For example, the consonants B, V, D, and T are acoustically similar and therefore prone to substitution. Interestingly, Conrad (1964) observed that people make acoustic confusion errors even when lists of items are presented visually. That is, people are more prone to incorrectly recall an acoustically similar substitute (e.g., V for B) than they are a visually similar substitute (e.g., L for V) even when the lists are presented visually. Acoustic similarity between items in a serial recall list dramatically affects recall in general. Lists that are more similar result in poorer recall performance (Conrad, 1964; Conrad & Hull, 1964).

It is beyond the scope of this chapter to cover the extensive body of research leading up to current models of working memory. However, two key models that have developed and are still currently extensively applied to the understanding of information processing in general, and auditory processing in particular, are discussed. The first of these models was first presented by Baddeley and Hitch (1974) and later refined by Baddeley (Baddeley, 1992; Baddeley & Hitch, 1994). The second model, which has been applied extensively in human factors research, was developed by Wickens (1984) and is termed *multiple-resource theory *. First, we turn to a discussion of Baddeley’s concept of working memory.
### **Working Memory Components **

In 1974, Baddeley and Hitch published a seminal article discussing a series of 10 investigations systematically designed to determine if verbal reasoning, comprehension, and learning shared a common working memory system. Their results strongly suggested that the three activities utilized a common system that they referred to as working memory. The working memory system was postulated to be a limited-capacity work space that could coordinate the demands of storage and control. Baddeley (1992) has presented compelling evidence for a three-component model of working memory.

**FIGURE 4.1 **Baddeley’s model of working memory.

Baddeley’s (Baddeley, 1992; Baddeley & Hitch, 1974, 1994) three-component working memory system consists of a central executive, attentional controlling system and two slave systems (see Figure 4.1 ). The slave systems consist of a visuospatial sketch pad for processing and manipulating visual images and a phonological or articulatory loop for manipulation of speech-based information.

Baddeley (1998) further developed the model and later subdivided the phonological loop into two components. Considerable research has focused on revealing the characteristics and neural mechanisms of the phonological loop. This research is particularly germane to the current investigations.
#### **Phonological Loop **

Baddeley (1998) described the phonological loop as consisting of two components. The first is a phonological store capable of holding information for up to 2 s. The second component is an articulatory control process responsible for subvocal rehearsal, which feeds into the phonological store. The articulatory rehearsal process is thought to be capable of converting written material into a phonological code and then registering it in the phonological store.
#### **Phonological Store Capacity **

The phonological loop is of limited capacity, which has primarily been demonstrated by examining the so-called word length effect (Baddeley & Logie, 1999). The spoken length of words, rather than the number of phonemes (or syllables), appears to be a primary determinant in the number of to-be-recalled words that can be kept in the phonological store.

The phonological store capacity in Baddeley’s model is postulated to vary considerably from individual to individual, with an average duration of 2 s (Baddeley, Gathercole, & Papagno, 1998). Baddeley et al. (1998) presented compelling evidence that the phonological store plays an integral role in language learning, particularly in children and for adults when learning new vocabulary or additional languages. Individuals with larger phonological stores acquire more extensive vocabularies as children and learn additional languages more easily as adults.

They cite evidence that the phonological store, however, plays a less-crucial role in verbal memory (i.e., word recall tasks) of familiar words. Evidence for this lesser role in familiar word recall stems in part from neurological patients with specific phonological memory deficits that demonstrate normal language comprehension and production capabilities but significantly lower recall in nonword memory tasks.

Experimental paradigms designed to disrupt the phonological store include (a) the word length effect, (b) phonological similarity, and (c) articulatory suppression (Baddeley et al., 1998). The word length effect can be demonstrated by impaired recall for words that take longer to say even though the number of syllables is equivalent. Phonological similarity is demonstrated by impaired recall for multiple words that sound alike, and articulatory suppression occurs when participants are required to say irrelevant words or syllables that block articulatory rehearsal.

Each of these processes is thought to activate distinct brain regions. Specifically, processes involving the phonological store have been shown to result in greater activation of the perisylvian region of the left hemisphere (Baddeley et al., 1998; Paulesu, Frith, & Frackowiak, 1993), while articulatory rehearsal processes are associated with greater activation of Broca’s area and premotor areas (Paulesu et al., 1993; Smith & Jonides, 1999). The precise brain mechanisms associated with these processes are not particularly germane to the current discussion. Those interested in more in-depth coverage of this topic are referred to work by Banich (Banich, 2004; Banich & Mack, 2003); Zatorre (Zatorre et al., 1992; Zatorre & Peretz, 2003); and others (Hagoort, 2005; Logie, Venneri, Sala, Redpath, & Marshall, 2003; Petersson, Reis, Askeloef, Castro-Caldas, & Ingvar, 2000).

Speech is believed to gain automatic access to the phonological store, regardless of the mental workload demands imposed by a concurrent task (Baddeley, Lewis, & Vallar, 1984; Gathercole, 1994; Penny, 1989). Text, on the other hand, does not have this advantage. Verbal information presented visually (text) requires an extra step to convert it to a phonological form, typically using subvocal rehearsal. Therefore, according to Gathercole, a concurrent memory load can be expected to disrupt text processing to a greater degree than would be expected for speech processing.

Further, the phonological store is thought to hold information at a phonemic level rather than at a word level (Baddeley, 1997), as evidenced by the disrupting effects of nonsense syllables. Nonsense syllables that share the phonemic components of speech disrupt processing in the phonological store (Salame & Baddeley, 1987, 1990). However, speech is more disruptive than nonsense syllables, a phenomenon referred to as the irrelevant speech effect (Larsen & Baddeley, 2003; Salame & Baddeley, 1990).

In addition to these aspects of working memory, a number of other important concepts pertaining to the cognitive architecture involved in auditory processing are worthy of discussion. One such important concept involves distinguishing between serial and parallel processing.
## **SERIAL VERSUS PARALLEL PROCESSING **

Much of the early work using the dichotic listening paradigm examined the notion of when information can no longer be processed in parallel but rather must be processed serially. This idea was the basis of the proposed bottleneck or filter concept in attentional processing. It was difficult to discern with any degree of certainty just where the filter was likely to occur, and the focus of research changed from looking for a physical filter to thinking of attention as a resource (see discussion in Hunt & Ellis, 2004; Kahneman, 1973).

Despite the difficulty in determining if a particular filter mechanism exists and if so, where, considerable insight was gained into our ability to process information in parallel versus serially. For example, in Treisman’s feature integration theory (FIT; Treisman & Gelade, 1980), unique primitive features are thought to be processed without the need for conscious focused attention. Such sensory primitives are thought to be processed in parallel. Conversely, examining feature conjunctions (examining stimuli that may share one or more features with other stimuli in the environment) are thought to require focused attention and can only be processed serially. Although most of Treisman’s work on FIT has been conducted with visual stimuli, it can be reasoned that similar phenomena are present with auditory stimuli. Therefore, sounds with unique characteristics varying from all other stimuli by one salient feature may stand out above the background noise, regardless of how many distracting sounds are present.

Parallel processing is a key component of more recent theories pertaining to the cognitive architecture of information processing. Parallel distributed processing (PDP) models, which are also referred to as ANN models, suggest that multiple streams of information may be processed simultaneously.
## **ARTIFICIAL NEURAL NETWORKS **

It is remarkable that we understand language at all given the number of multiple simultaneous constraints. In language processing, for example, we must simultaneously consider the constraints of both syntax and semantics. As discussed further in a discussion of speech processing in Chapter 8 , we simultaneously utilize the constraints imposed by the rules of syntax and semantics to arrive at an appropriate interpretation of a sentence (Rumelhart, McClelland, & Group, 1986). ANN or PDP models have made progress toward explaining how we might accomplish this significant feat.

ANNs have played a significant role in our understanding of speech processing (cf. Coltheart, 2004). Early network models developed by Collins and Quillian (1969) began to unravel the mysteries of our semantic knowledge structure. More recently, neural network models have been used to examine and explain how we accomplish lexical decisions by simultaneously considering syntactical and semantic constraints in everyday language-processing tasks (McClelland, Rumelhart, & Hinton, 1986). For example, syntactic information allows us to correctly interpret a sentence such as

The cat that the boy kicked chased the mouse.

However, as McClelland and colleagues (1986) pointed out, we need to consider the simultaneous constraints of semantic information to understand sentences such as the following:

I saw the dolphin speeding past on a catamaran.

I came across a flock of geese horseback riding near the beach.

ANN models have been used to demonstrate how we are able to simultaneously utilize information from several different sources interactively. These models are based on the interaction of a number of simple processing elements (i.e., letter, phoneme, and word identification; syntactical rules; and semantic constraints). Examples of ANN models of language processing are examined in greater depth in the discussion of speech processing in Chapter 8 . In particular, the TRACE model, an interactive-activation model of word recognition developed by McClelland and Elman (1986), and the dynamic-net model proposed by Norris (1990) are discussed.
## **SUMMARY **

Humans have the capability of selecting and attending to a specific auditory pattern in the midst of competing patterns. The chosen pattern can be located and held in memory long enough to interpret it along several simultaneous dimensions. From mere acoustic patterns, humans are able not only to recognize familiar acoustic patterns (i.e., Beethoven’s *Fifth Symphony) *but also to gain considerable insight into the nature of the sound source (i.e., whether a particular musical piece is played by one type of instrument or another or whether it is comprised of a number of simultaneous instruments in harmony). Humans not only can understand the words of a speaker but also can gain considerable insight into the age, gender, and emotional state and intent of the speaker.
## **CONCLUDING REMARKS **

Bottom-up and top-down processing, auditory selective attention, auditory stream segregation, and temporary storage of acoustic patterns while accessing the mental lexicon are some of the many intelligent processes used to interpret auditory patterns. Each of these processes requires attentional resources; hence, each contributes in different ways to overall mental workload. We address the mental workload required by these intelligent processes in the chapters to come. First, in the next chapter we discuss many of the key issues as well as methods and techniques for assessing mental workload.

- * See discussions on maintenance versus elaborative rehearsal by Anderson and colleagues (Anderson, 2000; Anderson & Bower, 1972).