# Unknown Chapter

*Source: Auditory Cognition and Human Performance by C. L. Baldwin (2012)*

---

# 8 **Mental Workload and Speech Processing **

## **INTRODUCTION **

Each of us engages in communication with others, speaking and listening with seemingly little effort, virtually every day of our lives. Our ability to talk to those around us not only is essential to our survival, but also enriches our lives through social interaction, allows us to pass on knowledge, provides an avenue for entertainment, and permits outlets for creativity and imagination. The ability to communicate ideas, thoughts, and feelings to others through language sets us apart from all other species on the planet. Normally, we are able to process speech—through face-to-face conversations; from the radio, telephones, TV, and other media—all with relative ease and efficiency. But, this apparent effortlessness hides a remarkably complex process that is the focus of this chapter.

Extracting meaningful semantic information from a series of transient acoustical variations is a highly complex and resource-demanding process. Well over a century of research has focused on understanding the mechanisms of speech processing (Bagley, 1900–1901), with considerable accomplishments occurring particularly in the last few decades with the advent of brain-imaging techniques. Today, understanding of our remarkable ability to process speech has been informed by the integration of theories and empirical research from such disciplines as linguistics, psycholinguistics, psychology, cognitive science, computational science, and neuroscience (C. Cherry, 1953a; Friederici, 1999; Norris, 1994; Norris et al., 1995; Plomp & Mimpen, 1979; Treisman, 1964b). Yet, after over a century of research, many of the details of this process remain elusive. As Nygaard and Pisoni (1995) put it:

If one could identify stretches of the acoustic waveform that correspond to units of perception, then the path from sound to meaning would be clear. However, this correspondence or mapping has proven extremely difficult to find, even after … years of research on the problem. (p. 63)

This chapter examines current views on the lexical interpretation of acoustic information. The interaction of sensory, acoustical, and cognitive factors is discussed with a focus on their impact on the mental workload of speech processing.
## **CHAPTER OVERVIEW **

To establish a framework for understanding the interaction of sensory-acoustical and perceptual-cognitive factors and their impact on mental workload, we begin by discussing the complexity of speech recognition. Spoken word perception presents unique challenges. Identifying words requires the ability to match a highly variable acoustic pattern, unfolding across time, to representations from our stored repertoire of lexical units. Then, these words (which may be ambiguous—temporarily having several potential meanings) must be held for at least the duration of a sentence until we can comprehend their intended semantic message. How we accomplish this matching of acoustical information to semantic content—referred to as *binding *—has been the subject of numerous investigations in many disciplines, and it forms the cornerstone of most models of speech perception. The challenges of speech processing and the levels of speech processing involved in this binding process are discussed.

Next, an important theoretical debate regarding how speech is processed is presented. We discuss whether speech processing is modular or whether it uses the same general mechanisms involved in nonspeech tasks. These theories have important implications for mental workload and the degree of interference that can be expected when speech processing must be carried out in conjunction with other concurrent tasks.

Major theoretical models of speech perception are then highlighted, with a focus on their implications for how sensory and cognitive factors have an impact on the mental workload of the comprehension task. Finally, a number of issues that have an impact on the mental demand of speech processing are discussed. These issues range from acoustic aspects of the listening environment and speech rate to the types of speech utilized, including real versus synthetic speech. First, we look at the complexity of speech processing—and why it is often more difficult than reading.
## **SPEECH-PROCESSING CHALLENGES **

Unlike the printed word, the speech signal carries a wealth of information beyond straight lexical content. When listening, we are not told explicitly who is speaking and whether an utterance is made angrily or whimsically. We must infer these supralinguistic aspects from the acoustic stimulus itself. Multiple redundancies are processed in parallel to confer meaning on the transient acoustic waveform. Speaker characteristics, emotional tone, situational context, and prosodic cues are a few of the informational features conveyed by spoken language. Each extralinguistical feature has the potential to affect the resource demands of speech processing. These features are integrated in parallel with syntactical knowledge that can be richly complex—as we see from the challenge placed on working memory resources when we process object-relative clauses in a sentence, such as, “The ball that was thrown by the boy rolled into the street.”

Consider the many different ways that one can say the word *yes *. It can be said in a matter-of-fact way, indicating simply that one is still listening to the current conversation. Or, it can be used to signify agreement with a wide range of enthusiastic overtures, ranging from the emphatic, “Yes, this is certainly the case, and I agree wholeheartedly,” to a mere, “Yes, I see your point.” Or, it can even signify agreeable opposition, as in the case of, “Yes, but …” So, beyond the obvious speaker- to-speaker variations in pronunciation of specific words, a tremendous amount of information is conveyed by subtle changes in the acoustics of any given word.

The speaker, not the listener, drives the rate of speech processing. With text, the reader has the ability to change the pace as processing difficulty increases—not so with listening. Speakers articulate words and phrases at their own pace, and this pace varies tremendously from person to person (Miller, Grosjean, & Lomanto, 1984). As discussed in more depth in this chapter, the variability of speech rate poses additional challenges to figuring out how to segment the acoustic stream effectively.

Another factor adding to the complexity of speech processing is that, unlike text, listening to speech is often an interactive rather than a passive process. Many everyday situations require us not only to listen to a speaker’s meaning but also simultaneously to formulate thoughts and prepare for speech production to facilitate meaningful communication. The task of timing our conversational utterances to begin at appropriate points—called turn-taking—is both resource demanding and critical to successful communication (Sacks, Schegloff, & Jefferson, 1974). Actively engaging in a conversation requires more mental effort than simply passively listening to the radio (Strayer & Johnston, 2001).

Despite the extensive effort that has gone into the development of models of speech perception, considerably less emphasis has been placed on understanding the mental effort involved in speech processing. As Fischler (1998) pointed out, attention and language are “two of the most widely studied aspects of human cognitive skills” (p. 382). Despite this, much less concern has been given to the mental effort required to process speech (Fischler, 1998). The dominant assumption appears to have been that understanding the meaning of speech may be effortful, but that the perceptual process of extracting the speech signal occurs rather automatically. And in fact, for most listeners, in many situations, speech processing appears to require little effort. However, when the listening situation is less than optimal, overall task demands are high, or the message is particularly complex, the effort required to process speech becomes apparent, and the nature of this effort has important implications for understanding human performance in a wide variety of settings.

Aside from the inherent difficulty of processing syntactically complex semantic strings, sensory and additional cognitive factors have been shown to affect the mental workload of speech processing. Degraded listening conditions due to faint signals, noise, reverberation, or competing stimuli (verbal or nonverbal) represent some of the acoustical factors that can increase the mental effort of speech processing. Challenging acoustical situations include listening to a phone conversation when the connection is poor and the speaker is barely audible or trying to follow the conversation of a friend at a noisy, lively party where there are many multiple competing sources of sound and vision. College students asked to assess the meaning of syntactically complex sentences in the presence of noise or a concurrent digit load showed significant performance difficulties (Dick et al., 2001)—even to the extent of resembling the deficits that brain-damaged aphasic patients typically exhibit. Similarly, 18- to 25-year-olds displayed sentence-processing deficits characteristic of much older adults when the sentences they had to process were presented at low presentation levels in conjunction with other demands for their attention—like driving (Baldwin & Struckman-Johnson, 2002).

Cognitive factors may also increase the mental workload of speech processing. For example, speech processing requires greater mental effort in the absence of contextual cues or when the material is difficult to comprehend due to low cohesion or coherence, complex syntactical structure, or the presence of unfamiliar words (Chambers & Smyth, 1998; Sanders & Gernsbacher, 2004). Several studies have demonstrated that text cohesion—the degree to which a reader must fill in contextual gaps—influences the difficulty of text comprehension (McNamara & Shapiro, 2005). Similar effects are present in discourse or speech comprehension. Ambiguous pronouns, such as in the sentence, “Ivan read the poem to Bob, and then Jill asked him to leave,” result in low coherence and leave listeners trying to figure out which “him” has to leave.

Competition for mental resources may also arise from simultaneous visual and psychomotor demands. Even under relatively ideal listening conditions, processing speech imposes attentional processing requirements that can impair performance on other tasks. So, while highly overlearned and relatively automatic tasks, such as maintaining lane position while driving, may not be affected by concurrent auditory processing, more resource-demanding tasks such as speed maintenance and decision making are adversely affected (Brown & Poulton, 1961; see review in Ho & Spence, 2005). Recarte and Nunes (2002) found that, similar to other mentally demanding tasks, simply listening to words for later recall impaired drivers’ ability to maintain speed on a highway.

Adding to the complexity of the speech-processing task is the fact that spoken words are not neatly separated into individual components like their printed form. Rather, the acoustic pattern of spoken words is a complex arrangement of sound energy requiring extensive prior knowledge of the language in order to impose phonemic context and ultimately to construct semantic structure and meaning from the continually varying physical stimulus. This challenge is referred to as the *segmentation problem *.
### **Segmentation **

A key issue to resolve in speech perception is how we take the physical stimulus, which is a time-varying continuous signal, and construct a perceptual interpretation of discrete linguistic units (Nygaard & Pisoni, 1995). Unlike the printed word, there are no gaps between spoken words. In fact, although linguistic units such as syllables, phonemes, and words appear to be perceived as discrete units, their corresponding acoustical representations are often continuous and overlapping. Listeners must determine where one word ends and another begins—the segmentation problem (Davis, Marslen- Wilson, & Gaskell, 2002). Our ability to translate the rather continuous physical stream into discrete linguistic units is not completely understood, but segmentation does point to the interaction of sensory-acoustical and perceptual-cognitive factors, that is, distinct acoustical changes and inferences based on expectations stemming from contextual knowledge. Listeners are unable to segment speech in unfamiliar languages. Even trained phoneticians correctly recognize only about 50% of the phonemes spoken in an unfamiliar language (Shockey & Reddy, 1975, as cited by Plomp, 2002).
#### **Cues to Segmentation **

Several theoretical explanations have been proposed for how segmentation occurs (Cutler, 1995; Davis et al., 2002; Norris et al., 1995). One early proposal was that segmentation is accomplished through sequential recognition (Cole & Jakimik, 1980; Marslen-Wilson & Welsh, 1978). According to this account, we are able to perceive the start of a new word once we have identified the word preceding it.

However, a strict interpretation of this theoretical perspective would indicate that we must be able to determine the offset of a word before we could tell where the next word begins. Current evidence does not favor this approach. For example, using a gating paradigm, Grosjean (1985) found that roughly 50% of monosyllabic words are not recognized until some time after their offset. That is, the first word is not identified until partway through the next consecutive word.

A second primary theoretical explanation for word segmentation abilities involves lexical competition (Cutler, 1995; McClelland & Elman, 1986). According to this perspective, multiple lexical candidates across word boundaries compete, and the “best fit” is selected to segment the acoustical stream. This perspective fits within the framework of the theoretical tenets of models of speech perception, such as TRACE (McClelland & Elman, 1986) and shortlist (Norris, 1994), discussed later in this chapter.

A third possibility is that word segmentation occurs explicitly in a separate process (Cutler, 1995; Norris et al., 1995). According to this perspective, lexical selection is guided by probability information regarding where a word boundary is likely to be (Norris et al., 1995). Acoustic cues, such as the onset of strong syllables, are used as cues to word segmentation. As pointed out by Norris et al. (1995), most content words (90%) do in fact begin with a strong syllable, and roughly 75% of the time strong syllables are the initial syllables of content words.*

In addition, speakers pronounce phonemes differently depending on the phonemic context. These allophonic cues (phonemic variations) provide listeners with cues to segmentation (Kent & Read, 1992). For example, a phoneme at the beginning of a word will tend to have a longer duration than the same phoneme in later segments of a word (Peterson & Lehiste, 1960). Listeners make use of these cues to determine word boundaries during segmentation (Davis et al., 2002).

These three accounts—the sequential, lexical competition, and separate mechanism theories—may not be mutually exclusive (McQueen, Norris, & Cutler, 1994; Norris et al., 1995). First, most models of speech perception, including the lexical competition models such as TRACE and shortlist, are primarily sequential. These models suggest that once a word has been recognized, alternative lexical candidates are inhibited. Therefore, once an initial word is recognized, subsequent words are easier to identify since they will be segmented from the preceding word, thus reducing the number of lexical alternatives (Norris et al., 1995). Another challenge to spoken word perception is its tremendous variability.
### **Variability **

The speech signal varies tremendously as a function of both phonetic context and speaker. Different speakers produce substantially different acoustic versions of the same utterance, and any one speaker will produce the same utterance differently depending on the context in which it is spoken. This was illustrated in the example of the many ways a person might say the word *yes *. In the words of Plomp (2002), “No two utterances of the same speech fragment are acoustically equal” (p. 94). Our remarkable ability to comprehend speech relies on our ability to map a highly variable signal to an invariant lexical code.

The difficulty inherent in recognizing such a variant signal is illustrated by an investigation conducted by Strange, Verbrugge, Shankweiler, and Edman (1976). They compared the formant frequencies for vowels produced by untrained speakers of different age and gender (5 men, 5 women, and 5 children). Listeners were asked to identify either vowels produced by a single talker or a random mix of vowels spoken by several talkers either in isolation or embedded in a constant consonantvowel-consonant (CVC) word. Their results indicated that embedding the vowel in the word context greatly improved vowel identification (17% and 9.5% misclassifications in the mixed- and single-talker conditions, respectively, as compared to 42.6% and 31.2% misclassifications in the isolated vowel condition, respectively).

An even more important observation in the study by Strange et al. (1976) was the tremendous variability in the acoustic signal of the vowels when produced by men, women, and children. The overlap in formant frequency for different vowels was so great among the different categories of speakers that configuration for a vowel for one speaker category corresponded with a completely different vowel from another speaker category. These observations illustrate the variability of the speech signal and emphasize the importance of the interaction between sensory and cognitive factors in speech recognition. Although listeners were able to correctly identify vowel sounds better when vowels were produced by the same speaker category, overall recognition was still significantly less than optimal.

Given the degree to which the acoustic pattern of individual words varies from speaker to speaker and even within the same speaker at different times and in different contexts, our ability to comprehend speech is nothing less than remarkable. However, recognizing variant speech signals in everyday contexts requires considerable processing resources. A more variant or degraded speech signal can be expected to require considerably more processing resources or mental workload. The difficulty of learning to understand a new acquaintance with a heavy accent or a young child illustrates how phonemic variance increases the demands of the speech-processing task.

Much of the variability issue we have been discussing has pertained to variation at the phonological level. At a minimum, theories of language find it useful to distinguish between phonological, lexical, and syntactic levels of processing (Jackendoff, 2007). Each of these levels is discussed in turn, beginning with the phonological level. Since phonemic variation is so prevalent, our discussion of processing at the phonological level includes additional examples of variation.
#### **Phonological Level **

Depending on such factors as context, emotional tone, and even the personality of the speaker, the acoustical properties of a phoneme or syllable will change quite dramatically (see Kuhl, 2004; Strange et al., 1976). Phonemes are the smallest unit of speech, which if changed would change the meaning of the word. They can be compared to morphemes, which are the smallest unit of speech that has meaning. For example, the /b/ and the /oy/ in boy are both phonemes, although neither has meaning by itself.

Together, they form one morpheme: boy. Adding the single letter /s/ to the word *boy *, making it boys, changes the meaning of the word. In this example, the /s/ in boys is both a phoneme and a special class of morpheme—called a bound or grammatical morpheme. Word endings such as /s/ and /ed/ have meaning but cannot stand alone like free morphemes (i.e., boy and cat). See the work of Carroll (2004) for further details.
#### **Coarticulation **

Segments of speech are not typically produced in isolation but rather in phonemic context, meaning in combination with other segments to form syllables, words, and phrases. When produced in combination, speech segments interact or are coproduced in a process called coarticulation (Kent & Read, 1992). Coarticulation specifically refers to situations in which the vocal tract adjusts in such a way that it enables the production of two or more sounds concurrently or in rapid succession. The production of one phoneme can be influenced by either preceding or subsequent phonemes in forward (anticipatory) or backward (retentive) coarticulation. The resulting effect is a change in the acoustical properties of any given phoneme as a function of its phonemic surroundings.

Different acoustic variations for the same phoneme due to coarticulation are called *allophones *. Allophones are cognitively classified as the same phoneme, despite the fact that they differ acoustically. One may be aspirated while another is not, or they may vary substantially in duration. For example, the phoneme /t/ results in quite a different allophone when used in the word *tap *relative to its use in *dot *(Jusczyk, 2003). In other words, depending on the surrounding sounds, the same phoneme is said in different ways. In fact, in certain contexts different phonemes can essentially sound more like each other than individual allophones do. For example, as McLennan, Luce, and Charles-Luce (2005) pointed out, in everyday English conversation, /t/ and /d/ in the middle of a word (such as in water or greedy) often sound similar. Despite this tremendous acoustic/phonemic variability in the speech signal, even infants are capable of classifying speech sounds (see Kuhl, 2004). Infants of only 6 months of age can distinguish vowels of the same category /a/ from /e/ across a wide variety of speakers (Doupe & Kuhl, 1999; Kuhl, 1993).

A number of theories have been presented to illustrate how listeners are able to interpret the many different acoustical versions of the same phoneme. One of the more influential early theoretical perspectives involved the search for invariant acoustic cues (see, e.g., Blumstein & Stevens, 1981; Cole & Scott, 1974). However, the identification of invariant acoustic cues, that is, acoustical cues that remain stable across particular phonemes regardless of context or speaker, has remained elusive (see review in Nygaard & Pisoni, 1995).

A more recent theoretical explanation for the ability to recognize phonemes under conditions of natural speech variability posits the existence of a phonemotopic map in the perisylvian regions in the temporal cortex (Shestakova, Brattico, Soloviev, Klucharev, & Huotilainen, 2004). Making use of magnetoencephalography (MEG), Shestakova and colleagues (2004) examined N1m (the magnetic analog to the N1 event-related potential [ERP] component) responses to three Russian vowels spoken by 450 different speakers. The use of numerous different speakers resulted in variability across phonemes, as would be expected in natural speech. Using a dipole modeling procedure, they identified the average N1m equivalent current dipole

**FIGURE 8.1 **Results of dipole location for vowel sounds. (Redrawn with permission from Shestakova, A., Brattico, E., Soloviev, A., Klucharev, V., & Huotilainen, M. (2004). Orderly cortical representation of vowel categories presented by multiple exemplars. *Cognitive Brain Research, 21 *(3), 342–350.)

(ECD; a source localization technique) peak latency, source strength, and location coordinates for each of the three vowel sounds across individual listeners. As illustrated in a reproduction of their results (see Figure 8.1 ), greater Euclidean distances were observed for the more spectrally distinct vowels [a] and [i] as well as a statistically significant absolute location difference in the *z *(inferior-superior) coordinate between these same vowels. The vowel [u], which more closely resembled [i] in spectral characteristics, did not differ with the other vowels in terms of either Euclidean distance or absolute location.

The results of Shestakova and colleagues (2004), along with similar previous findings (Obleser, Elbert, Lahiri, & Eulitz, 2003), provided evidence for the existence of a phonemotopic map in the left and right hemispheric perisylvian regions of the human brain that would aid phonemic recognition.

Phonological processing is also aided by grammatical information. Grammar consists of a set of rules for how phonological units and words can be organized and used to generate meaningful sentences (Carroll, 1994). In all languages, there are certain phonological units that are unacceptable combinations and therefore cannot be used to construct meaningful semantic units. For example, in English the phonological units *p *and *b *are never combined. Therefore, although we can use *p *to form the word *pin *and *b *to form the word *bin, pbin *is not a grammatically acceptable combination. These grammatical rules narrow somewhat the number of acceptable sounds in any given language—thus aiding processing at the phonological level. Phonological information must be matched to information in the mental lexicon— our repertoire of stored lexical units.
## **LEXICAL SELECTION **

All theories of speech perception must account for the process of lexical selection (Altmann, 1990; Jackendoff, 1999). At a minimum, this involves inclusion of a *mental lexicon *, a stored repertoire of lexical units and a set of rules by which the lexical units can be combined. Exactly what is contained in the mental lexicon is a point of debate. However, it is acknowledged that any theory of speech must explain how information from the acoustic level is bound with information at the lexical selection level to achieve semantic comprehension (Hagoort, 2003). This process is referred to as *binding *, thus identifying its similarity to combinatorial processes in other modalities. The example of binding in visual processing that is often given is the issue of how color and shape—known to be processed by separate neuronal pathways—are bound together to allow the perception of one coherent object. Speech processing relies on a similar process that is further complicated by the fact that the acoustical input does not occur simultaneously as in vision. Acoustical information to be bound to lexical information occurs over time; therefore, initial information must be held in some type of storage until lexical selection occurs (Davis et al., 2002; Marslen-Wilson, 1987; O’Rourke & Holcomb, 2002). As discussed in Chapter 2 , this temporary holding place can be called the phonological store (Baddeley, 1992). The phonological store, a subcomponent of the phonological loop having a duration of 1 to 2 s, is thought to be particularly essential to lexical selection when processing demands are high and to facilitate learning of novel or unfamiliar sound patterns (Baddeley, 1992; Baddeley et al., 1998). Ambiguous, novel, or unfamiliar acoustical information can be held in the phonological store until lexical selection or meaning is assigned. The mental resource requirements of the lexical selection process are affected by both sensory and cognitive factors. For example, when the incoming signal is degraded, the number of potential lexical alternatives will increase, and lexical selection will require greater mental resources (Baldwin, 2007).
### **Sensory-Level Effects **

The quality of the listening environment plays a critical role in the difficulty of the lexical selection process. Imagine, for example, that a listener is presented with the sentence, “The bats were lying around the dugout after the storm.” If the word *bats *is distinctly heard, then for up to 400 ms, both contextually appropriate and inappropriate meanings of the word would be activated, and the most appropriate meaning would be selected after the addition of contextual information. However, suppose the word *bat *was not distinctly heard, but perhaps only the phoneme /_at/ was distinctly heard. Then, words with all possible preceding consonants (i.e., cat, vat, mat, bat) would initially be generated, followed by the activation of all meanings associated with each of the possible words. The automatic activation of such an extensive list of words and subsequent meanings would likely make the lexical selection process more time consuming and mentally demanding. Acoustic factors also have an impact on the duration and strength of the phonological store.

Presentation intensity (how loud or salient a signal is above background noise) has a direct effect on the duration of the echoic trace in phonological storage. Increases in intensity are associated with increased echoic memory duration (Baldwin, 2007). An echoic trace of longer duration may increase the functional capacity of the phonological store, thus facilitating the lexical selection process during a challenging period of high processing demand. Further, the echoic trace is thought to decay rather than extinguish abruptly. Thus, the phonological material available in the initial period of a more persistent echoic trace would be expected to be of greater clarity relative to shorter duration traces. The greater clarity of the phonological material may decrease the mental resource requirements of lexical extraction. Models of spoken word recognition propose that multiple lexical candidates are activated by the initial word segments until alternatives can be ruled out through subsequent acoustical input (Davis et al., 2002). Accordingly, initial segments of greater ambiguity would result in activation of a greater number of lexical alternatives and therefore a more cognitively demanding lexical selection process. Presentation level, a sensory factor, may therefore interact with subsequent perceptual and cognitive stages of auditory processing, potentially compromising performance, particularly when task demands are high.
### **Cognitive-Level Effects **

Cognitive-level effects, including the frequency of the lexical candidate and its distinctiveness, will also influence the resource demand of the lexical selection process. Lexical items that are used more frequently (i.e., the word *back *vs. the word *bade) *are recognized more easily (Luce, Pisoni, & Goldinger, 1990). At the same time, lexical items that have fewer neighbors—similar-sounding words—are recognized more easily. As someone who has played the game Scrabble knows, the lexical neighborhood containing the letter *q *is much smaller than the one containing the letters *s *or *e *. Likewise, after hearing the initial phonemic components *xy *in the word *xylophone *, there are few lexical neighbors to be activated to complete the word. In contrast, hearing *cy *could activate a number of candidates, including cyclone, cyborg, silo, psychology, and so on. Thus, the lexical units themselves will influence the mental demands of the selection process, and the initial acoustic information must be held until the ambiguity can be resolved. Sometimes, this ambiguity still cannot be resolved at the word level, and the potential lexical candidates and their associated meanings must be held for an even longer period of time.

Till and colleagues provided strong evidence for the importance of echoic memory in the lexical-semantic processing stages (Till, Mross, & Kintsch, 1988). They had participants read sentences containing an ambiguous word such as *mint *and then perform a lexical decision task in which they heard words that were (a) semantically related and appropriate to the meaning (money); (b) semantically related but inappropriate to the meaning (candy); or (c) inferentially related (earthquake). The interval between the prime word (mint) and the target word varied from 200 to 1,500 ms. Within the first 400 ms, the meaning for both contextually appropriate and inappropriate words was facilitated. Beyond 400 ms, only meaning for contextually appropriate words was facilitated, suggesting that participants were processing words according to their context while suppressing contextually inappropriate meanings. The ability to discriminate between alternative meanings of ambiguous words based on context relies on the ability to store the auditory information until such decisions can be made.
### **Sensory-Cognitive Interactions **

The interaction between sensory and perceptual-cognitive processes has particular relevance for examining speech recognition in hearing-impaired and older listeners. Older listeners with elevated hearing thresholds are known to rely on context for speech processing more than their younger counterparts (Marshall, Duke, & Walley, 1996). Marshall and colleagues observed that, in low-context conditions, older listeners required more of the acoustic-phonetic information of a target word to identify words relative to younger listeners. Older participants took longer than their younger counterparts to identify the initial phonemes at the beginning of stimulus words. Once phonemes were recognized and phoneme cohorts were identified, older individuals tended to compensate or make up for lost time by performing the later stages of word recognition (isolating the unique word from among lexical alternatives and accepting this identification) faster than their younger counterparts. The additional cognitive effort that hearing-impaired and older listeners must apply in later stages of word recognition to compensate for initial phoneme identification difficulties could compromise performance when overall task demands are high. During periods of high task demand, a more persistent echoic trace would allow listeners greater flexibility in allocation of limited attentional resources. Additional issues related to the auditory processing abilities of older adults are discussed in Chapter 10 .
## **SYNTACTICAL LEVEL **

*Syntax *refers to the rules governing how words can be used in coherent discourse to convey meaning. In most languages, such as English, sentences must contain a subject (S) and verb (V) and frequently contain an object (O). In these SVO syntactical structures, we see that something or someone does something to something or someone. For example, consider the following simple sentences using the SVO structure:

The boy kicked the ball.

The cat chased the mouse.

Our knowledge of syntactical structure tells us that it was the ball that was kicked and that the boy did the kicking. There is no ambiguity about whether the ball did the kicking. This is true even for the second sentence—in theory, a mouse could chase a cat. However, our knowledge of syntax constrains the interpretation of these sentences.

Syntax involves much more than mere word order. We have no difficulty understanding who is doing what to whom, even if we rearrange the order of the words in the sentence as follows:

The ball was kicked by the boy.

The mouse was chased by the cat.

Syntax plays a more powerful rule as sentences become more complex. For example, consider the following sentences:

The boy who saw the cat kicked the ball.

The cat who was hit by the ball chased the mouse.

Syntax allows us to interpret these sentences correctly. The more complicated the syntax is, the more difficult the utterance will be to comprehend. Consider the following examples from Christianson, Hollingworth, Halliwell, and Ferreira (2001) of what are called “garden path sentences” since they lead the reader or listener down a garden path to a sometimes surprising end:

While the man hunted the deer ran into the woods. (p. 369)

While Anna dressed the baby spit up on the bed. (p. 369)

Several factors determine the difficulty of processing these garden path sentences and affect the likelihood that the reader or listener will misunderstand the sentence or require additional time to reinterpret the sentence. People with smaller working memory capacities take longer to read garden path sentences relative to those with larger capacities (King & Just, 1991; Miyake, Just, & Carpenter, 1994). When encountering ambiguities, people with larger working memory capacities appear to be able to hold multiple meanings in mind simultaneously and therefore require less time to resolve the ambiguities when contextual cues are provided (MacDonald, Just, & Carpenter, 1992; Miyake et al., 1994). However, people in general have more difficulty processing ambiguous or syntactically complex sentences.

Evidence that these complex sentences are more difficult is found in the observation that people take more time to process ambiguous words. Consider the example provided by MacDonald et al. (1992) of the temporarily ambiguous word *warned *in the sentences that follow. When it is initially encountered, it could be either the main verb of the sentence (as in the first sentence), or it may indicate that a relative clause is about to follow (as in the second sentence).

“The experienced soldiers warned about the dangers before the midnight raid.”

“The experienced soldiers warned about the dangers conducted the midnight raid.” (p. 61)

Chambers and Smyth (1998) observed that several syntactical and structural properties of discourse affect coherence and assist the listener in interpreting ambiguous words, pronouns in particular. Coherence is increased when an utterance contains a pronoun in the same structural position as the person to whom it refers. For example, Sentence 1 is more coherent than Sentence 2:

- 1. Mary raced toward Jill and then she unexpectedly came to a stop. (she = Mary)
- 2. Mary raced toward Jill and then unexpectedly she came to a stop. (she = ?)

The mental resource demands of speech perception will be affected by both sensory and cognitive factors at the phonological, lexical-semantic, and syntactic levels of processing. Effective models of speech perception must account for these influences at all three levels.
## **APPROACHES TO SPEECH-PROCESSING THEORY **

Discussion of several key models of speech processing illustrate how each attempts to deal with the various factors that have an impact on the binding of phonological, lexical, and syntax information. Emphasis is placed on the implications of the theoretical models for mental workload demands. Numerous models have been developed. In general, they share the common assumption that speech processing involves multiple levels of processing (Jackendoff, 1999; Marslen-Wilson, 1987; Massaro, 1982; McClelland & Elman, 1986). The models differ in several key ways. One key point of dissociation is whether they tend to view “speech as special,” incorporating a modular view in the tradition of Fodor (1983) and Chomsky (1984) or whether they view speech processing as accomplished by the general cognitive architecture in the tradition of connectionist modelers (McClelland & Elman, 1986; Rumelhart et al., 1986). We see this theme of modularity versus singularity arise in the proposed content of the mental lexicon, particularly whether the rules for lexical combination (syntax) are contained in the same or different architectural structures—dual- versus single-mechanism theories (Joanisse & Seidenberg, 1999, 2005; Ullman, 2001a; Ullman et al., 1997). We also see this controversy over modularity arise in critical debates on the nature of verbal working memory (vWM: Caplan & Waters, 1999; Just & Carpenter, 1992; Just, Carpenter, & Keller, 1996; MacDonald & Christiansen, 2002; Waters & Caplan, 1996).

In addition, key models of spoken word perception can be differentiated by the degree of interactivity presumed to occur between the various levels of processing (Altmann, 1990; Cutler, 1995; Jackendoff, 2007; Marslen-Wilson & Welsh, 1978). All theories must account for the fact that speech comprehension requires listeners to quickly recode a transient acoustic waveform into a more lasting representation by matching it to specific items stored among the many tens of thousands of items in the mental lexicon for further speech processing to occur (Luce et al., 1990). This involves binding information from phonological, lexical-semantic, and syntactical processes (Jackendoff, 2007). The initial stages of speech processing include an acoustic-sensory level at which sounds are registered and a phonological level (at which sounds are recognized as portions of speech). Additional levels, occurring either subsequently or simultaneously—depending on one’s view— include lexical selection, word identification, and semantic comprehension, aided by syntax. The registration of sounds at the acoustic-sensory level relies heavily on bottom-up processes. Models of speech processing differ concerning the influence that top-down processes can exert on lower-level processes and the extent to which various processing stages are thought to interact (Friederici, 1999; Jusczyk & Luce, 2002; Nygaard & Pisoni, 1995). For example, Norris’s (1994) shortlist model presumes that speech recognition is an entirely bottom-up, data-driven process, while the TRACE model of McClelland and Elman (1986) assumes interactive activation of bottom-up and top-down processes.

We first begin with a discussion of the modularity issue in the form of single- versus dual-mechanism accounts of processing. This ongoing debate has frequently centered on how people process regular and irregular past-tense verbs. It has been referred to at times as the words-and-rules issue.
### **Single- versus Dual-Mechanism Theories **

The dual- versus single-mechanism issue has generated a productive debate in the literature. Dual-mechanism theories propose that the mental lexicon and the rules for combining its elements are separate processes. More specifically, they postulate separate cognitive and neural mechanisms for learning and knowledge of grammatical rules versus the storage of sound-word associations and semantic meanings (Chomsky, 1995; Friederici, 1999; Pinker, 1994; Pinker & Ullman, 2002; Ullman, 2001b; Ullman et al., 1997).

Much of the debate has involved examining the differences between the way in which regular and irregular past-tense verb forms are learned and retrieved. Regular past-tense forms follow clear rules, like adding - *ed *to change *walk *to *walked *and *learn *to *learned *. Irregular forms (which are much less frequent) do not conform to the rules. For example, the irregular past-tense form of *go *is *went *and of *ring *is *rang *. The dual-mechanism theory proposes that the rule-based forms are processed via the frontal cortex and basal ganglia mechanisms, while the irregulars rely on a mental lexicon involving temporal lobe mechanisms (Ullman, 2001c).

Support for the dual-mechanism theory comes primarily from investigations of children’s language acquisition and double dissociations found in different forms of aphasia. For example, neurological evidence provided support for dual-processing mechanisms distinguished by separable functional brain subregions in Broca’s area (Newman, Just, Keller, Roth, & Carpenter, 2003; Ullman, 2004; Ullman et al., 1997).

Ullman and colleagues (1997) suggested that language processing relies on both an extensive mental lexicon in which arbitrary sound-meaning pairings of words are stored and a separate rule-based system that they termed the mental grammar. In this model, the mental grammar facilitates the combination of words and phrases according to generative rules (Pinker, 1994; Pinker & Ullman, 2002). As children learn rules (e.g., adding -ed to many verbs results in a past-tense morphology, as in talk and talked), they are able to apply the rule in novel situations, such as to make past-tense forms of novel verbs (e.g., skug can be transformed to skugged). Ullman and colleagues discussed evidence that this rule-based system relies on brain mechanisms (left frontal areas, including Broca’s area) separate from the more memorybased repository system, which relies primarily on left temporal cortical areas. In support of the dual-mechanism account, Ullman and colleagues observed neural dissociations between patients with lesions affecting each of the areas of theoretical interest. For example, they reported that patients with frontal lesions often exhibit aggrammatic forms of aphasia characterized by omission or misuse of grammatical morphemes. (Grammatical morphemes include items such as *re- *, - *ed *, and - *s *, morphemes that by themselves have little or no meaning but change the meaning

when combined with a lexical morpheme like *work *or *play) *. However, their ability to use content words and nouns may be relatively intact. Conversely, individuals with lesions in temporal or parietal areas are more likely to use relatively intact syntactical structure while having profound deficits in accessing semantically appropriate content words. Specifically, patients with posterior aphasias and diseases such as Alzheimer’s that affect the general declarative memory system have more difficulties producing irregular past-tense forms relative to regular and novel past-tense verb forms. However, they may actually overuse grammatical rules. Conversely, patients with anterior aphasias and diseases affecting motor areas (such as Parkinson’s) typically have more difficulty using rules to produce regular and novel past forms.

Others argued that a dual-mechanism account is not necessary to explain the results of specific neurological impairments (Joanisse & Seidenberg, 1999; Kello, Sibley, & Plaut, 2005; Plaut, 2003). Single-mechanism accounts (which are primarily based on connectionist or neural network models) provide computational evidence that separate mechanisms are not necessary to explain the various patterns of results that are normally taken as support for the dual-mechanism route. Instead, a single computational system suffices (Jackendoff, 2007; Joanisse & Seidenberg, 1999, 2003). Models have been developed that can even account for the specific language deficits observed in different forms of aphasia (e.g., Joanisse & Seidenberg, 1999).

Joanisse and Seidenberg (1999) demonstrated that the past-tense dissociations observed in previous research could be explained within a single connectionist architecture by assuming the network to have both phonological and semantic nodes. They pointed out that there are similarities between many regular and irregular past-tense verbs. For example, like regular past tense, most irregular past-tense verbs still begin with the same letter and often contain the coda (the end portion of a syllable) of the main verb. They provide the following examples: *bake-baked *and *take-took *. Some irregular forms are quite similar to regular forms that are phonemically similar. They provided the examples of *crept *and *slept *, pointing out their similarity to *cropped *and *stepped *. Finally, they pointed out a third pattern that demonstrates an internal form of regularity even though it differs from regular past-tense verbs. Examples of this category are sing-sang, ring-rang, and grow-grew, throw-threw, blow-blew.

These observations led Joanisse and Seidenberg (1999) to suggest that difficulties with phonological processing would result in greater difficulty forming past-tense forms of novel words than of irregular past-tense verbs. Conversely, to construct most irregular past-tense verbs, the entire word must be recognized. Thus, irregular pasttense verb formation would be more affected by semantic difficulties. They pointed out that this corresponds with observations from the two forms of aphasia. Those with Broca’s-type aphasia or Parkinson’s tend to have greater difficulty with planning articulatory output and have difficulty applying the regular past-tense rule to novel words. Conversely, those with posterior aphasia or Alzheimer’s disease tend to have semantic deficits and are more impaired at producing irregular past-tense forms.

Joanisse and Seidenberg (1999) provided support for their position using a connectionist model that contained phonological processing nodes corresponding to acoustic features of speech (which would be used in hearing and speaking) and semantic nodes consisting of each verb to which the phonological nodes must be matched for comprehension or conversion from present to past tense. They mimicked language acquisition by training the network using a set of present- and past-tense monosyllabic verbs (containing both the dominant regular form and a smaller proportion of the irregular form). After training, they then damaged the network in one of two ways. First, they damaged only portions of the phonological layers. In the other version, they damaged only portions of the semantic layer. The resulting performance of the connectionist model mirrored their predictions and the dissociations observed in aphasics. Damage to the phonological layers resulted in more errors when the network attempted to generate past-tense forms of novel words (thus when trying to apply the rule) rather than irregular forms. Conversely, the network demonstrated more errors generating the irregular past-tense forms (relative to novel words) when portions of the semantic layer were damaged.

**FIGURE 8.2 **Broca’s and Wernicke’s areas. (Drawn by Melody Boyleston.)

Joanisse and Seidenberg (2005) found evidence that the phonological characteristics of the past-tense verb may play a larger role in the neural pathways that are activated than whether the verb is regular or irregular. Recall that regular past-tense verbs are thought to rely more on frontal areas, such as Broca’s area, while irregular past-tense forms are thought to rely more heavily on posterior temporal areas (Pinker & Ullman, 2002; Ullman et al., 1997) (see Figure 8.2 ). Using functional magnetic resonance imaging (fMRI), Joanisse and Seidenberg (2005) observed that irregular past-tense forms that shared phonological similarities with regular past-tense forms (e.g., slept, fled, sold) also produced more frontal activation similar to regular forms and relative to irregular forms that were not phonologically similar to regulars (e.g., took, gave) (p. 282). These observations led Joanisse and Seidenberg to the conclusion that dual mechanisms are not necessary to explain the dissociations between regular and irregular past-tense verb formation common to different forms of aphasia.

A debate of similar nature—but perhaps even more germane to the topic of mental workload—involves the nature of vWM. On one side of the debate, in the modularity or “speech-is-special” camp, are those who propose that language processing makes use of a specialized vWM system that can be distinguished from the more general vWM system used for conscious, controlled processing of verbal information (Caplan & Waters, 1999; Waters & Caplan, 1996). Conversely, others have argued that vWM is a general system that supports all verbal tasks (Just & Carpenter, 1992; Just et al., 1996). We briefly review this debate here, paying particular attention to its implications for positing independent versus general resource pools for verbal tasks. Those who suggest that language has a specialized vWM (Caplan & Waters, 1999;

DeDe et al., 2004; Waters & Caplan, 1996) proposed that this separate system draws on a resource pool largely independent from that of the general working memory system.
### **Single or Multiple Verbal Working Memory Systems **

A single vWM account was proposed by Just and Carpenter (1992) in reference to a capacity theory of language comprehension. Their capacity theory postulated that individual differences in working memory capacity would affect language comprehension processes in several ways. Notably, working memory capacity would affect the activation and interaction of a greater number of alternatives when presented sentences were ambiguous, particularly regarding syntax. For example, ambiguous sentences were presented, such as, “The waitress was talking about the calamity that occurred before the end of her shift.”

Listeners with high working memory capacities demonstrated recognition of the ambiguity, whereas listeners with low working memory capacities were unaffected by the ambiguity. Just and Carpenter (1992) suggested that the individuals with low working memory capacity were able to keep only one possible interpretation in mind and therefore selected that interpretation and were subsequently unaware of alternative interpretations of the sentence.

Just and Carpenter (1992) determined working memory capacity based on performance on Daneman and Carpenter’s (1980) reading span task. The reading span task requires participants to read a set of unrelated sentences and subsequently recall the final word from each sentence. Modified versions of the reading span task (including non-language-specific versions such as the operation span) are frequently used to assess working memory capacity (Barrett, Tugade, & Engle, 2004; Engle, Kane, & Tuholski, 1999; Unsworth, Heitz, Schrock, & Engle, 2005). Complex span tasks are thought to involve conscious, controlled processing in addition to temporary storage (see Engle, 2001, for a review) and have been shown to be predictive of a number of cognitive tasks, including reading comprehension (Daneman & Carpenter, 1980); tests of general fluid intelligence (Conway et al., 2002); deductive reasoning (Barrouillet, 1996); and mathematical problem solving (Engle, Tuholski, Laughlin, & Conway, 1999; Passolunghi & Siegel, 2001). See the work of Conway and Kane (2001) for a review. The observation that performance on many language tasks (including reading comprehension and sentence processing) is associated with working memory span task performance provides evidence that they share a common mechanism (Just & Carpenter, 1992).

Conversely, Waters and Caplan (Caplan & Waters, 1999; Waters & Caplan, 1996), in the style of Fodor (1983), postulated that some aspects of language comprehension, specifically those largely unconscious aspects that occur nearly automatically, represent a unique cognitive domain that can reasonably be expected to have developed a unique resource pool at least partially independent from overall general working memory capacity. They called this position the separate-language interpretation resource (SSLR) hypothesis.

Caplan and Waters (1999) contrasted the SSLR theory with a single-resource (SR) theory of researchers, such as that of Just and Carpenter (1992), that would postulate that language processing must draw working memory resources from a single pool. According to SR theory, then, as working memory capacity diminishes, fewer resources are available for language processing. Performing an additional task that requires working memory resources interferes with sentence processing (Just & Carpenter, 1992), thus supporting the conclusion that they compete for resources. In fact, difficulties with syntactical language abilities have been observed when college students are exposed to general stressors such as concurrent digit load, noise, and speeded speech (Bates, Dick, & Wulfeck, 1999).

Additional evidence that speech processing is a form of well-learned auditory pattern perception, rather than a specialized task with its own processing module, comes from both behavioral investigations (see Massaro, 1995, for a review) and neuroimaging research demonstrating that complex environmental sounds and linguistic sounds evoke similar patterns of behavior and largely overlapping areas of brain activation (Dick et al., 2007). In fact, complex musical structures appear to be processed in much the same way as linguistic syntactical constraints in trained musicians (Patel, 2003).

Moving on from questions regarding the possibility of modularity of speech processing, next we examine another distinction between major models of speech perception. Focusing specifically now on models of word recognition, we discuss the relative independence or interactivity proposed between bottom-up and top-down processes.
## **MODELS OF WORD RECOGNITION **

Several influential models of word recognition have been proposed. Each attempts to model the binding of acoustical information with lexical, syntactical information. A fundamental point of departure for the various models is whether they propose that these various stages of speech processing occur serially or in parallel and the degree to which they interact (Altmann, 1990; Cutler, 1995). At one end of the spectrum is the TRACE model (McClelland & Elman, 1986) of speech perception, one of the most widely regarded models of speech perception. TRACE is a computational model that proposes that several levels of processing—distinctive features, phonemes, and words—are simultaneously activated and interactively drive word recognition and speech comprehension. Conversely, computational models such as shortlist (Norris, 1994) propose an entirely bottom-up process. In shortlist, lexical information does not influence phonemic information; the process is strictly serial and feed forward.

Other models propose a limited degree of interaction. The COHORT model (Marslen-Wilson & Welsh, 1978; Marslen-Wilson, 1987) has been revised many times. It contains three primary stages or levels: access, selection, and integration. Originally, the early COHORT model contained a degree of interaction proposing that *access *from the spoken sound to *selection *at the lexical level was strictly bottom up—but that contextual cues from semantic and syntactical stages could influence the *integration *stage, thus assisting in the identification of which specific lexical form was most appropriate for the context. However, later versions of the COHORT model (Marslen-Wilson, 1987) revised this notion of interaction, proposing instead that the entire process proceeded in a bottom-up serial fashion and that stages became integrated but did not interact.

The neighborhood activation model (NAM; Luce & Pisoni, 1998) is similar to the COHORT model in that initial activation is thought to be an entirely bottom-up process. However, unlike the cohort model, NAM proposes that any portion of the word (not only the initial segment) is equally important in driving the activation process (Cutler, 1995). The COHORT model (Marslen-Wilson, 1987) proposed that only initial word phonemes were involved in driving the lexical selection process. Thus, hearing the word *bright *would activate cohorts such as bite, bake, blight, and black, but it would not activate the cohorts light and site. According to the COHORT model, word initial phonemes constrain activation of lexical items much more so than later phonemes; thus, unless the initial phoneme of the word is ambiguous—not heard properly—only cohorts beginning with the same initial phoneme will be activated. Conversely, NAM emphasizes a lexical activation process that involves the entire word segment, a feature it shares with the TRACE model.

The fuzzy logic model of perception (FLMP; Massaro & Cohen, 1991) provides yet another theoretical description of the process of integration between bottom-up and top-down processes. Like TRACE, FLMP assumes that initial sensory-perceptual processing occurs followed by a decision based on a relative goodness of fit or match with lexical candidates. The distinction here is that the FLMP model proposes that these bottom-up and top-down processes occur in parallel but independently (Cutler, 1995). Massaro and Cohen suggested that acoustical and contextual information independently influence perceptual recognition in both speech and reading. Both TRACE and FLMP assume that prelexical and contextual information influence lexical selection; the former simply proposes that the influences interactively occur, while the latter proposes that they exert independent influence.

Evidence from both behavioral studies and computational models overwhelmingly rule out a straight serial-sequential or feed-forward model of speech processing (McQueen, 2005). In the words of McClelland and Elman (1986, p. 4), “It is not only prior context but also subsequent context that influences perception.” Subsequent phonemes are used in the segmentation process to determine where one word ends and another begins—meaning that we often do not recognize one word until some time after hearing the next word (Grosjean, 1985). This illustrates how context can have an impact on lexical selection. But, context can even affect our interpretation of acoustical information at the prelexical-phoneme selection level.

Ganong (1980) artificially manipulated the voice onset time (VOT) of acoustic sounds so that they were in the ambiguous zone between two phonemes. Recall that in the ta-da continuum, sounds with a VOT time of less than 30 ms are consistently labeled “d,” while sounds with a VOT greater than 40 ms are labeled “t.” Ganong constructed words and nonwords such as *dash *and *tash *and *dask *and *task *using ambiguous initial phoneme sounds (d-t) with a VOT of 35 ms. Listeners made use of lexical information to disambiguate the acoustic information, demonstrating a preference for the phoneme that resulted in a word. This demonstrated pattern of interactivity and influence—as proposed in the TRACE and FLMP models of speech processing—indicates that both sensory and cognitive factors will have an impact on the mental workload of speech processing.
## **PARAMETERS INFLUENCING MENTAL WORKLOAD **

A number of different things have an impact on the difficulty of understanding speech. These parameters with an impact on the mental workload of speech processing can be environmental, as in the case of competing talkers or background noise; or a function of the properties of the speech content (i.e., syntactically complex sentences, unfamiliar vocabulary) or the speech signal (i.e., degraded transmissions, compressed signal, synthetic voice). These parameters can be divided into bottom- up and top-down influences. First, an important bottom-up influence—acoustic factors—is discussed.
### **Acoustic Factors **

One prediction resulting from current speech-processing models is that the difficulty of the speech-processing task will involve both the quality of the acoustical stimulus and the size of the lexical set from which a selection or decision must be made. At the same time, these two aspects interact: A more degraded acoustic stimulus will increase the number of lexical candidates that are activated and subsequently will increase the demand of speech recognition (see discussion in Baldwin, 2007). Numerous audiological reports have demonstrated that the difficulty of a speechprocessing test varies in accordance with both the acoustical properties of the test stimuli and the semantic features of the test stimuli (i.e., test set size and contextual cues) (Carhart, 1965; Miller, Heise, & Lichten, 1951). For example, word recognition is better in the context of sentences, even if the sentences are only syntactically appropriate but are not semantically appropriate (Miller & Isard, 1963).

The acoustic quality of the speech is affected by the presentation level or intensity (loudness) of speech, the acoustic environment in which it is presented (e.g., noise, competing speech), and the sensory capabilities of the listener. Each of these aspects independently and interactively has an impact on the mental workload demands of the speech-processing task.
#### **Presentation Level or Intensity **

Presentation level or presentation intensity affects the difficulty of speech processing (Baldwin & Struckman-Johnson, 2002). This is often a neglected aspect of speech perception research. Although researchers have investigated numerous aspects of auditory speech processing, the role of speech intensity beyond mere detectability has received only scant attention. In cognitive research, researchers frequently fail to report the decibel level used for presentation of speech stimuli, making comparisons across investigations problematic.

When presentation levels for speech stimuli are cited, they often seem to be chosen apparently arbitrarily within a range of 50 to 90 dB (Loven & Collins, 1988; Tschopp, Beckenbauer, & Harris, 1991a), perhaps under the assumption that as long as speech is intelligible, it will require equivalent attentional resources for processing across this wide range of (above-threshold) presentation levels. Research by Baldwin and colleagues (Baldwin & Ash, 2010; Baldwin, Lewis, & Morris, 2006; Baldwin & Struckman-Johnson, 2002) contradicted this assumption.

Baldwin and colleagues (Baldwin, Lewis, et al., 2006; Baldwin, May, et al., 2006; Baldwin & Struckman-Johnson, 2002) used a dual-task paradigm to examine the mental workload associated with speech processing in various conditions. As discussed in Chapter 5 , dual-task methods provide a more sensitive index of workload than can be obtained with primary task measures—such as speech intelligibility scores—alone. Primary task measures, namely, speech intelligibility quotients, have frequently been the quantitative measure of choice. For example, the effects of numerous physical and environmental parameters on the intelligibility of speech have been explored. Speaks, Karmen, and Benitez (1967) established the presentation levels associated with optimum speech intelligibility in environments with low background noise levels. Speaks et al. (1967) found that percentage correct identification of sentences in a quiet background rose sharply between presentation intensities of 20 and 30 dB. In fact, correct identification rose from 20% to 80% between 20 and 25 dB and reached nearly 100% by 30 dB, remaining constant through presentation intensities of 80 dB before beginning a gradual decline. Examining the impact of vocal force—shouting versus a soft whisper—Pickett (1956) found that speech intelligibility remained relatively constant as speakers increased their vocal force from 55 to 78 dB but diminished above and below this range.

Speech intelligibility has also been examined under less-ideal conditions, such as with degradation due to a chopping circuit that intermittently cuts out portions of the verbal message (Payne et al., 1994) in the presence of noise (Broadbent, 1958; Kryter, 1972, 1985; Smith et al., 1981); for synthetic speech (Robinson & Eberts, 1987; Simpson & Marchionda-Frost, 1984); and under conditions of electronic versus natural amplification (Tschopp, Beckenbauer, & Harris, 1991). Despite the abundance of literature on speech intelligibility under diverse conditions, the potential of signal intensity (from either less-than-ideal listening conditions or degraded sensory capabilities) to alter the mental workload requirements of speech stimuli has received considerably less attention.
#### **Speech Pace **

Speech pace can affect the mental workload of speech processing. Pace, as used here, refers to how quickly one verbal item (e.g., a sentence or conversational turn) follows another. It can be distinguished from the term *speech rate *, which refers to the number of syllables per second, a topic taken up in the next section. To distinguish between the two, imagine a situation in which a lively conversation takes place among a group of good friends. The speech pace may be quite rapid, with one person interjecting a statement immediately after or perhaps even during the statement of the previous speaker. Assuming that the speech is not overlapping (when it could arguably be considered competing speech), there is some evidence to suggest that the pace of the speech affects the effort required to process it.

Baldwin and Struckman-Johnson (2002) used a dual-task paradigm for assessing the mental workload of speech processing as a function of presentation intensity and speech pace. Their listeners made sentence verifications for speech played at 45, 55, and 65 dB presented at a rate of either 3 or 5 s between each sentence. Both manipulations—presentation rate and intensity—were shown to affect the mental workload of the speech-processing task. People made more processing errors and took longer to respond when sentences were presented at lower intensities and when the sentences were presented with less time in between (a faster pace). Importantly, though, differences in mental workload were observed only in the dual-task conditions. That is, when the sentence verification task was performed in the presence of a simulated driving task used as a loading task, the extra effort required to process lower-intensity and faster-paced speech was evident. This extra effort was not evident when listeners could devote all of their attentional resources to the speechprocessing task. In addition to the pace of speech materials, the rate at which this information is presented is another important factor.
#### **Speech Rate **

Speech rate is comprised of a number of different components, including articulation rate and pause rate. Studies have shown that there is tremendous variability both within and between speakers in both the number and duration of pauses and the articulation rate (Miller et al., 1984). Both of these factors have an impact on speech perception (see review in Nygaard & Pisoni, 1995). As previously discussed, we rely on minute changes in the temporal characteristics of speech to make distinctions between phonemic categories, such as to distinguish /pa/ from /ba/. Not surprisingly, then, changes in speech rate affect the mental workload of speech processing.

Speech rate can be measured a number of ways, including the number of syllables or words per minute uttered or the duration of individual syllables and pauses. An early controversy concerned whether speech rate varied primarily because of the number of pauses inserted within utterances or whether the syllable or word utterance rate changed independent of the pauses. Conclusive evidence was provided that both these factors, independently as well as interactively, have effects on speech rate (Miller et al., 1984). Current evidence indicates that speaking rate does vary considerably, within a given individual as a function of factors such as the emotional state of the speaker, the familiarity with the material being spoken, and the workload of the situation. These rate changes have an impact on speech-processing difficulty (see review in Nygaard & Pisoni, 1995).

Researchers commonly distinguish between speech rate and articulation rate. Speech rate includes pauses and is typically measured in words per minute (wpm). Articulation rate excludes pauses (generally of at least 250 ms) and is typically measured in syllables per second (sps) (Goldman-Eisler, 1956; Sturm & Seery, 2007). Average speech rates are around 140–150 wpm with average articulation rates of roughly 4 sps (Venkatagiri, 1999). Slow speech ranges from 136 to 144 wpm with an articulation rate of 3.36–4.19 sps, and fast speech rates are on the order of 172 wpm with an articulation rate range of 5.29–5.9 sps. Contrary to reports in some popular literature, there is no evidence that females have significantly higher speech rates than males (Venkatagiri, 1999).

Changes in speech rate and speech rate variability can have a differential impact on individual aspects of speech processing. For example, as rate increases, vowel durations are shortened more than consonant durations (Gay, 1978), but the relative rate of the difference varies between individual speakers. Phonemes that depend heavily on temporal distinctions, such distinctions made on the basis of VOT, are particularly compromised.

Many sources of evidence indicate that speech rate variability affects the mental workload of speech processing. Variable speech rates decrease word recognition independently and exacerbate processing difficulties when combined with other factors, such as trying to recognize words from multiple speakers (Sommers, Nygaard, & Pisoni, 1994). Listeners who already have problems understanding speech find it even more difficult with increased speech rates. For example, children who speak Black English (BE) have more difficulty comprehending sentences spoken in Standard English (SE) than do children who speak SE. These comprehension difficulties are exacerbated when SE sentences are spoken increasingly faster, whereas such rates do not have a negative impact on SE-speaking children (Nelson & McRoskey, 1978). Similarly, children with language disorders have increased speech-processing difficulties as speech rates increase (Blosser, Weidner, & Dinero, 1976).

Fast speech rates require more mental effort to process and can result in increased processing errors. Importantly, speech production rates tend to increase as workload increases (Brenner, Doherty, & Shipp, 1994), thus potentially further compromising communication. It should be noted that there is some evidence that increasing speech rate artificially through linear time compression may lead to better speech recognition than naturally produced fast speech (Janse, 2004), an issue that is discussed further in reference to designing speech displays in Chapter 11. Both high-workload and high-stress operational environments, unfortunately, can result in rapid speech rates, which may exacerbate communication errors. In one investigation, Morrow and colleagues (Taylor, Yesavage, Morrow, & Dolhert, 1994) observed that during high-workload periods, air traffic controllers may speak to pilots at rates as high as 365 wpm, compared with a normal average closer to 235 wpm. This increased pace was associated with pilots exhibiting nearly 20% more command execution errors. Speech rate is only one of the aspects of a broader category, referred to as prosody or prosodic cues, which both affect and are affected by high-workload situations and have an impact on the mental workload of speech processing.
#### **Prosody **

Prosody, or the melodic pattern in speech, greatly influences speech perception (Schirmer, Kotz, & Friederici, 2002). The same word or phrase can have vastly different meanings, depending on prosodic information (or the way it is said). Prosody is comprised of the intonation, loudness, stress patterns, and tempo of speech (Mitchell, Elliott, Barry, Cruttenden, & Woodruff, 2003). Changes in the prosody affect the interpretation of words and sentences. Consider the following example, in which emphasis is placed on the underlined word:

The professor asked the question to the young man in the class.

The professor asked the question to the young man in the class.

The professor asked the question to the young man in the class.

The professor asked the question to the young man in the class .

There are subtle but important differences in the interpretation of each of these sentences depending on their prosody and, in particular, the word that is emphasized. We see that prosodic elements provide information regarding what subjects

are important to the current meaning and therefore should be emphasized. In addition, prosodic elements provide important cues to help the listener understand the emotional nature of utterance and detect the presence of intent of humor or sarcasm.

*Affective Prosody *

One particularly interesting aspect of research pertaining to prosody concerns examinations of how we process the emotional aspects of speech. Specifically, researchers have examined how affective prosody influences comprehension and whether this aspect of the signal is processed independently of the semantic and syntactic content (Berckmoes & Vingerhoets, 2004; Vingerhoets, Berckmoes, & Stroobant, 2003).

Processing emotional aspects of speech results in greater activation of the right cerebral hemisphere, relative to left hemispheric lateralization observed for processing syntactic and semantic aspects (Mitchell et al., 2003; Vingerhoets et al., 2003). This greater right hemispheric activation associated with processing the emotional aspects of speech occurs in both listening (Berckmoes & Vingerhoets, 2004) and reading (Katz, Blasko, & Kazmerski, 2004). Similarly, our brains process humor and sarcasm differently from other nonliteral forms of language. When processing sarcasm, for example, we utilize contextual cues such as knowledge of the speaker’s gender and social status in our interpretation of the actual words spoken (Katz et al., 2004).

In particular, Schirmer et al. (2002) found that men and women demonstrate different behavioral and electrophysiological responses to emotional prosody. Women demonstrated a priming effect, responding faster to target words that matched the emotional valence of the prosody of a preceding word. Women also demonstrated smaller N400 responses to match relative to mismatch conditions, which were evident in broadly distributed, nonhemisphere-specific P200 and P300 ERP responses as well. Males did not show these emotional-prosodic priming effects. However, males did demonstrate significantly faster reaction times for positive relative to negative target words. Females did not show this difference. The observation that females often pay more attention to the processing of emotional cues may explain in part the greater bilateral activation among females relative to males during speech-processing tasks discussed previously in Chapter 3 .

The processing of emotional content in speech may have an impact on where we direct visual attention. For example, a visual dot presented on the left side of a speaker’s face (right hemisphere directed) is discriminated more efficiently when people listen to speech with emotional prosody, whereas the opposite effect (greater discrimination on the right side of the speaker’s face) is found when people listen to neutral speech (Thompson, Malloy, & LeBlanc, 2009). It appears that people expect to get more emotional information from processing visual cues on the left side of the face and more speech information from processing cues on the right side of the face (Nicholls, Searle, & Bradshaw, 2004). Importantly, Nicholls and colleagues demonstrated that it was the *expectation *of which side of the face would be most informative rather than where the information actually was. They did this by presenting mirror images of a face and observing decreased McGurk effects to the mirrored image. Expectations play an important role in speech processing. The impact of expectations and context are discussed in greater depth in this chapter.

The focus is turned next to the topic of synthetic speech. One of the primary reasons why even relatively high-quality synthetic speech may be difficult to understand is because of its lack of prosodic cues.
#### **Synthetic Speech **

Synthetic speech generation is increasingly common as a method of enhancing automated systems (i.e., computer-aided instruction, information retrieval systems, and guidance through complex menu structures) and as communicative aides for persons with disabilities. Most everyone reading this will have heard synthetic speech in one form or another. It generally relies on text-to-speech (TTS) technologies, which have experienced considerable research and advances in the last several decades. For some individuals, like celebrated astrophysicist Stephen Hawking, TTS speech-generating devices greatly enhance communicative capabilities. Speech-generating devices use TTS technologies to convert words, alphabets, digits, and graphic symbols to audible speech output (Koul, 2003). In other applications, such as for automated menus and navigational aids, synthetic speech greatly reduces the physical storage needs relative to digital recordings of natural speech, thus allowing speech interfaces in a great number of portable devices.

Tremendous progress has been made since the late 1970s in improving the quality of synthetic speech algorithms. Current TTS systems are capable of producing speech sounds that are much more natural and intelligible than their earlier counterparts of the 1970s and early 1990s (Taylor, 2009). However, despite this progress, today’s systems are still recognizable as nonhuman, and they still generally require greater mental effort to process than natural speech, particularly in adverse listening conditions (Francis & Nusbaum, 2009; Koul, 2003). So, there is still work to be done.

As Taylor (2009) pointed out, much of the early research focused on making synthetic voices sound more natural without sacrificing intelligibility. The two concepts do not go hand in hand. Some of the very aspects that made early synthetic speech more intelligible (i.e., accurate distinct articulation) also tended to make it sound unnatural and robotic.

Synthetic voices have now become increasingly easier to understand, with intelligibility scores of the better systems nearly rivaling those of natural speech, at least in ideal listening conditions (Koul, 2003), while also sounding more natural. A number of methods have been used to assess the effectiveness or quality of synthetic speech. These range from measuring the intelligibility of individual phonemes and single words to metrics that assess comprehension from processing and perception of more complex forms (i.e., sentences and discourse). Comprehension relates to the ability to form a coherent mental representation and involves higher-level processing (Kintsch & van Dijk, 1978; Koul, 2003). Comprehension measures can be further subdivided into those that are simultaneous (requiring some online recognition or detection during processing) or successive, for which testing occurs immediately after presentation.

In general, intelligibility scores for synthetic speech are lower than for natural speech. In ideal listening conditions, single-word intelligibility scores for natural speech generally range from 97.2% to 99% but are reduced to 81.7% to 96.7% for even the best synthetic speech systems (see review in Koul, 2003). In adverse listening conditions, such as those with poor signal-to-noise ratios, intelligibility scores decrease dramatically for both natural and synthetic speech, but synthetic speech degrades more distinctly (Fucci, Reynolds, Bettagere, & Gonzales, 1995; Koul & Allen, 1993).

Similar results are obtained when perception of synthetic versus natural sentences is compared. Under ideal listening conditions and supportive context, sentence perception is nearly equivalent, although there is considerable variability across synthesizers (see review in Koul, 2003), and synthesized child voices are generally harder to understand than adult voices (Von Berg, Panorska, Uken, & Qeadan, 2009).

When more sensitive indices are used (i.e., sentence verification latency) or when synthetic and natural speech sentences are compared under divided attention conditions, results indicated that the synthetic speech required greater cognitive effort to process. Relative to natural speech, processing synthetic speech is slower and less accurate (Ralston, Pisoni, Lively, Greene, & Mullennix, 1991). Two additional factors that have an impact on the acoustic signal indirectly are discussed next.
#### **Acoustic Environment **

The impact of the acoustic environment on mental workload is a topic deserving greater attention. Noise, reverberation, and competing speech have particular deleterious effects on speech intelligibility and can be expected to increase the mental workload of speech processing. However, due to the robust nature of speech processing, not all task paradigms have been sensitive enough to obtain measurable changes in mental workload in support of this position. Urquhart (2003) used a dual-task paradigm to examine the impact of noise (from an army vehicle) on speech processing when stimuli were played at either 83 or 96 dB. Performance on a complex cognitive task increased when speech stimuli were presented at 96 dB relative to 83 dB. However, subjective measures of mental workload obtained from the NASA Task Load Index (NASA TLX; Hart & Staveland, 1988) and the Modified Cooper Harper Rating Scale were not sensitive to these changes in mental workload. Presuming that the decreases in cognitive task performance indicate the task was harder in the presence of noise at the lower (83 dB) condition, the lack of sensitivity of either subjective rating scale is surprising. It is possible that listeners were not aware of the increased workload or that their awareness of the difference was forgotten by the time they completed the rating scales at the end of the experiment.

Another important environmental variable is the number of other simultaneous conversations present. As discussed in Chapter 7 in reference to the impact of noise on workload, irrelevant or competing speech is particularly detrimental to successful speech processing. Open office plans and other work environments where speech must be processed under these conditions increase the mental workload of speech perception. Competing speech can be particularly difficult for those with hearing impairment.
### **Hearing Impairment **

Hearing impairment functionally degrades the quality of the incoming acoustic stimulus. If the acoustical trace is degraded, a larger set of lexical candidates is likely to be activated, making the lexical selection process more difficult. In other words, a hearing-impaired person may have difficulty distinguishing between consonants such as *f, s, th *, and *k *, particularly in conditions of low presentation intensity. When presented with the initial consonant sound in a word such as the *th *in the word *that *, this person could potentially have to select from a larger activated lexical set consisting of *f *at, *s *at, and *c *at in addition to the lexical versions of *th *at, *th *an, and so on. This person would necessarily need to rely more heavily on context to make a lexical selection and might require a longer period of time for the subsequent selection stage. Fletcher (1953) reported that, on average, non-hearing-impaired listeners require 150 to 350 ms to process vowel sounds in normal conversation. Listeners with presbycusis (age-related hearing loss) might take longer to complete the early processing stages by receiving a less-robust preperceptual image with which to begin the processing task. Grosjean (1980; Grosjean & Gee, 1987) found that, in the context of a sentence, lexical decisions regarding the identification of an initial monosyllabic word were often not finalized until presentation of the next noun in the sentence. Grosjean pointed out that this may be a full 500 ms later. A hearing-impaired listener, already experiencing slower phonemic recognition, would then have a larger number of lexical alternatives from which to choose—thus likely causing additional delays in later processing stages and potentially communication breakdown if speech is presented too quickly. This is one way in which hearing impairment may exacerbate or be confused for cognitive impairments among older adults, a topic we take up in Chapter 10 . The number of lexical alternatives generated can have an impact on the mental workload of speech processing. The hearing-impaired listener benefits greatly from knowing the general context of the spoken material, much in the same way that an effective title aids reading comprehension. We turn our attention next to these more cognitive-contextual cues.
### **Contextual Factors **

As with all sensory processing, context is important in speech processing. The same acoustical signal can be perceived as different speech sounds depending on the context in which it is found (Holt & Lotto, 2002). A well-known example is the acoustically similar sentence, “I scream, you scream, we all scream for ice cream.” Context is necessary to accurately differentiate between the phonetically similar “I scream” and “ice cream.” The importance of context in disambiguating variant speech signals was observed by Shockey and Reddy (1975, as cited by Plomp, 2002) in an investigation of the phonetic recognition abilities of phoneticians listening to a foreign language. The phoneticians could recognize only 56% of the phonetic symbols in the unfamiliar language, and there was great discrepancy in identification of particular phonemes between phoneticians.

Contextual cues reduce the mental workload of speech processing on multiple levels. Lexical or word knowledge assists in the recognition of ambiguous phonemic categories. For example, Ganong (1980) demonstrated that listeners tended to make phonemic categorizations that favored a real word (choosing dash instead of tash or task instead of dask) when presented with ambiguous phonemic information. At the lexical level, words that are more frequent and have fewer lexical neighbors (words that differ by only one phoneme) are recognized more easily than words

with high neighborhood densities (Dirks, Takayanagi, & Moshfegh, 2001; Norris, 2006; Slattery, 2009). At the sentence level, word recognition is facilitated by sentence-level context (Revill, Tanenhaus, & Aslin, 2008; Slattery, 2009) and to a lesser degree syntactical information even if it does not make grammatical sense (Miller & Isard, 1963). Contextual effects aid word recognition in all listeners but are differentially beneficial to older listeners (Abada, Baum, & Titone, 2008; Dubno et al., 2008; Laver & Burke, 1993), as discussed further in Chapter 10 .
#### **Speaker Familiarity **

It is easier to understand the speech of someone with whom we are familiar. A particularly dramatic example of this is how it may be difficult to understand the speech of someone with a heavy accent when we first get to know him or her. However, with greater exposure and familiarity with his or her speech, it becomes easier. Presenting speech material in a familiar or single voice aids word recognition and decreases the mental workload of speech processing. Experiments involving recall of lists of words indicated that recall is better (particularly for items at the beginning of the list) if the list is presented by a single speaker rather than multiple speakers (Martin, Mullennix, Pisoni, & Summers, 1989), and word recognition is greater when presented by a single relative to multiple speakers (Mullennix, Pisoni, & Martin, 1989). The processing of nonlexical information from multiple speakers appears to require attentional resources that must be redirected from the processing of other tasks. Processing speech from familiar voices is less resource demanding than from unfamiliar voices.

In sum, both acoustic and contextual factors can independently and interactively have an impact on the mental workload of speech processing. Adverse listening conditions stemming from low signal intensity, background noise, or hearing impairment increase attentional processing requirements. Fast speech rate and variability in prosodic cues also have a negative impact, while contextual factors serve to reduce ambiguity and decrease processing requirements. In the remainder of this chapter, we examine the impact that processing speech can have on performance of other nonlexical tasks.
## **APPLICATIONS **

Changes in the mental workload requirements of speech processing have implications for how well people can carry out operational tasks at the same time, such as driving, flying, or interacting with computer systems. One area that has received considerable attention in recent years is that of conversing on a mobile phone while driving.
### **Mobile Phones and Driving **

It is well documented that driver distraction is a threat to transportation safety (Hancock, Lesch, & Simmons, 2003; Lee et al., 2001; Lee, McGehee, Brown, & Reyes, 2002; Stutts, Reinfurt, Staplin, & Rodgman, 2001). Mobile phone conversations are considerably more distracting than are conversations with passengers (McEvoy et al., 2005; Strayer et al., 2003). Talking on a mobile phone irrespective of whether it is a handheld or hands-free device slows response time to external events and stimuli by 0.25 s on average (Caird et al., 2008). Conversing on a mobile phone has been found to increase crash risk up to fourfold, and contrary to popular belief, hands-free phone devices offer no significant reduction in crash risk (McEvoy, Stevenson, & Woodward, 2007; Redelmeier & Tibshirani, 1997). Also contrary to popular belief, greater experience using a cell phone while driving does not appear to mitigate the increased crash risk (Cooper & Strayer, 2008).

Degradation in the speech signal is commonplace when conversing on a mobile phone (Kawano, Iwaki, Azuma, Moriwaki, & Hamada, 2005). These degradations can be expected to further increase the processing requirements of the conversation, leaving reduced attentional resources for other tasks such as driving and hazard detection.

When speech processing must be time-shared with a visual task, like driving, processing requirements are reduced when the speech task is presented from the same spatial location where visual attention must be focused (Spence & Read, 2003). This observation provides possible support for a reduction in mental workload and potentially some crash risk reduction when using hands-free mobile devices that utilize the front speakers of the vehicle (i.e., Bluetooth interfaces). However, this conclusion awaits further research.

Speech processing can have detrimental effects on concurrent tasks other than driving. Processing spoken information has been shown to disrupt simple and choice reaction time tasks in both young and old (Tun, Wingfield, & Stine, 1991) and manual tapping tasks (Seth-Smith, Ashton, & McFarland, 1989) and can even disrupt walking in older adults (Lindenberger, Marsiske, & Baltes, 2000).
### **Air Traffic Control Communications **

Communication between pilots and air traffic controllers is a special class of speech communications. Failure of this communication process has been attributed as a causal factor in a great number of aviation accidents and incidents (Hawkins, 1987). In an attempt to reduce the number of miscommunication incidents, a number of human factors guidelines have been implemented in this area. The use of restricted vocabulary and implementation of strict adherence to procedural routines are some of the practices that have improved air traffic control communications. This important topic is discussed in more detail in Chapter 11 in a discussion of auditory display design in aviation.
## **SUMMARY **

Extracting meaning from the transient acoustical variations that make up speech is an effortful process. Knowledge of the language, speaker familiarity, semantic context, syntax, and situational expectations are some of the many things used to assist in the interpretation of speech sounds. The effort involved in speech processing can be increased by sensory factors, such as poor signal quality, noise, or hearing impairment, as well as by cognitive factors such as low text cohesion, complex syntax, or the absence of context clues.

Studies of language acquisition and double dissociations found in different forms of aphasia have led some researchers to suggest that language is processed by dual mechanisms, one for the large repertoire of stored lexical units and another for the rules for combining these lexical elements. By this account, arriving at the correct form of a regular past-tense verb (such as dated from date and voted from vote) uses one mechanism with distinct neural circuitry, while arriving at the past tense of an irregular verb (i.e., went from goes, dug from dig) relies on a separate mechanism. Others argued, largely through artificial neural network or connectionist modeling, that separate pathways are not necessary to account for dissociations between regular and irregular past-tense verb formation.
## **CONCLUDING REMARKS **

Sensory and cognitive processes interact to affect the mental workload of speech processing. Up to this point in the book, I have primarily focused on discussing how changes in the auditory environment affect this process, making it more or less effortful. However, more often than not, auditory processing takes place in conjunction with visual processing. This visual processing can be either complementary or contradictory. In the next chapter, I turn to the topic of cross-modal processing, paying particular attention to the interaction of visual and auditory processing.

* Acoustic cues also appear to play a role in segmenting speech material at the paragraph level. Waasaf (2007) demonstrated that both speakers and simultaneous interpreters make use of intonation patterns to indicate the start and end of paragraphs, using higher pitches at the start of new paragraphs and lower pitches to signify the end of a paragraph.